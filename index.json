[{"categories":[""],"content":"1111111111 ","date":"2023-11-01","objectID":"/test1/:0:0","tags":null,"title":"Test1","uri":"/test1/"},{"categories":["c++"],"content":"STL容器 ","date":"2023-11-01","objectID":"/stl/:0:0","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"STL序列式容器 ","date":"2023-11-01","objectID":"/stl/:1:0","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"array数组 array 容器是 C++ 11 标准中新增的 序列容器，简单地理解，它就是在 C++ 普通数组的基础上，添加了一些成员函数和全局函数。在使用上，它比普通数组更安全，且效率并没有因此变差。 和其它容器不同，array 容器的大小是固定的，无法动态的==扩展==或==收缩==，这也就意味着，在使用该容器的过程无法借由增加或移除元素而改变其大小，它只允许访问或者替换存储的元素。 在 array\u003cT,N\u003e 类模板中，T 用于指明容器中的存储的具体数据类型，N 用于指明容器的大小，需要注意的是，这里的 N 必须是常量，不能用变量表示。 std::array\u003cdouble, 10\u003e values; array的初始化赋值语法 std::array\u003cdouble, 10\u003e values {0.1,1.4,1.8,,2.4}; 这里只初始化了前 4 个元素，剩余的元素都会被初始化为 0.0。 成员函数 功能 begin() 返回指向容器中第一个元素的随机访问迭代器。 end() 返回指向容器最后一个元素之后一个位置的随机访问迭代器，通常和 begin() 结合使用。 rbegin() 返回指向最后一个元素的随机访问迭代器。 rend() 返回指向第一个元素之前一个位置的随机访问迭代器。 cbegin() 和 begin() 功能相同，只不过在其基础上增加了 const 属性，不能用于修改元素。 cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crbegin() 和 rbegin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crend() 和 rend() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 size() 返回容器中当前元素的数量，其值始终等于初始化 array 类的第二个模板参数 N。 max_size() 返回容器可容纳元素的最大数量，其值始终等于初始化 array 类的第二个模板参数 N。 empty() 判断容器是否为空，和通过 size()==0 的判断条件功能相同，但其效率可能更快。 at(n) 返回容器中 n 位置处元素的引用，该函数自动检查 n 是否在有效的范围内，如果不是则抛出 out_of_range 异常。 front() 返回容器中第一个元素的直接引用，该函数不适用于空的 array 容器。 back() 返回容器中最后一个元素的直接应用，该函数同样不适用于空的 array 容器。 data() 返回一个指向容器首个元素的指针。利用该指针，可实现复制容器中所有元素等类似功能。 fill(val) 将 val 这个值赋值给容器中的每个元素。 array1.swap(array2) 交换 array1 和 array2 容器中的所有元素，但前提是它们具有相同的长度和类型。 ","date":"2023-11-01","objectID":"/stl/:1:1","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"vector容器 vector 容器是 STL 中最常用的容器之一，它和 array 容器非常类似，都可以看做是对 C++ 普通数组的 “升级版”。不同之处在于，array 实现的是==静态数组==（容量固定的数组），而 vector 实现的是一个==动态数组==，即可以进行元素的插入和删除，在此过程中，vector 会动态调整所占用的内存空间，整个过程无需人工干预。 vector 常被称为向量容器，因为该容器擅长在尾部插入或删除元素，在==常量时间内就可以完成==，时间复杂度为 O(1)；而对于在容器头部或者中部插入或删除元素，则花费时间要长一些（移动元素需要耗费时间），时间复杂度为线性阶 O(n)。 vector初始化 std::vector\u003cint\u003e numbers = {1, 2, 3, 4, 5}; 函数成员 函数功能 begin() 返回指向容器中第一个元素的迭代器。 end() 返回指向容器最后一个元素所在位置后一个位置的迭代器，通常和 begin() 结合使用。 rbegin() 返回指向最后一个元素的迭代器。 rend() 返回指向第一个元素所在位置前一个位置的迭代器。 cbegin() 和 begin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crbegin() 和 rbegin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crend() 和 rend() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 ==size()== 返回实际元素个数。 max_size() 返回元素个数的最大值。这通常是一个很大的值，一般是 232-1，所以我们很少会用到这个函数。 resize() 改变实际元素的个数。 capacity() 返回当前容量。 ==empty()== 判断容器中是否有元素，若无元素，则返回 true；反之，返回 false。 reserve() 增加容器的容量。 shrink _to_fit() 将内存减少到等于当前元素实际所使用的大小。 operator[ ] 重载了 [ ] 运算符，可以向访问数组中元素那样，通过下标即可访问甚至修改 vector 容器中的元素。 at() 使用经过边界检查的索引访问元素。 front() 返回第一个元素的引用。 back() 返回最后一个元素的引用。 data() 返回指向容器中第一个元素的指针。 assign() 用新元素替换原有内容。 push_back() 在序列的尾部添加一个元素。 pop_back() 移出序列尾部的元素。 insert() 在指定的位置插入一个或多个元素。 erase() 移出一个元素或一段元素。 clear() 移出所有的元素，容器大小变为 0。 swap() 交换两个容器的所有元素。 emplace() 在指定的位置直接生成一个元素。 emplace_back() 在序列尾部生成一个元素。 ","date":"2023-11-01","objectID":"/stl/:1:2","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"deque队列 STL 中的 deque 是双端队列容器，deque 容器和 vector 容器有很多相似之处，deque 容器也==擅长在序列尾部添加或删除元素==（时间复杂度为O(1)），而不擅长在序列中间添加或删除元素。deque 容器也可以根据需要修改自身的容量和大小。 和 vector 不同的是，deque 还==擅长在序列头部添加或删除元素==，所耗费的时间复杂度也为常数阶O(1)。并且更重要的一点是，deque 容器中存储元素并不能保证所有元素都存储到连续的内存空间中。 当需要向==序列两端频繁的添加或删除元素==时，应首选 deque 容器。 函数成员 函数功能 begin() 返回指向容器中第一个元素的迭代器。 end() 返回指向容器最后一个元素所在位置后一个位置的迭代器，通常和 begin() 结合使用。 rbegin() 返回指向最后一个元素的迭代器。 rend() 返回指向第一个元素所在位置前一个位置的迭代器。 cbegin() 和 begin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crbegin() 和 rbegin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crend() 和 rend() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 size() 返回实际元素个数。 max_size() 返回容器所能容纳元素个数的最大值。这通常是一个很大的值，一般是 232-1，我们很少会用到这个函数。 resize() 改变实际元素的个数。 empty() 判断容器中是否有元素，若无元素，则返回 true；反之，返回 false。 shrink _to_fit() 将内存减少到等于当前元素实际所使用的大小。 at() 使用经过边界检查的索引访问元素。 front() 返回第一个元素的引用。 back() 返回最后一个元素的引用。 assign() 用新元素替换原有内容。 push_back() 在序列的尾部添加一个元素。 ==push_front()== 在序列的头部添加一个元素。 pop_back() 移除容器尾部的元素。 pop_front() 移除容器头部的元素。 insert() 在指定的位置插入一个或多个元素。 erase() 移除一个元素或一段元素。 clear() 移出所有的元素，容器大小变为 0。 swap() 交换两个容器的所有元素。 emplace() 在指定的位置直接生成一个元素。 ==emplace_front()== 在容器头部生成一个元素。和 push_front() 的区别是，该函数直接在容器头部构造元素，省去了复制移动元素的过程。 emplace_back() 在容器尾部生成一个元素。和 push_back() 的区别是，该函数直接在容器尾部构造元素，省去了复制移动元素的过程。 ","date":"2023-11-01","objectID":"/stl/:1:3","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"list链表 STL 中的 list 容器，又称双向链表容器，即该容器的底层是以双向链表的形式实现的。这意味着，list 容器中的元素可以分散存储在内存空间里，而不是必须存储在一整块连续的内存空间中。 list 容器中各个元素的前后顺序是靠指针来维系的，每个元素都配备了 2 个指针，分别指向它的前一个元素和后一个元素。其中第一个元素的前向指针总为 null，因为它前面没有元素；同样，尾部元素的后向指针也总为 null。 基于这样的存储结构，list 容器具有一些其它容器（array、vector 和 list）所不具备的优势，==即它可以在序列已知的任何位置快速插入或删除元素==（时间复杂度为O(1)）。并且在 list 容器中移动元素，也比其它容器的效率高。 使用 list 容器的缺点是，==它不能像 array 和 vector 那样，通过位置直接访问元素==。举个例子，如果要访问 list 容器中的第 3 个元素，==它不支持容器对象名[3]这种语法格式==，正确的做法是从容器中第一个元素或最后一个元素开始遍历容器，直到找到该位置。 实际场景中，==如何需要对序列进行大量添加或删除元素的操作==，而直接访问元素的需求却很少，这种情况建议使用 list 容器存储序列。 成员函数 功能 begin() 返回指向容器中第一个元素的双向迭代器。 end() 返回指向容器中最后一个元素的双向迭代器。 rbegin() 返回指向最后一个元素的反向双向迭代器。 rend() 返回指向第一个元素所在位置前一个位置的反向双向迭代器。 cbegin() 和 begin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crbegin() 和 rbegin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 crend() 和 rend() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。 empty() 判断容器中是否有元素，若无元素，则返回 true；反之，返回 false。 size() 返回当前容器实际包含的元素个数。 max_size() 返回容器所能包含元素个数的最大值。这通常是一个很大的值，一般是 232-1，所以我们很少会用到这个函数。 front() 返回第一个元素的引用。 back() 返回最后一个元素的引用。 assign() 用新元素替换容器中原有内容。 emplace_front() 在容器头部生成一个元素。该函数和 push_front() 的功能相同，但效率更高。 push_front() 在容器头部插入一个元素。 pop_front() 删除容器头部的一个元素。 emplace_back() 在容器尾部直接生成一个元素。该函数和 push_back() 的功能相同，但效率更高。 push_back() 在容器尾部插入一个元素。 pop_back() 删除容器尾部的一个元素。 emplace() 在容器中的指定位置插入元素。该函数和 insert() 功能相同，但效率更高。 insert() 在容器中的指定位置插入元素。 erase() 删除容器中一个或某区域内的元素。 swap() 交换两个容器中的元素，必须保证这两个容器中存储的元素类型是相同的。 resize() 调整容器的大小。 clear() 删除容器存储的所有元素。 splice() 将一个 list 容器中的元素插入到另一个容器的指定位置。 remove(val) 删除容器中所有等于 val 的元素。 remove_if() 删除容器中满足条件的元素。 unique() 删除容器中相邻的重复元素，只保留一个。 merge() 合并两个事先已排好序的 list 容器，并且合并之后的 list 容器依然是有序的。 sort() 通过更改容器中元素的位置，将它们进行排序。 reverse() 反转容器中元素的顺序。 ","date":"2023-11-01","objectID":"/stl/:1:4","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"STL关联式容器 STL 中的关联式容器主要包括 map、multimap、set 以及 multiset。和 序列式容器 不同的是，关联式容器在存储元素时还会为每个元素在配备一个键，整体以键值对的方式存储到容器中。 相比序列式容器，关联式容器可以通过键值直接找到对应的元素，而无需遍历整个容器。另外，关联式容器在存储元素，默认会根据各元素==键值的大小做升序排序==。相比其它类型容器，关联式容器==查找、访问、插入和删除指定元素的效率更高==。 ","date":"2023-11-01","objectID":"/stl/:2:0","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"pair C++ STL 标准库提供了 pair 类模板，其专门用来将 2 个普通元素 first 和 second（可以是 C++ 基本数据类型、结构体、类自定的类型）创建成一个新元素 \u003cfirst, second\u003e。 通过其构成的元素格式不难看出，使用 pair 类模板来创建 “键值对” 形式的元素，再合适不过。 pair初始化 使用花括号 #include \u003cutility\u003e int main() { std::pair\u003cint, double\u003e myPair = {42, 3.14}; // 或者可以使用构造函数的简化语法 // std::pair\u003cint, double\u003e myPair{42, 3.14}; // 使用 myPair 进行操作... return 0; } 使用构造函数 #include \u003cutility\u003e int main() { std::pair\u003cint, double\u003e myPair(42, 3.14); // 使用 myPair 进行操作... return 0; } 使用make_pair #include \u003cutility\u003e int main() { std::pair\u003cint, double\u003e myPair = std::make_pair(42, 3.14); // 使用 myPair 进行操作... return 0; } 通过pair.first和pair.second分别访问K和V ","date":"2023-11-01","objectID":"/stl/:2:1","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"map map 做为 关联容器 的一种，存储的都是 pair 对象，也就是用 pair 类模板创建的键值对。其中，各个键值对的键和值可以是任意数据类型，包括 C++ 基本数据类型（int、double 等）、使用结构体或类自定义的类型。 通常情况下，map 容器中存储的各个键值对都选用 string 字符串作为键的类型。 与此同时，在使用 map 容器存储多个键值对时，该容器会自动根据各键值对的键的大小，按照既定的规则进行排序。默认情况下，map 容器选用 std::less\u003cT\u003e 排序规则（其中 T 表示键的数据类型），其会根据键的大小对所有键值对做升序排序。 当然，根据实际情况的需要，我们可以手动指定 map 容器的排序规则，既可以选用 STL 标准库中提供的其它排序规则（比如 std::greater\u003cT\u003e），也可以自定义排序规则。 另外需要注意的是，使用 map 容器存储的各个键值对，==键的值既不能重复也不能被修改==。换句话说，map 容器中存储的各个键值对不仅键的值独一无二，键的类型也会用 const 修饰，这意味着只要键值对被存储到 map 容器中，其键的值将不能再做任何修改。 map 容器存储的都是 pair 类型的键值对元素，更确切的说，该容器存储的都是 pair\u003cconst K, T\u003e 类型（其中 K 和 T 分别表示键和值的数据类型）的键值对元素。 map初始化 使用初始化列表： 使用迭代器范围初始化： 使用插入操作： #include \u003cmap\u003e int main() { std::map\u003cint, std::string\u003e myMap; myMap.insert(std::make_pair(1, \"one\")); myMap.insert(std::make_pair(2, \"two\")); myMap.insert(std::make_pair(3, \"three\")); // 使用 myMap 进行操作... return 0; } begin() 返回指向容器中第一个（注意，是已排好序的第一个）键值对的双向迭代器。如果 map 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 end() 返回指向容器最后一个元素（注意，是已排好序的最后一个）所在位置后一个位置的双向迭代器，通常和 begin() 结合使用。如果 map 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 rbegin() 返回指向最后一个（注意，是已排好序的最后一个）元素的反向双向迭代器。如果 map 容器用 const 限定，则该方法返回的是 const 类型的反向双向迭代器。 rend() 返回指向第一个（注意，是已排好序的第一个）元素所在位置前一个位置的反向双向迭代器。如果 map 容器用 const 限定，则该方法返回的是 const 类型的反向双向迭代器。 cbegin() 和 begin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的键值对。 cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的键值对。 crbegin() 和 rbegin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的键值对。 crend() 和 rend() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的键值对。 find(key) 在 map 容器中查找键为 key 的键值对，如果成功找到，则返回指向该键值对的双向迭代器；反之，则返回和 end() 方法一样的迭代器。另外，如果 map 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 lower_bound(key) 返回一个指向当前 map 容器中第一个大于或等于 key 的键值对的双向迭代器。如果 map 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 upper_bound(key) 返回一个指向当前 map 容器中第一个大于 key 的键值对的迭代器。如果 map 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 equal_range(key) 该方法返回一个 pair 对象（包含 2 个双向迭代器），其中 pair.first 和 lower_bound() 方法的返回值等价，pair.second 和 upper_bound() 方法的返回值等价。也就是说，该方法将返回一个范围，该范围中包含的键为 key 的键值对（map 容器键值对唯一，因此该范围最多包含一个键值对）。 empty() 若容器为空，则返回 true；否则 false。 size() 返回当前 map 容器中存有键值对的个数。 max_size() 返回 map 容器所能容纳键值对的最大个数，不同的操作系统，其返回值亦不相同。 operator[] map容器重载了 [] 运算符，只要知道 map 容器中某个键值对的键的值，就可以向获取数组中元素那样，通过键直接获取对应的值。 at(key) 找到 map 容器中 key 键对应的值，如果找不到，该函数会引发 out_of_range 异常。 insert() 向 map 容器中插入键值对。 erase() 删除 map 容器指定位置、指定键（key）值或者指定区域内的键值对。后续章节还会对该方法做重点讲解。 swap() 交换 2 个 map 容器中存储的键值对，这意味着，操作的 2 个键值对的类型必须相同。 clear() 清空 map 容器中所有的键值对，即使 map 容器的 size() 为 0。 emplace() 在当前 map 容器中的指定位置处构造新键值对。其效果和插入键值对一样，但效率更高。 emplace_hint() 在本质上和 emplace() 在 map 容器中构造新键值对的方式是一样的，不同之处在于，使用者必须为该方法提供一个指示键值对生成位置的迭代器，并作为该方法的第一个参数。 count(key) 在当前 map 容器中，查找键为 key 的键值对的个数并返回。注意，由于 map 容器中各键值对的键的值是唯一的，因此该函数的返回值最大为 1。 ","date":"2023-11-01","objectID":"/stl/:2:2","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"set STL 中的 set 也是一种 关联式容器，其用来存储一组不重复的元素集合，而且，set 存储元素和 map 以及 multimap 类似，==都是有序的==。 从语法上讲 set 容器并没有强制对存储元素的类型做 const 修饰，即 set 容器中存储的元素的值是可以修改的。但是，C++ 标准为了防止用户修改容器中元素的值，对所有可能会实现此操作的行为做了限制，使得在正常情况下，用户是无法做到修改 set 容器中元素的值的。 对于初学者来说，切勿尝试直接修改 set 容器中已存储元素的值，这很有可能破坏 set 容器中元素的有序性，最正确的修改 set 容器中元素值的做法是：先删除该元素，然后再添加一个修改后的元素。 set初始化 #使用初始化列表 std::set\u003cint\u003e mySet = {1, 2, 3, 4, 5}; #使用插入初始化 std::set\u003cint\u003e mySet; mySet.insert(1); mySet.insert(2); mySet.insert(3); mySet.insert(4); mySet.insert(5); 成员方法 功能 begin() 返回指向容器中第一个（注意，是已排好序的第一个）元素的双向迭代器。如果 set 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 end() 返回指向容器最后一个元素（注意，是已排好序的最后一个）所在位置后一个位置的双向迭代器，通常和 begin() 结合使用。如果 set 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 rbegin() 返回指向最后一个（注意，是已排好序的最后一个）元素的反向双向迭代器。如果 set 容器用 const 限定，则该方法返回的是 const 类型的反向双向迭代器。 rend() 返回指向第一个（注意，是已排好序的第一个）元素所在位置前一个位置的反向双向迭代器。如果 set 容器用 const 限定，则该方法返回的是 const 类型的反向双向迭代器。 cbegin() 和 begin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的元素值。 cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的元素值。 crbegin() 和 rbegin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的元素值。 crend() 和 rend() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改容器内存储的元素值。 find(val) 在 set 容器中查找值为 val 的元素，如果成功找到，则返回指向该元素的双向迭代器；反之，则返回和 end() 方法一样的迭代器。另外，如果 set 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 lower_bound(val) 返回一个指向当前 set 容器中第一个大于或等于 val 的元素的双向迭代器。如果 set 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 upper_bound(val) 返回一个指向当前 set 容器中第一个大于 val 的元素的迭代器。如果 set 容器用 const 限定，则该方法返回的是 const 类型的双向迭代器。 equal_range(val) 该方法返回一个 pair 对象（包含 2 个双向迭代器），其中 pair.first 和 lower_bound() 方法的返回值等价，pair.second 和 upper_bound() 方法的返回值等价。也就是说，该方法将返回一个范围，该范围中包含的值为 val 的元素（set 容器中各个元素是唯一的，因此该范围最多包含一个元素）。 empty() 若容器为空，则返回 true；否则 false。 size() 返回当前 set 容器中存有元素的个数。 max_size() 返回 set 容器所能容纳元素的最大个数，不同的操作系统，其返回值亦不相同。 insert() 向 set 容器中插入元素。 erase() 删除 set 容器中存储的元素。 swap() 交换 2 个 set 容器中存储的所有元素。这意味着，操作的 2 个 set 容器的类型必须相同。 clear() 清空 set 容器中所有的元素，即令 set 容器的 size() 为 0。 emplace() 在当前 set 容器中的指定位置直接构造新元素。其效果和 insert() 一样，但效率更高。 emplace_hint() 在本质上和 emplace() 在 set 容器中构造新元素的方式是一样的，不同之处在于，使用者必须为该方法提供一个指示新元素生成位置的迭代器，并作为该方法的第一个参数。 count(val) 在当前 set 容器中，查找值为 val 的元素的个数，并返回。注意，由于 set 容器中各元素的值是唯一的，因此该函数的返回值最大为 1。 ","date":"2023-11-01","objectID":"/stl/:2:3","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"STL无序关联式容器 ","date":"2023-11-01","objectID":"/stl/:3:0","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"unordered_map unordered_map 容器，直译过来就是 “无序 map 容器” 的意思。所谓 “无序”，指的是 unordered_map 容器不会像 map 容器那样对存储的数据进行排序。 换句话说，unordered_map 容器和 map 容器仅有一点不同，即 map 容器中存储的数据是有序的，而 unordered_map 容器中是无序的。 具体来讲，unordered_map 容器和 map 容器一样，以键值对，即 pair 类型 的形式存储数据，存储的各个键值对的键互不相同且不允许被修改。 但由于 unordered_map 容器底层采用的是哈希表存储结构，该结构本身不具有对数据的排序功能，所以此容器内部不会自行对存储的键值对进行排序。 成员方法 功能 begin() 返回指向容器中第一个键值对的正向迭代器。 end() 返回指向容器中最后一个键值对之后位置的正向迭代器。 cbegin() 和 begin() 功能相同，只不过在其基础上增加了 const 属性，即该方法返回的迭代器不能用于修改容器内存储的键值对。 cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，即该方法返回的迭代器不能用于修改容器内存储的键值对。 empty() 若容器为空，则返回 true；否则 false。 size() 返回当前容器中存有键值对的个数。 max_size() 返回容器所能容纳键值对的最大个数，不同的操作系统，其返回值亦不相同。 operator[key] 该模板类中重载了 [] 运算符，其功能是可以向访问数组中元素那样，只要给定某个键值对的键 key，就可以获取该键对应的值。注意，如果当前容器中没有以 key 为键的键值对，则其会使用该键向当前容器中插入一个新键值对。 at(key) 返回容器中存储的键 key 对应的值，如果 key 不存在，则会抛出 out_of_range 异常。 find(key) 查找以 key 为键的键值对，如果找到，则返回一个指向该键值对的正向迭代器；反之，则返回一个指向容器中最后一个键值对之后位置的迭代器（如果 end() 方法返回的迭代器）。 count(key) 在容器中查找以 key 键的键值对的个数。 equal_range(key) 返回一个 pair 对象，其包含 2 个迭代器，用于表明当前容器中键为 key 的键值对所在的范围。 emplace() 向容器中添加新键值对，效率比 insert() 方法高。 emplace_hint() 向容器中添加新键值对，效率比 insert() 方法高。 insert() 向容器中添加新键值对。 erase() 删除指定键值对。 clear() 清空容器，即删除容器中存储的所有键值对。 swap() 交换 2 个 unordered_map 容器存储的键值对，前提是必须保证这 2 个容器的类型完全相等。 bucket_count() 返回当前容器底层存储键值对时，使用桶（一个线性链表代表一个桶）的数量。 max_bucket_count() 返回当前系统中，unordered_map 容器底层最多可以使用多少桶。 bucket_size(n) 返回第 n 个桶中存储键值对的数量。 bucket(key) 返回以 key 为键的键值对所在桶的编号。 load_factor() 返回 unordered_map 容器中当前的负载因子。负载因子，指的是的当前容器中存储键值对的数量（size()）和使用桶数（bucket_count()）的比值，即 load_factor() = size() / bucket_count()。 max_load_factor() 返回或者设置当前 unordered_map 容器的负载因子。 rehash(n) 将当前容器底层使用桶的数量设置为 n。 reserve() 将存储桶的数量（也就是 bucket_count() 方法的返回值）设置为至少容纳count个元（不超过最大负载因子）所需的数量，并重新整理容器。 hash_function() 返回当前容器使用的哈希函数对象。 ","date":"2023-11-01","objectID":"/stl/:3:1","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"unordered_set unordered_set 容器，可直译为 “无序 set 容器”，即 unordered_set 容器和 set 容器很像，唯一的区别就在于 set 容器会自行对存储的数据进行排序，而 unordered_set 容器不会。 STL unordered_set特征 不再以键值对的形式存储数据，而是直接存储数据的值。 容器内部存储的各个元素的值都互不相等，且不能被修改。 不会对内部存储的数据进行排序。 成员方法 功能 begin() 返回指向容器中第一个元素的正向迭代器。 end(); 返回指向容器中最后一个元素之后位置的正向迭代器。 cbegin() 和 begin() 功能相同，只不过其返回的是 const 类型的正向迭代器。 cend() 和 end() 功能相同，只不过其返回的是 const 类型的正向迭代器。 empty() 若容器为空，则返回 true；否则 false。 size() 返回当前容器中存有元素的个数。 max_size() 返回容器所能容纳元素的最大个数，不同的操作系统，其返回值亦不相同。 find(key) 查找以值为 key 的元素，如果找到，则返回一个指向该元素的正向迭代器；反之，则返回一个指向容器中最后一个元素之后位置的迭代器（如果 end() 方法返回的迭代器）。 count(key) 在容器中查找值为 key 的元素的个数。 equal_range(key) 返回一个 pair 对象，其包含 2 个迭代器，用于表明当前容器中值为 key 的元素所在的范围。 emplace() 向容器中添加新元素，效率比 insert() 方法高。 emplace_hint() 向容器中添加新元素，效率比 insert() 方法高。 insert() 向容器中添加新元素。 erase() 删除指定元素。 clear() 清空容器，即删除容器中存储的所有元素。 swap() 交换 2 个 unordered_map 容器存储的元素，前提是必须保证这 2 个容器的类型完全相等。 bucket_count() 返回当前容器底层存储元素时，使用桶（一个线性链表代表一个桶）的数量。 max_bucket_count() 返回当前系统中，unordered_map 容器底层最多可以使用多少桶。 bucket_size(n) 返回第 n 个桶中存储元素的数量。 bucket(key) 返回值为 key 的元素所在桶的编号。 load_factor() 返回 unordered_map 容器中当前的负载因子。负载因子，指的是的当前容器中存储元素的数量（size()）和使用桶数（bucket_count()）的比值，即 load_factor() = size() / bucket_count()。 max_load_factor() 返回或者设置当前 unordered_map 容器的负载因子。 rehash(n) 将当前容器底层使用桶的数量设置为 n。 reserve() 将存储桶的数量（也就是 bucket_count() 方法的返回值）设置为至少容纳count个元（不超过最大负载因子）所需的数量，并重新整理容器。 hash_function() 返回当前容器使用的哈希函数对象。 ","date":"2023-11-01","objectID":"/stl/:3:2","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"容器的常见操作 ","date":"2023-11-01","objectID":"/stl/:4:0","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"排序 要对 std::vector 进行排序，可以使用标准库提供的排序算法 std::sort。std::sort 函数可以按升序对容器中的元素进行排序。以下是对 std::vector 进行排序的示例代码： #include \u003ciostream\u003e #include \u003cvector\u003e #include \u003calgorithm\u003e int main() { std::vector\u003cint\u003e myVector = {5, 2, 3, 1, 4}; // 使用 std::sort 对容器进行排序 std::sort(myVector.begin(), myVector.end()); // 输出排序后的结果 for (const auto\u0026 element : myVector) { std::cout \u003c\u003c element \u003c\u003c \" \"; } std::cout \u003c\u003c std::endl; return 0; } ","date":"2023-11-01","objectID":"/stl/:4:1","tags":null,"title":"STL","uri":"/stl/"},{"categories":["c++"],"content":"遍历 迭代器遍历unordered_map，迭代器是一个类似指针的对象。在 C++ 中，迭代器是一种抽象的概念，它提供了一组操作符（如 * 和 ++）来访问容器中的元素。迭代器可以被视为容器内部的指针，它指向容器中的某个元素或位置。 对于 std::unordered_map 容器，myMap.begin() 返回的是一个迭代器，指向容器中第一个键值对的位置。通过解引用迭代器，可以获取该位置上的键值对。 遍历vector容器，通过* 解引用 #include \u003ciostream\u003e #include \u003cvector\u003e int main() { std::vector\u003cint\u003e myVector = {1, 2, 3, 4, 5}; // 使用迭代器遍历容器 for (auto it = myVector.begin(); it != myVector.end(); ++it) { std::cout \u003c\u003c *it \u003c\u003c \" \"; } std::cout \u003c\u003c std::endl; return 0; } ","date":"2023-11-01","objectID":"/stl/:4:2","tags":null,"title":"STL","uri":"/stl/"},{"categories":["6.s081"],"content":"Network ","date":"2023-10-31","objectID":"/6.s081_lab11/:0:0","tags":null,"title":"6.s081_lab11","uri":"/6.s081_lab11/"},{"categories":["6.s081"],"content":"ethernet（以太网地址，MAC地址） 让我从最底层开始，我们先来看一下一个以太网packet的结构是什么。当两个主机非常靠近时，或许是通过相同的线缆连接，或许连接在同一个wifi网络，或许连接到同一个以太网交换机。当局域网中的两个主机彼此间要通信时，最底层的协议是以太网协议。你可以认为Host1通过以太网将Frame发送给Host2。Frame是以太网中用来描述packet的单词，本质上这就是两个主机在以太网上传输的一个个的数据Byte。以太网协议会在Frame中放入足够的信息让主机能够识别彼此，并且识别这是不是发送给自己的Frame。每个以太网packet在最开始都有一个Header，其中包含了3个数据。Header之后才是payload数据。Header中的3个数据是：目的以太网地址，源以太网地址，以及packet的类型。 每一个以太网地址都是48bit的数字，这个数字唯一识别了一个网卡。packet的类型会告诉接收端的主机该如何处理这个packet。接收端主机侧更高层级的网络协议会按照packet的类型检查并处理以太网packet中的payload。 整个以太网packet，包括了48bit+48bit的以太网地址，16bit的类型，以及任意长度的payload这些都是通过线路传输。除此之外，虽然对于软件来说是不可见的，但是在packet的开头还有被硬件识别的表明packet起始的数据（注，Preamble + SFD），在packet的结束位置还有几个bit表明packet的结束（注，FCS）。packet的开头和结束的标志不会被系统内核所看到，其他的部分会从网卡送到系统内核。 如果你们查看了这门课程的最后一个lab，你们可以发现我们提供的代码里面包括了一些新的文件，其中包括了kernel/net.h，这个文件中包含了大量不同网络协议的packet header的定义。上图中的代码包含了以太网协议的定义。我们提供的代码使用了这里结构体的定义来解析收到的以太网packet，进而获得目的地址和类型值（注，实际中只需要对收到的raw data指针强制类型转换成结构体指针就可以完成解析）。 学生提问：硬件用来识别以太网packet的开头和结束的标志是不是类似于lab中的End of Packets？ Robert教授：并不是的，EOP是帮助驱动和网卡之间通信的机制。这里的开头和结束的标志是在线缆中传输的电信号或者光信号，这些标志位通常在一个packet中是不可能出现的。以结束的FCS为例，它的值通常是packet header和payload的校验和，可以用来判断packet是否合法。 有关以太网48bit地址，是为了给每一个制造出来的网卡分配一个唯一的ID，所以这里有大量的可用数字。这里48bit地址中，前24bit表示的是制造商，每个网卡制造商都有自己唯一的编号，并且会出现在前24bit中。后24bit是由网卡制造商提供的任意唯一数字，通常网卡制造商是递增的分配数字。所以，如果你从一个网卡制造商买了一批网卡，每个网卡都会被写入属于自己的地址，并且如果你查看这些地址，你可以发现，这批网卡的高24bit是一样的，而低24bit极有可能是一些连续的数字。 虽然以太网地址是唯一的，但是出了局域网，它们对于定位目的主机的位置是没有帮助的。如果网络通信的目的主机在同一个局域网，那么目的主机会监听发给自己的地址的packet。但是如果网络通信发生在两个国家的主机之间，你需要使用一个不同的寻址方法，这就是IP地址的作用。 在实际中，你可以使用tcpdump来查看以太网packet。这将会是lab的一部分。下图是tcpdump的一个输出： tcpdump输出了很多信息，其中包括： 接收packet的时间 第一行的剩下部分是可读的packet的数据 接下来的3行是收到packet的16进制数 如果按照前面以太网header的格式，可以发现packet中： 前48bit是一个广播地址，0xffffffffffff。广播地址是指packet需要发送给局域网中的所有主机。 之后的48bit是发送主机的以太网地址，我们并不能从这个地址发现什么，实际上这个地址是运行在QEMU下的XV6生成的地址，所以地址中的前24bit并不是网卡制造商的编号，而是QEMU编造的地址。 接下来的16bit是以太网packet的类型，这里的类型是0x0806，对应的协议是ARP。 剩下的部分是ARP packet的payload。 ","date":"2023-10-31","objectID":"/6.s081_lab11/:1:0","tags":null,"title":"6.s081_lab11","uri":"/6.s081_lab11/"},{"categories":["6.s081"],"content":"ARP 下一个与以太网通信相关的协议是ARP。在以太网层面，每个主机都有一个以太网地址。但是为了能在互联网上通信，你需要有32bit的IP地址。为什么需要IP地址呢？因为IP地址有额外的含义。IP地址的高位bit包含了在整个互联网中，这个packet的目的地在哪。所以IP地址的高位bit对应的是网络号，虽然实际上要更复杂一些，但是你可以认为互联网上的每一个网络都有一个唯一的网络号。路由器会检查IP地址的高bit位，并决定将这个packet转发给互联网上的哪个路由器。IP地址的低bit位代表了在局域网中特定的主机。当一个经过互联网转发的packet到达了局域以太网，我们需要从32bit的IP地址，找到对应主机的48bit以太网地址。这里是通过一个动态解析协议完成的，也就是Address Resolution Protocol，ARP协议。 当一个packet到达路由器并且需要转发给同一个以太网中的另一个主机，或者一个主机将packet发送给同一个以太网中的另一个主机时，发送方首先会在局域网中广播一个ARP packet，来表示任何拥有了这个32bit的IP地址的主机，请将你的48bit以太网地址返回过来。如果相应的主机存在并且开机了，它会向发送方发送一个ARP response packet。 下图是一个ARP packet的格式： 它会出现在一个以太网packet的payload中。所以你们看到的将会是这样的结构：首先是以太网header，它包含了48bit的目的以太网地址，48bit的源以太网地址，16bit的类型；之后的以太网的payload会是ARP packet，包含了上图的内容。 接收到packet的主机通过查看以太网header中的16bit类型可以知道这是一个ARP packet。在ARP中类型值是0x0806。通过识别类型，接收到packet的主机就知道可以将这个packet发送给ARP协议处理代码。 有关ARP packet的内容，包含了不少信息，但是基本上就是在说，现在有一个IP地址，我想将它转换成以太网地址，如果你拥有这个IP地址，请响应我。 同样的，我们也可以通过tcpdump来查看这些packet。在网络的lab中，XV6会在QEMU模拟的环境下发送IP packet。所以你们可以看到在XV6和其他主机之间有ARP的交互。下图中第一个packet是我的主机想要知道XV6主机的以太网地址，第二个packet是XV6在收到了第一个packet之后，并意识到自己是IP地址的拥有者，然后返回response。 tcpdump能够解析出ARP packet，并将数据打印在第一行。对应ARP packet的格式，在第一个packet中，10.0.2.2是SIP，10.0.2.15是DIP。在第二个packet中，52:54:00:12:34:56对应SHA。 同时，我们也可以自己分析packet的原始数据。对于第一个packet： 前14个字节是以太网header，包括了48bit目的以太网地址，48bit源以太网地址，16bit类型。 从后往前看，倒数4个字节是TIP，也就是发送方想要找出对应以太网地址的IP地址。每个字节对应了IP地址的一块，所以0a00 020f对应了IP地址10.0.2.15。 再向前数6个字节，是THA，也就是目的地的以太网地址，现在还不知道所以是全0。 再向前数4个字节是SIP，也就是发送方的IP地址，0a000202对应了IP地址10.0.2.2。 再向前数6个字节是SHA，也就是发送方的以太网地址。 剩下的8个字节表明了我们感兴趣的是以太网和IP地址格式。 第二个packet是第一个packet的响应。 学生提问：ethernet header中已经包括了发送方的以太网地址，为什么ARP packet里面还要包含发送方的以太网地址？ Robert教授：我并不清楚为什么ARP packet里面包含了这些数据，我认为如果你想的话是可以精简一下ARP packet。或许可以这么理解，ARP协议被设计成也可以用在其他非以太网的网络中，所以它被设计成独立且不依赖其他信息，所以ARP packet中包含了以太网地址。现在我们是在以太网中发送ARP packet，以太网packet也包含了以太网地址，所以，如果在以太网上运行ARP，这些信息是冗余的。但是如果在其他的网络上运行ARP，你或许需要这些信息，因为其他网络的packet中并没有包含以太网地址。 学生提问：tcpdump中原始数据的右侧是什么内容？ Robert教授：这些是原始数据对应的ASCII码，“.”对应了一个字节并没有相应的ASCII码，0x52对应了R，0x55对应了U。当我们发送的packet包含了ASCII字符时，这里的信息会更加有趣。 我希望你们在刚刚的讨论中注意到这一点，网络协议和网络协议header是嵌套的。我们刚刚看到的是一个packet拥有了ethernet header和ethernet payload。在ethernet payload中，首先出现的是ARP header，对于ARP来说并没有的payload。但是在ethernet packet中还可以包含其他更复杂的结构，比如说ethernet payload中包含一个IP packet，IP packet中又包含了一个UDP packet，所以IP header之后是UDP header。如果在UDP中包含另一个协议，那么UDP payload中又可能包含其他的packet，例如DNS packet。所以发送packet的主机会按照这样的方式构建packet：DNS相关软件想要在UDP协议之上构建一个packet；UDP相关软件会将UDP header挂在DNS packet之前，并在IP协议之上构建另一个packet；IP相关的软件会将IP heade挂在UDP packet之前；最后Ethernet相关的软件会将Ethernet header挂在IP header之前。所以整个packet是在发送过程中逐渐构建起来的。 类似的，当一个操作系统收到了一个packet，它会先解析第一个header并知道这是Ethernet，经过一些合法性检查之后，Ethernet header会被剥离，操作系统会解析下一个header。在Ethernet header中包含了一个类型字段，它表明了该如何解析下一个header。同样的在IP header中包含了一个protocol字段，它也表明了该如何解析下一个header。 软件会解析每个header，做校验，剥离header，并得到下一个header。一直重复这个过程直到得到最后的数据。这就是嵌套的packet header。 ","date":"2023-10-31","objectID":"/6.s081_lab11/:2:0","tags":null,"title":"6.s081_lab11","uri":"/6.s081_lab11/"},{"categories":["6.s081"],"content":"Internet Ethernet header足够在一个局域网中将packet发送到一个host。如果你想在局域网发送一个IP packet，那么你可以使用ARP获得以太网地址。但是IP协议更加的通用，IP协议能帮助你向互联网上任意位置发送packet。下图是一个IP packet的header，你们可以在lab配套的代码中的net.h文件找到。 如果IP packet是通过以太网传输，那么你可以看到，在一个以太网packet中，最开始是目的以太网地址，源以太网地址，以太网类型是0x0800，之后是IP header，最后是IP payload。 在一个packet发送到世界另一端的网络的过程中，IP header会被一直保留，而Ethernet header在离开本地的以太网之后会被剥离。或许packet在被路由的过程中，在每一跳（hop）会加上一个新的Ethernet header。但是IP header从源主机到目的主机的过程中会一直保留。 IP header具有全局的意义，而Ethernet header只在单个局域网有意义。所以IP header必须包含足够的信息，这样才能将packet传输给互联网上遥远的另一端。对于我们来说，关键的信息是三个部分，目的IP地址（ip_dst），源IP地址（ip_src）和协议（ip_p）。目的IP地址是我们想要将packet送到的目的主机的IP地址。地址中的高bit位是网络号，它会帮助路由器完成路由。IP header中的协议字段会告诉目的主机如何处理IP payload。 如果你们看到过MIT的IP地址，你们可以看到IP地址是18.x.x.x，虽然最近有些变化，但是在很长一段时间18是MIT的网络号。所以MIT的大部分主机的IP地址最高字节就是18。全世界的路由器在看到网络号18的时候，就知道应该将packet路由到离MIT更近的地方。 接下来我们看一下包含了IP packet的tcpdump输出。 因为这个IP packet是在以太网上传输，所以它包含了以太网header。呃……，实际上这个packet里面有点问题，我不太确定具体的原因是什么，但是Ethernet header中目的以太网地址不应该是全f，因为全f是广播地址，它会导致packet被发送到所有的主机上。一个真实网络中两个主机之间的packet，不可能出现这样的以太网地址。所以我提供的针对network lab的方案，在QEMU上运行有点问题。不管怎么样，我们可以看到以太网目的地址，以太网源地址，以及以太网类型0x0800。0x0800表明了Ethernet payload是一个IP packet。 IP header的长度是20个字节，所以中括号内的是IP header， 从后向前看： 目的IP地址是0x0a000202，也就是10.0.2.2。 源IP地址是0x0a00020f，也就是10.0.2.15。 再向前有16bit的checksum，也就是0x3eae。IP相关的软件需要检查这个校验和，如果结果不匹配应该丢包。 再向前一个字节是protocol，0x11对应的是10进制17，表明了下一层协议是UDP 其他的就是我们不太关心的一些字段了，例如packet的长度。 IP header中的protocol字段告诉了目的主机的网络协议栈，这个packet应该被UDP软件处理。 ","date":"2023-10-31","objectID":"/6.s081_lab11/:3:0","tags":null,"title":"6.s081_lab11","uri":"/6.s081_lab11/"},{"categories":["6.s081"],"content":"UDP IP header足够让一个packet传输到互联网上的任意一个主机，但是我们希望做的更好一些。每一个主机都运行了大量需要使用网络的应用程序，所以我们需要有一种方式能区分一个packet应该传递给目的主机的哪一个应用程序，而IP header明显不包含这种区分方式。有一些其他的协议完成了这里的区分工作，其中一个是TCP，它比较复杂，而另一个是UDP。TCP不仅帮助你将packet发送到了正确的应用程序，同时也包含了序列号等用来检测丢包并重传的功能，这样即使网络出现问题，数据也能完整有序的传输。相比之下，UDP就要简单的多，它以一种“尽力而为”的方式将packet发送到目的主机，除此之外不提供任何其他功能。 UDP header中最关键的两个字段是sport源端口和dport目的端口。 当你的应用程序需要发送或者接受packet，它会使用socket API，这包含了一系列的系统调用。一个进程可以使用socket API来表明应用程序对于特定目的端口的packet感兴趣。当应用程序调用这里的系统调用，操作系统会返回一个文件描述符。每当主机收到了一个目的端口匹配的packet，这个packet会出现在文件描述符中，之后应用程序就可以通过文件描述符读取packet。 这里的端口分为两类，一类是常见的端口，例如53对应的是DNS服务的端口，如果你想向一个DNS server发请求，你可以发送一个UDP packet并且目的端口是53。除此之外，很多常见的服务都占用了特定的端口。除了常见端口，16bit数的剩下部分被用来作为匿名客户端的源端口。比如说，我想向一个DNS server的53端口发送一个packet，目的端口会是53，但是源端口会是一个本地随机选择的端口，这个随机端口会与本地的应用程序的socket关联。所以当DNS server向本地服务器发送一个回复packet，它会将请求中的源端口拷贝到回复packet的目的端口，再将回复packet发送回本地的服务器。本地服务器会使用这个端口来确定应该将packet发送给哪个应用程序。 接下来我们看一下UDP packet的tcpdump输出。首先，我们同样会有一个以太网Header，以及20字节的IP header。IP header中的0x11表明这个packet的IP协议号是17，这样packet的接收主机就知道应该使用UDP软件来处理这个packet。 接下来的8个字节是UDP header。这里的packet是由lab代码生成的packet，所以它并没有包含常见的端口，源端口是0x0700，目的端口是0x6403。第4-5个字节是长度，第6-7个字节是校验和。XV6的UDP软件并没有生成UDP的校验和。 UDP header之后就是UDP的payload。在这个packet中，应用程序发送的是ASCII文本，所以我们可以从右边的ASCII码看到，内容是“a.message.from.xv6”。所以ASCII文本放在了一个UDP packet中，然后又放到了一个IP packet中，然后又放到了一个Ethernet packet中。最后发布到以太网上。 学生提问：当你发送一个packet给一个主机，但是你又不知道它的以太网地址，这个packet是不是会被送到路由器，之后再由路由器来找到以太网地址？ Robert教授：如果你发送packet到一个特定的IP地址，你的主机会先检查packet的目的IP地址来判断目的主机是否与你的主机在同一个局域网中。如果是的话，你的主机会直接使用ARP来将IP地址翻译成以太网地址，再将packet通过以太网送到目的主机。更多的场景是，我们将一个packet发送到互联网上某个主机。这时，你的主机会将packet发送到局域网上的路由器，路由器会检查packet的目的IP地址，并根据路由表选择下一个路由器，将packet转发给这个路由器。这样packet一跳一跳的在路由器之间转发，最终离目的主机越来越近。 学生提问：对于packet的长度有限制吗？ Robert教授：有的。这里有几个不同的限制，每一个底层的网络技术，例如以太网，都有能传输packet的上限。今天我们要讨论的论文基于以太网最大可传输的packet是1500字节。最新的以太网可以支持到9000或者10000字节的最大传输packet。为什么不支持传输无限长度的packet呢？这里有几个原因： 发送无限长度的packet的时间可能要很长，期间线路上会有信号噪音和干扰，所以在发送packet的时候可能会收到损坏的bit位。基本上每一种网络技术都会在packet中带上某种校验和或者纠错码，但是校验和也好，纠错码也好，只能在一定长度的bit位内稳定的检测错误。如果packet长度增加，遗漏错误的可能性就越来越大。所以一个校验和的长度，例如16bit或者32bit，限制了传输packet的最大长度。 另一个限制是，如果发送巨大的packet，传输路径上的路由器和主机需要准备大量的buffer来接收packet。这里的代价又比较高，因为较难管理一个可变长度的buffer，管理一个固定长度的buffer是最方便的。而固定长度的buffer要求packet的最大长度不会太大。 所以，以太网有1500或者9000字节的最大packet限制。除此之外，所有的协议都有长度字段，例如UDP的长度字段是16bit。所以即使以太网支持传输更大的packet，协议本身对于数据长度也有限制。 以上就是UDP的介绍。在lab的最后你们会通过实验提供的代码来向谷歌的DNS server发送一个查询，收到回复之后代码会打印输出。你们需要在设备驱动侧完成以太网数据的处理。 lab11 YOUR JOB 您的工作是在***kernel/e1000.c***中完成e1000_transmit()和e1000_recv()，以便驱动程序可以发送和接收数据包。当make grade表示您的解决方案通过了所有测试时，您就完成了。 [!TIP] 在编写代码时，您会发现自己参考了《E1000软件开发人员手册》。以下部分可能特别有用： Section 2是必不可少的，它概述了整个设备。 Section 3.2概述了数据包接收。 Section 3.3与Section 3.4一起概述了数据包传输。 Section 13概述了E1000使用的寄存器。 Section 14可能会帮助您理解我们提供的init代码。 浏览《E1000软件开发人员手册》。本手册涵盖了几个密切相关的以太网控制器。QEMU模拟82540EM。现在浏览第2章，了解该设备。要编写驱动程序，您需要熟悉第3章和第14章以及第4.1节（虽然不包括4.1的子节）。你还需要参考第13章。其他章节主要介绍你的驱动程序不必与之交互的E1000组件。一开始不要担心细节；只需了解文档的结构，就可以在以后找到内容。E1000具有许多高级功能，其中大部分您可以忽略。完成这个实验只需要一小部分基本功能。 我们在***e1000.c***中提供的e1000_init()函数将E1000配置为读取要从RAM传输的数据包，并将接收到的数据包写入RAM。这种技术称为DMA，用于直接内存访问，指的是E1000硬件直接向RAM写入和读取数据包。 由于数据包突发到达的速度可能快于驱动程序处理数据包的速度，因此e1000_init()为E1000提供了多个缓冲区，E1000可以将数据包写入其中。E1000要求这些缓冲区由RAM中的“描述符”数组描述；每个描述符在RAM中都包含一个地址，E1000可以在其中写入接收到的数据包。struct rx_desc描述描述符格式。描述符数组称为接收环或接收队列。它是一个圆环，在这个意义上，当网卡或驱动程序到达队列的末尾时，它会绕回到数组的开头。e1000_init()使用mbufalloc()为要进行DMA的E1000分配mbuf数据包缓冲区。此外还有一个传输环，驱动程序将需要E1000发送的数据包放入其中。e1000_init()将两个环的大小配置为RX_RING_SIZE和TX_RING_SIZE。 当***net.c***中的网络栈需要发送数据包时，它会调用e1000_transmit()，并使用一个保存要发送的数据包的mbuf作为参数。传输代码必须在TX（传输）环的描述符中放置指向数据包数据的指针。struct tx_desc描述了描述符的格式。您需要确保每个mbuf最终被释放，但只能在E1000完成数据包传输之后（E1000在描述符中设置E1000_TXD_STAT_DD位以指示此情况）。 当当E1000从以太网接收到每个包时，它首先将包DMA到下一个RX(接收)环描述符指向的mbuf，然后产生一个中断。e1000_recv()代码必须扫描RX环，并通过调用net_rx()将每个新数据包的mbuf发送到网络栈（在***net.c***中）。然后，您需要分配一个新的mbuf并将其放入描述符中，以便当E1000再次到达RX环中的该点时，它会找到一个新的缓冲区，以便DMA新数据包。 除了在RAM中读取和写入描述符环外，您的驱动程序还需要通过其内存映射控制寄存器与E1000交互，以检测接收到数据包何时可用，并通知E1000驱动程序已经用要发送的数据包填充了一些TX描述符。全局变量regs包含指向E1000第一个控制寄存器的指针；您的驱动程序可以通过将regs索引为数组来获取其他寄存器。您需要特别使用索引E1000_RDT和E1000_TDT。 要测试驱动程序，请在一个窗口中运行make ser","date":"2023-10-31","objectID":"/6.s081_lab11/:4:0","tags":null,"title":"6.s081_lab11","uri":"/6.s081_lab11/"},{"categories":["6.s081"],"content":"提示 首先，将打印语句添加到e1000_transmit()和e1000_recv()，然后运行make server和（在xv6中）nettests。您应该从打印语句中看到，nettests生成对e1000_transmit的调用。 实现e1000_transmit的一些提示： 首先，通过读取E1000_TDT控制寄存器，向E1000询问等待下一个数据包的TX环索引。 然后检查环是否溢出。如果E1000_TXD_STAT_DD未在E1000_TDT索引的描述符中设置，则E1000尚未完成先前相应的传输请求，因此返回错误。 否则，使用mbuffree()释放从该描述符传输的最后一个mbuf（如果有）。 然后填写描述符。m-\u003ehead指向内存中数据包的内容，m-\u003elen是数据包的长度。设置必要的cmd标志（请参阅E1000手册的第3.3节），并保存指向mbuf的指针，以便稍后释放。 最后，通过将一加到E1000_TDT再对TX_RING_SIZE取模来更新环位置。 如果e1000_transmit()成功地将mbuf添加到环中，则返回0。如果失败（例如，没有可用的描述符来传输mbuf），则返回-1，以便调用方知道应该释放mbuf。 实现e1000_recv的一些提示： 首先通过提取E1000_RDT控制寄存器并加一对RX_RING_SIZE取模，向E1000询问下一个等待接收数据包（如果有）所在的环索引。 然后通过检查描述符status部分中的E1000_RXD_STAT_DD位来检查新数据包是否可用。如果不可用，请停止。 否则，将mbuf的m-\u003elen更新为描述符中报告的长度。使用net_rx()将mbuf传送到网络栈。 然后使用mbufalloc()分配一个新的mbuf，以替换刚刚给net_rx()的mbuf。将其数据指针（m-\u003ehead）编程到描述符中。将描述符的状态位清除为零。 最后，将E1000_RDT寄存器更新为最后处理的环描述符的索引。 e1000_init()使用mbufs初始化RX环，您需要通过浏览代码来了解它是如何做到这一点的。 在某刻，曾经到达的数据包总数将超过环大小（16）；确保你的代码可以处理这个问题。 int e1000_transmit(struct mbuf *m) { // // 把m的信息赋值给desc,并把数据放入tx_mbufs中 // // the mbuf contains an ethernet frame; program it into // the TX descriptor ring so that the e1000 sends it. Stash // a pointer so that it can be freed after sending. // acquire(\u0026e1000_lock); // 获取 E1000 的锁，防止多进程同时发送数据出现 race uint32 ind = regs[E1000_TDT]; // 下一个可用的 buffer 的下标 struct tx_desc *desc = \u0026tx_ring[ind]; // 获取 buffer 的描述符，其中存储了关于该 buffer 的各种信息 // 如果该 buffer 中的数据还未传输完，则代表我们已经将环形 buffer 列表全部用完，缓冲区不足，返回错误 if(!(desc-\u003estatus \u0026 E1000_TXD_STAT_DD)) { release(\u0026e1000_lock); return -1; } // 如果该下标仍有之前发送完毕但未释放的 mbuf，则释放 if(tx_mbufs[ind]) { mbuffree(tx_mbufs[ind]); tx_mbufs[ind] = 0; } // 将要发送的 mbuf 的内存地址与长度填写到发送描述符中 desc-\u003eaddr = (uint64)m-\u003ehead; desc-\u003elength = m-\u003elen; // 设置参数，EOP 表示该 buffer 含有一个完整的 packet // RS 告诉网卡在发送完成后，设置 status 中的 E1000_TXD_STAT_DD 位，表示发送完成。 desc-\u003ecmd = E1000_TXD_CMD_EOP | E1000_TXD_CMD_RS; // 保留新 mbuf 的指针，方便后续再次用到同一下标时释放。 tx_mbufs[ind] = m; // 环形缓冲区内下标增加一。 regs[E1000_TDT] = (regs[E1000_TDT] + 1) % TX_RING_SIZE; release(\u0026e1000_lock); return 0; } static void e1000_recv(void) { // // 获取缓冲区的desc并设置rx_mbufs的length,然后将rx_mbufs传递给net_rx // // Check for packets that have arrived from the e1000 // Create and deliver an mbuf for each packet (using net_rx()). // while(1) { // 每次 recv 可能接收多个包 uint32 ind = (regs[E1000_RDT] + 1) % RX_RING_SIZE; struct rx_desc *desc = \u0026rx_ring[ind]; // 如果需要接收的包都已经接收完毕，则退出 if(!(desc-\u003estatus \u0026 E1000_RXD_STAT_DD)) { return; } rx_mbufs[ind]-\u003elen = desc-\u003elength; net_rx(rx_mbufs[ind]); // 传递给上层网络栈。上层负责释放 mbuf // 分配并设置新的 mbuf，供给下一次轮到该下标时使用 rx_mbufs[ind] = mbufalloc(0); desc-\u003eaddr = (uint64)rx_mbufs[ind]-\u003ehead; desc-\u003estatus = 0; regs[E1000_RDT] = ind; } } ","date":"2023-10-31","objectID":"/6.s081_lab11/:5:0","tags":null,"title":"6.s081_lab11","uri":"/6.s081_lab11/"},{"categories":["6.s081"],"content":"lab10 YOUR JOB 您应该实现足够的mmap和munmap功能，以使mmaptest测试程序正常工作。如果mmaptest不会用到某个mmap的特性，则不需要实现该特性。 完成后，您应该会看到以下输出： $ mmaptest mmap_test starting test mmap f test mmap f: OK test mmap private test mmap private: OK test mmap read-only test mmap read-only: OK test mmap read/write test mmap read/write: OK test mmap dirty test mmap dirty: OK test not-mapped unmap test not-mapped unmap: OK test mmap two files test mmap two files: OK mmap_test: ALL OK fork_test starting fork_test OK mmaptest: all tests succeeded $ usertests usertests starting ... ALL TESTS PASSED $ 提示： 首先，向UPROGS添加_mmaptest，以及mmap和munmap系统调用，以便让**user/mmaptest.c*进行编译。现在，只需从mmap和munmap返回错误。我们在kernel/fcntl.h***中为您定义了PROT_READ等。运行mmaptest，它将在第一次mmap调用时失败。 惰性地填写页表，以响应页错误。也就是说，mmap不应该分配物理内存或读取文件。相反，在usertrap中（或由usertrap调用）的页面错误处理代码中执行此操作，就像在lazy page allocation实验中一样。惰性分配的原因是确保大文件的mmap是快速的，并且比物理内存大的文件的mmap是可能的。 跟踪mmap为每个进程映射的内容。定义与第15课中描述的VMA（虚拟内存区域）对应的结构体，记录mmap创建的虚拟内存范围的地址、长度、权限、文件等。由于xv6内核中没有内存分配器，因此可以声明一个固定大小的VMA数组，并根据需要从该数组进行分配。大小为16应该就足够了。 实现mmap：在进程的地址空间中找到一个未使用的区域来映射文件，并将VMA添加到进程的映射区域表中。VMA应该包含指向映射文件对应struct file的指针；mmap应该增加文件的引用计数，以便在文件关闭时结构体不会消失（提示：请参阅filedup）。运行mmaptest：第一次mmap应该成功，但是第一次访问被mmap的内存将导致页面错误并终止mmaptest。 添加代码以导致在mmap的区域中产生页面错误，从而分配一页物理内存，将4096字节的相关文件读入该页面，并将其映射到用户地址空间。使用readi读取文件，它接受一个偏移量参数，在该偏移处读取文件（但必须lock/unlock传递给readi的索引结点）。不要忘记在页面上正确设置权限。运行mmaptest；它应该到达第一个munmap。 实现munmap：找到地址范围的VMA并取消映射指定页面（提示：使用uvmunmap）。如果munmap删除了先前mmap的所有页面，它应该减少相应struct file的引用计数。如果未映射的页面已被修改，并且文件已映射到MAP_SHARED，请将页面写回该文件。查看filewrite以获得灵感。 理想情况下，您的实现将只写回程序实际修改的MAP_SHARED页面。RISC-V PTE中的脏位（D）表示是否已写入页面。但是，mmaptest不检查非脏页是否没有回写；因此，您可以不用看D位就写回页面。 修改exit将进程的已映射区域取消映射，就像调用了munmap一样。运行mmaptest；mmap_test应该通过，但可能不会通过fork_test。 修改fork以确保子对象具有与父对象相同的映射区域。不要忘记增加VMA的struct file的引用计数。在子进程的页面错误处理程序中，可以分配新的物理页面，而不是与父级共享页面。后者会更酷，但需要更多的实施工作。运行mmaptest；它应该通过mmap_test和fork_test。 本实验是实现一个内存映射文件的功能，将文件映射到内存中，从而在与文件交互时减少磁盘操作。 (1). 根据提示1，首先是配置mmap和munmap系统调用，此前已进行过多次类似流程，不再赘述。在***kernel/fcntl.h***中定义了宏，只有在定义了LAB_MMAP时这些宏才生效，而LAB_MMAP是在编译时在命令行通过gcc的-D参数定义的 void* mmap(void* addr, int length, int prot, int flags, int fd, int offset); int munmap(void* addr, int length); (2). 根据提示3，定义VMA结构体，并添加到进程结构体中 #define NVMA 16 // 虚拟内存区域结构体 struct vm_area { int used; // 是否已被使用 uint64 addr; // 起始地址 int len; // 长度 int prot; // 权限 int flags; // 标志位 int vfd; // 对应的文件描述符 struct file* vfile; // 对应文件 int offset; // 文件偏移，本实验中一直为0 }; struct proc { ... struct vm_area vma[NVMA]; // 虚拟内存区域 } (3). 在allocproc中将vma数组初始化为全0 static struct proc* allocproc(void) { ... found: ... memset(\u0026p-\u003evma, 0, sizeof(p-\u003evma)); return p; } (4). 根据提示2、3、4，参考lazy实验中的分配方法（将当前p-\u003esz作为分配的虚拟起始地址，但不实际分配物理页面），此函数写在***sysfile.c***中就可以使用静态函数argfd同时解析文件描述符和struct file uint64 sys_mmap(void) { uint64 addr; int length; int prot; int flags; int vfd; struct file* vfile; int offset; uint64 err = 0xffffffffffffffff; // 获取系统调用参数 if(argaddr(0, \u0026addr) \u003c 0 || argint(1, \u0026length) \u003c 0 || argint(2, \u0026prot) \u003c 0 || argint(3, \u0026flags) \u003c 0 || argfd(4, \u0026vfd, \u0026vfile) \u003c 0 || argint(5, \u0026offset) \u003c 0) return err; // 实验提示中假定addr和offset为0，简化程序可能发生的情况 if(addr != 0 || offset != 0 || length \u003c 0) return err; // 文件不可写则不允许拥有PROT_WRITE权限时映射为MAP_SHARED if(vfile-\u003ewritable == 0 \u0026\u0026 (prot \u0026 PROT_WRITE) != 0 \u0026\u0026 flags == MAP_SHARED) return err; struct proc* p = myproc(); // 没有足够的虚拟地址空间 if(p-\u003esz + length \u003e MAXVA) return err; // 遍历查找未使用的VMA结构体 for(int i = 0; i \u003c NVMA; ++i) { if(p-\u003evma[i].used == 0) { p-\u003evma[i].used = 1; p-\u003evma[i].addr = p-\u003esz; p-\u003evma[i].len = length; p-\u003evma[i].flags = flags; p-\u003evma[i].prot = prot; p-\u003evma[i].vfile = vfile; p-\u003evma[i].vfd = vfd; p-\u003evma[i].offset = offset; // 增加文件的引用计数 filedup(vfile); p-\u003esz += length; return p-\u003evma[i].addr; } } return err; } (5). 根据提示5，此时访问对应的页面就会产生页面错误，需要在usertrap中进行处理，主要完成三项工作：分配物理页面，读取文件内容，添加映射关系 void usertrap(void) { ... if(cause == 8) { ... } else if((which_dev = devintr()) != 0){ // ok } else if(cause == 13 || cause == 15) { #ifdef LAB_MMAP // 读取产生页面故障的虚拟地址，并判断是否位于有效区间 uint64 fault_va = r_stval(); if(PGROUNDUP(p-\u003etrapframe-\u003esp) - 1 \u003c fault_va \u0026\u0026 fault_va \u003c ","date":"2023-10-31","objectID":"/6.s081_lab10/:0:0","tags":null,"title":"6.s081_lab10","uri":"/6.s081_lab10/"},{"categories":["6.s081"],"content":"Chapter 8 文件系统的目的是组织和存储数据。文件系统通常支持用户和应用程序之间的数据共享，以及持久性，以便在重新启动后数据仍然可用。 xv6文件系统提供类似于Unix的文件、目录和路径名（参见第1章），并将其数据存储在virtio磁盘上以便持久化（参见第4章）。文件系统解决了几个难题： 注：完整计算机中的CPU被支撑硬件包围，其中大部分是以I/O接口的形式。Xv6是以qemu的“-machine virt”选项模拟的支撑硬件编写的。这包括RAM、包含引导代码的ROM、一个到用户键盘/屏幕的串行连接，以及一个用于存储的磁盘。 文件系统需要磁盘上的数据结构来表示目录和文件名称树，记录保存每个文件内容的块的标识，以及记录磁盘的哪些区域是空闲的。 文件系统必须支持崩溃恢复（crash recovery）。也就是说，如果发生崩溃（例如，电源故障），文件系统必须在重新启动后仍能正常工作。风险在于崩溃可能会中断一系列更新，并使磁盘上的数据结构不一致（例如，一个块在某个文件中使用但同时仍被标记为空闲）。 不同的进程可能同时在文件系统上运行，因此文件系统代码必须协调以保持不变量。 访问磁盘的速度比访问内存慢几个数量级，因此文件系统必须保持常用块的内存缓存。 本章的其余部分将解释xv6如何应对这些挑战。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:0:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.1 概述 xv6文件系统实现分为七层，如图8.1所示。磁盘层读取和写入virtio硬盘上的块。缓冲区高速缓存层缓存磁盘块并同步对它们的访问，确保每次只有一个内核进程可以修改存储在任何特定块中的数据。日志记录层允许更高层在一次事务（transaction）中将更新包装到多个块，并确保在遇到崩溃时自动更新这些块（即，所有块都已更新或无更新）。索引结点层提供单独的文件，每个文件表示为一个索引结点，其中包含唯一的索引号（i-number）和一些保存文件数据的块。目录层将每个目录实现为一种特殊的索引结点，其内容是一系列目录项，每个目录项包含一个文件名和索引号。路径名层提供了分层路径名，如***/usr/rtm/xv6/fs.c***，并通过递归查找来解析它们。文件描述符层使用文件系统接口抽象了许多Unix资源（例如，管道、设备、文件等），简化了应用程序员的工作。 文件描述符（File descriptor） 路径名（Pathname） 目录（Directory） 索引结点（Inode） 日志（Logging） 缓冲区高速缓存（Buffer cache） 磁盘（Disk） 图8.1 XV6文件系统的层级 文件系统必须有将索引节点和内容块存储在磁盘上哪些位置的方案。为此，xv6将磁盘划分为几个部分，如图8.2所示。文件系统不使用块0（它保存引导扇区）。块1称为超级块：它包含有关文件系统的元数据（文件系统大小（以块为单位）、数据块数、索引节点数和日志中的块数）。从2开始的块保存日志。日志之后是索引节点，每个块有多个索引节点。然后是位图块，跟踪正在使用的数据块。其余的块是数据块：每个都要么在位图块中标记为空闲，要么保存文件或目录的内容。超级块由一个名为mkfs的单独的程序填充，该程序构建初始文件系统。 本章的其余部分将从缓冲区高速缓存层开始讨论每一层。注意那些在较低层次上精心选择的抽象可以简化较高层次的设计的情况。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:1:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.2 Buffer cache层 Buffer cache有两个任务： 同步对磁盘块的访问，以确保磁盘块在内存中只有一个副本，并且一次只有一个内核线程使用该副本 缓存常用块，以便不需要从慢速磁盘重新读取它们。代码在***bio.c***中。 Buffer cache层导出的主接口主要是bread和bwrite；前者获取一个buf，其中包含一个可以在内存中读取或修改的块的副本，后者将修改后的缓冲区写入磁盘上的相应块。内核线程必须通过调用brelse释放缓冲区。Buffer cache每个缓冲区使用一个睡眠锁，以确保每个缓冲区（因此也是每个磁盘块）每次只被一个线程使用；bread返回一个上锁的缓冲区，brelse释放该锁。 让我们回到Buffer cache。Buffer cache中保存磁盘块的缓冲区数量固定，这意味着如果文件系统请求还未存放在缓存中的块，Buffer cache必须回收当前保存其他块内容的缓冲区。Buffer cache为新块回收最近使用最少的缓冲区。这样做的原因是认为最近使用最少的缓冲区是最不可能近期再次使用的缓冲区 ","date":"2023-10-31","objectID":"/6.s081_lab9/:2:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.3 代码：Buffer cache Buffer cache是以循环双链表表示的缓冲区。main（*kernel/main.c*:27）调用的函数binit使用静态数组buf（*kernel/bio.c*:43-52）中的NBUF个缓冲区初始化列表。对Buffer cache的所有其他访问都通过bcache.head引用链表，而不是buf数组。 struct { struct spinlock lock; struct buf buf[NBUF];//NBUF #define NBUF (MAXOPBLOCKS*3) // size of disk block cache // Linked list of all buffers, through prev/next. // Sorted by how recently the buffer was used. // head.next is most recent, head.prev is least. struct buf head; } bcache; struct buf { int valid; // has data been read from disk? int disk; // does disk \"own\" buf? uint dev; uint blockno;//device block number struct sleeplock lock; uint refcnt; struct buf *prev; // LRU cache list struct buf *next; uchar data[BSIZE]; }; 缓冲区有两个与之关联的状态字段。字段valid表示缓冲区是否包含块的副本。字段disk表示缓冲区内容是否已交给磁盘，这可能会更改缓冲区（例如，将数据从磁盘写入data）。 Bread（*kernel/bio.c*:93）调用bget为给定扇区（*kernel/bio.c*:97）获取缓冲区。如果缓冲区需要从磁盘进行读取，bread会在返回缓冲区之前调用virtio_disk_rw来执行此操作。 Bget（*kernel/bio.c*:59）扫描缓冲区列表，查找具有给定设备和扇区号（*kernel/bio.c*:65-73）的缓冲区。如果存在这样的缓冲区，bget将获取缓冲区的睡眠锁。然后Bget返回锁定的缓冲区。 // Look through buffer cache for block on device dev. // If not found, allocate a buffer. // In either case, return locked buffer. static struct buf* bget(uint dev, uint blockno) { struct buf *b; acquire(\u0026bcache.lock); // Is the block already cached? for(b = bcache.head.next; b != \u0026bcache.head; b = b-\u003enext){ if(b-\u003edev == dev \u0026\u0026 b-\u003eblockno == blockno){ b-\u003erefcnt++; release(\u0026bcache.lock); acquiresleep(\u0026b-\u003elock); return b; } } // Not cached. // Recycle the least recently used (LRU) unused buffer. for(b = bcache.head.prev; b != \u0026bcache.head; b = b-\u003eprev){ if(b-\u003erefcnt == 0) { b-\u003edev = dev; b-\u003eblockno = blockno; b-\u003evalid = 0; b-\u003erefcnt = 1; release(\u0026bcache.lock); acquiresleep(\u0026b-\u003elock); return b; } } panic(\"bget: no buffers\"); } // Return a locked buf with the contents of the indicated block. struct buf* bread(uint dev, uint blockno) { struct buf *b; b = bget(dev, blockno); if(!b-\u003evalid) {//获取缓冲区之后判断该缓冲区是否需要从磁盘读取 virtio_disk_rw(b, 0); b-\u003evalid = 1; } return b; } 如果对于给定的扇区没有缓冲区，bget必须创建一个，这可能会重用包含其他扇区的缓冲区。它再次扫描缓冲区列表，查找未在使用中的缓冲区（b-\u003erefcnt = 0）：任何这样的缓冲区都可以使用。Bget编辑缓冲区元数据以记录新设备和扇区号，并获取其睡眠锁。注意，b-\u003evalid = 0的布置确保了bread将从磁盘读取块数据，而不是错误地使用缓冲区以前的内容。 每个磁盘扇区最多有一个缓存缓冲区是非常重要的，并且因为文件系统使用缓冲区上的锁进行同步，可以确保读者看到写操作。Bget的从第一个检查块是否缓存的循环到第二个声明块现在已缓存（通过设置dev、blockno和refcnt）的循环，一直持有bcache.lock来确保此不变量。这会导致检查块是否存在以及（如果不存在）指定一个缓冲区来存储块具有原子性。 bget在bcache.lock临界区域之外获取缓冲区的睡眠锁是安全的，因为非零b-\u003erefcnt防止缓冲区被重新用于不同的磁盘块。睡眠锁保护块缓冲内容的读写，而bcache.lock保护有关缓存哪些块的信息。 如果所有缓冲区都处于忙碌，那么太多进程同时执行文件系统调用；bget将会panic。一个更优雅的响应可能是在缓冲区空闲之前休眠，尽管这样可能会出现死锁。 一旦bread读取了磁盘（如果需要）并将缓冲区返回给其调用者，调用者就可以独占使用缓冲区，并可以读取或写入数据字节。如果调用者确实修改了缓冲区，则必须在释放缓冲区之前调用bwrite将更改的数据写入磁盘。Bwrite（*kernel/bio.c*:107）调用virtio_disk_rw与磁盘硬件对话。 当调用方使用完缓冲区后，它必须调用brelse来释放缓冲区(brelse是b-release的缩写，这个名字很隐晦，但值得学习：它起源于Unix，也用于BSD、Linux和Solaris）。brelse（*kernel/bio.c*:117）释放睡眠锁并将缓冲区移动到链表的前面（*kernel/bio.c*:128-133）。移动缓冲区会使列表按缓冲区的使用频率排序（意思是释放）：列表中的第一个缓冲区是最近使用的，最后一个是最近使用最少的。bget中的两个循环利用了这一点：在最坏的情况下，对现有缓冲区的扫描必须处理整个列表，但首先检查最新使用的缓冲区（从bcache.head开始，然后是下一个指针），在引用局部性良好的情况下将减少扫描时间。选择要重用的缓冲区时，通过自后向前扫描（跟随prev指针）选择最近使用最少的缓冲区。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:3:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.4 日志层 文件系统设计中最有趣的问题之一是崩溃恢复。出现此问题的原因是，许多文件系统操作都涉及到对磁盘的多次写入，并且在完成写操作的部分子集后崩溃可能会使磁盘上的文件系统处于不一致的状态。例如，假设在文件截断（将文件长度设置为零并释放其内容块）期间发生崩溃。根据磁盘写入的顺序，崩溃可能会留下对标记为空闲的内容块的引用的inode，也可能留下已分配但未引用的内容块。 后者相对来说是良性的，但引用已释放块的inode在重新启动后可能会导致严重问题。重新启动后，内核可能会将该块分配给另一个文件，现在我们有两个不同的文件无意中指向同一块。如果xv6支持多个用户，这种情况可能是一个安全问题，因为旧文件的所有者将能够读取和写入新文件中的块，而新文件的所有者是另一个用户。 Xv6通过简单的日志记录形式解决了文件系统操作期间的崩溃问题。xv6系统调用不会直接写入磁盘上的文件系统数据结构。相反，它会在磁盘上的log（日志）中放置它希望进行的所有磁盘写入的描述。一旦系统调用记录了它的所有写入操作，它就会向磁盘写入一条特殊的commit（提交）记录，表明日志包含一个完整的操作。此时，系统调用将写操作复制到磁盘上的文件系统数据结构。完成这些写入后，系统调用将擦除磁盘上的日志。 如果系统崩溃并重新启动，则在运行任何进程之前，文件系统代码将按如下方式从崩溃中恢复。如果日志标记为包含完整操作，则恢复代码会将写操作复制到磁盘文件系统中它们所属的位置。如果日志没有标记为包含完整操作，则恢复代码将忽略该日志。恢复代码通过擦除日志完成。 为什么xv6的日志解决了文件系统操作期间的崩溃问题？如果崩溃发生在操作提交之前，那么磁盘上的登录将不会被标记为已完成，恢复代码将忽略它，并且磁盘的状态将如同操作尚未启动一样。如果崩溃发生在操作提交之后，则恢复将重播操作的所有写入操作，如果操作已开始将它们写入磁盘数据结构，则可能会重复这些操作。在任何一种情况下，日志都会使操作在崩溃时成为原子操作：恢复后，要么操作的所有写入都显示在磁盘上，要么都不显示。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:4:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.5 日志设计 日志驻留在超级块中指定的已知固定位置。它由一个头块（header block）和一系列更新块的副本（logged block）组成。头块包含一个扇区号数组（每个logged block对应一个扇区号）以及日志块的计数。磁盘上的头块中的计数或者为零，表示日志中没有事务；或者为非零，表示日志包含一个完整的已提交事务，并具有指定数量的logged block。在事务提交（commit）时Xv6才向头块写入数据，在此之前不会写入，并在将logged blocks复制到文件系统后将计数设置为零。因此，事务中途崩溃将导致日志头块中的计数为零；提交后的崩溃将导致非零计数。 注：logged block表示已经记录了操作信息的日志块，而log block仅表示日志块 每个系统调用的代码都指示写入序列的起止，考虑到崩溃，写入序列必须具有原子性。为了允许不同进程并发执行文件系统操作，日志系统可以将多个系统调用的写入累积到一个事务中。因此，单个提交可能涉及多个完整系统调用的写入。为了避免在事务之间拆分系统调用，日志系统==仅在没有文件系统调用==进行时提交。 同时提交多个事务的想法称为组提交（group commit）。组提交减少了磁盘操作的数量，因为成本固定的一次提交分摊了多个操作。组提交还同时为磁盘系统提供更多并发写操作，可能允许磁盘在一个磁盘旋转时间内写入所有这些操作。Xv6的virtio驱动程序不支持这种批处理，但是Xv6的文件系统设计允许这样做。 Xv6在磁盘上留出固定的空间来保存日志。事务中系统调用写入的块总数必须可容纳于该空间。这导致两个后果：任何单个系统调用都不允许写入超过日志空间的不同块。这对于大多数系统调用来说都不是问题，但其中两个可能会写入许多块：write和unlink。一个大文件的write可以写入多个数据块和多个位图块以及一个inode块；unlink大文件可能会写入许多位图块和inode。Xv6的write系统调用将大的写入分解为适合日志的多个较小的写入，unlink不会导致此问题，因为实际上Xv6文件系统只使用一个位图块。日志空间有限的另一个后果是，除非确定系统调用的写入将可容纳于日志中剩余的空间，否则日志系统无法允许启动系统调用。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:5:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.6 代码：日志 在系统调用中一个典型的日志使用就像这样： begin_op(); ... bp = bread(...); bp-\u003edata[...] = ...; log_write(bp); ... end_op(); begin_op（*kernel/log.c*:126）等待直到日志系统当前未处于提交中，并且直到有足够的未被占用的日志空间来保存此调用的写入。log.outstanding统计预定了日志空间的==系统调用数==；为此保留的总空间为log.outstanding乘以MAXOPBLOCKS。递增log.outstanding会预定空间并防止在此系统调用期间发生提交。代码保守地假设每个系统调用最多可以写入MAXOPBLOCKS个不同的块。 log_write（*kernel/log.c*:214）充当bwrite的代理。它将块的扇区号记录在内存中，在磁盘上的日志中预定一个槽位（ log.lh.n++;），并调用bpin将缓存固定在block cache中，以防止block cache将其逐出。 struct logheader { int n; int block[LOGSIZE];//一个包含磁盘扇区号的数组，logheader后面跟着的是这些扇区的数据(槽位) }; struct log { struct spinlock lock; int start; int size; int outstanding; // how many FS sys calls are executing. int committing; // in commit(), please wait. int dev; struct logheader lh; }; void log_write(struct buf *b) { int i; if (log.lh.n \u003e= LOGSIZE || log.lh.n \u003e= log.size - 1) panic(\"too big a transaction\"); if (log.outstanding \u003c 1) panic(\"log_write outside of trans\"); acquire(\u0026log.lock); for (i = 0; i \u003c log.lh.n; i++) { if (log.lh.block[i] == b-\u003eblockno) // log absorbtion break; } log.lh.block[i] = b-\u003eblockno; if (i == log.lh.n) { // Add new block to log? bpin(b);//（refcnt++） log.lh.n++; } release(\u0026log.lock); } 注：固定在block cache是指在缓存不足需要考虑替换时，不会将这个block换出，因为事务具有原子性：假设块45被写入，将其换出的话需要写入磁盘中文件系统对应的位置，而日志系统要求所有内存必须都存入日志，最后才能写入文件系统。 bpin`是通过增加引用计数防止块被换出的，之后需要再调用`bunpin 在提交之前，块必须留在缓存中(buffer cache)：在提交之前，缓存的副本是修改的唯一记录；只有在提交后才能将其写入磁盘上的位置；同一事务中的其他读取必须看到修改。log_write会注意到在单个事务中多次写入一个块的情况，并在日志中为该块分配相同的槽位。这种优化通常称为合并（absorption）。例如，包含多个文件inode的磁盘块在一个事务中被多次写入是很常见的。通过将多个磁盘写入合并到一个磁盘中，文件系统可以节省日志空间并实现更好的性能，因为只有一个磁盘块副本必须写入磁盘。 注：日志需要写入磁盘，以便重启后读取，但日志头块和日志数据块也会在block cache中有一个副本 end_op（*kernel/log.c*:146）首先减少未完成系统调用的计数。如果计数现在为零，则通过调用commit()提交当前事务。这一过程分为四个阶段。write_log()（*kernel/log.c*:178）将事务中修改的每个块从缓冲区缓存复制到磁盘上日志槽位中。write_head()（*kernel/log.c*:102）将头块写入磁盘：这是提交点，写入后的崩溃将导致从日志恢复重演事务的写入操作。install_trans（*kernel/log.c*:69）从日志中读取每个块，并将其写入文件系统中的适当位置。最后，end_op写入计数为零的日志头（将logheader的n成员变量赋值为0，然后调用write_head()写入磁盘）；这必须在下一个事务开始写入日志块之前发生，以便崩溃不会导致使用一个事务的头块和后续事务的日志块进行恢复。 static void commit() { if (log.lh.n \u003e 0) { write_log(); // Write modified blocks from cache to log write_head(); // Write header to disk -- the real commit install_trans(); // Now install writes to home locations，将文件系统日志部分的数据写入对应的文件中 log.lh.n = 0; write_head(); // Erase the transaction from the log } } recover_from_log（*kernel/log.c*:116）是由initlog（*kernel/log.c*:55）调用的，而它又是在第一个用户进程运行（*kernel/proc.c*:539）之前的引导期间由fsinit（*kernel/fs.c*:42）调用的。它读取日志头，如果头中指示日志包含已提交的事务，则模拟end_op的操作。 日志的一个示例使用发生在filewrite（*kernel/file.c*:135）中。事务如下所示： begin_op(); ilock(f-\u003eip); r = writei(f-\u003eip, ...); iunlock(f-\u003eip); end_op(); 这段代码被包装在一个循环中，该循环一次将大的写操作分解为几个扇区的单个事务，以避免日志溢出。作为此事务的一部分，对writei的调用写入许多块：文件的inode、一个或多个位图块以及一些数据块。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:6:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.7 代码：块分配器 文件和目录内容存储在磁盘块中，磁盘块必须从空闲池中分配。xv6的块分配器在磁盘上维护一个空闲位图，每一位代表一个块。0表示对应的块是空闲的；1表示它正在使用中。程序mkfs设置对应于引导扇区、超级块、日志块、inode块和位图块的比特位。 块分配器提供两个功能：balloc分配一个新的磁盘块，bfree释放一个块。Balloc中位于*kernel/fs.c*:71的循环从块0到sb.size（文件系统中的块数）遍历每个块。它查找位图中位为零的空闲块。如果balloc找到这样一个块，它将更新位图并返回该块。为了提高效率，循环被分成两部分。外部循环读取位图中的每个块。内部循环检查单个位图块中的所有BPB位。由于任何一个位图块在buffer cache中一次只允许一个进程使用，因此，如果两个进程同时尝试分配一个块，可能会发生争用。 Bfree（*kernel/fs.c*:90）找到正确的位图块并清除正确的位。同样，bread和brelse隐含的独占使用避免了显式锁定的需要。 与本章其余部分描述的大部分代码一样，必须在事务内部调用balloc和bfree。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:7:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.8 索引结点层 术语inode（即索引结点）可以具有两种相关含义之一。它可能是指包含文件大小和数据块编号列表的磁盘上的数据结构。或者“inode”可能指内存中的inode，它包含磁盘上inode的副本以及内核中所需的额外信息。 磁盘上的inode都被打包到一个称为inode块的连续磁盘区域中。每个inode的大小都相同，因此在给定数字n的情况下，很容易在磁盘上找到第n个inode。事实上，这个编号n，称为inode number或i-number，是在具体实现中标识inode的方式。 磁盘上的inode由struct dinode（*kernel/fs.h*:32）定义。字段type区分文件、目录和特殊文件（设备）。type为零表示磁盘inode是空闲的。==字段nlink统计引用此inode的目录条目数==，以便识别何时应释放磁盘上的inode及其数据块。字段size记录文件中内容的字节数。addrs数组记录保存文件内容的磁盘块的块号。 内核将活动的inode集合保存在内存中；struct inode（*kernel/file.h*:17）是磁盘上struct dinode的内存副本。只有当有C指针引用某个inode时，内核才会在内存中存储该inode。==ref字段统计引用内存中inode的C指针的数量==，如果引用计数降至零，内核将从内存中丢弃该inode。iget和iput函数分别获取和释放指向inode的指针，修改引用计数。指向inode的指针可以来自文件描述符、当前工作目录和如exec的瞬态内核代码。 xv6的inode代码中有四种锁或类似锁的机制。icache.lock保护以下两个不变量：inode最多在缓存中出现一次；缓存inode的ref字段记录指向缓存inode的内存指针数量。每个内存中的inode都有一个包含睡眠锁的lock字段，它确保以独占方式访问inode的字段（如文件长度）以及inode的文件或目录内容块。如果inode的ref大于零，则会导致系统在cache中维护inode，而不会对其他inode重用此缓存项。最后，每个inode都包含一个nlink字段（在磁盘上，如果已缓存则复制到内存中），该字段统计引用文件的目录项的数量；如果inode的链接计数大于零，xv6将不会释放inode。 iget()返回的struct inode指针在相应的iput()调用之前保证有效：inode不会被删除，指针引用的内存也不会被其他inode重用。iget()提供==对inode的非独占访问==，因此可以有许多指向同一inode的指针。文件系统代码的许多部分都依赖于iget()的这种行为，既可以保存对inode的长期引用（如打开的文件和当前目录），也可以防止争用，同时避免操纵多个inode（如路径名查找）的代码产生死锁。 iget返回的struct inode可能没有任何有用的内容。为了确保它保存磁盘inode的副本，代码必须调用ilock。这将==锁定inode（以便没有其他进程可以对其进行ilock）==，并从磁盘读取尚未读取的inode。iunlock释放inode上的锁。将inode指针的获取与锁定分离有助于在某些情况下避免死锁，例如在目录查找期间。多个进程可以持有指向iget返回的inode的C指针，但一次只能有一个进程锁定inode。 inode缓存只缓存内核代码或数据结构持有C指针的inode。它的主要工作实际上是同步多个进程的访问；缓存是次要的。如果经常使用inode，在inode缓存不保留它的情况下buffer cache可能会将其保留在内存中。inode缓存是直写的，这意味着修改已缓存inode的代码必须立即使用iupdate将其写入磁盘。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:8:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.9 代码：Inodes 为了分配新的inode（例如，在创建文件时），xv6调用ialloc（*kernel/fs.c*:196）。Ialloc类似于balloc：它一次一个块地遍历磁盘上的索引节点结构体，查找标记为空闲的一个。当它找到一个时，它通过将新type写入磁盘来声明它，然后末尾通过调用iget（*kernel/fs.c*:210）从inode缓存返回一个条目。ialloc的正确操作取决于这样一个事实：一次只有一个进程可以保存对bp的引用：ialloc可以确保其他进程不会同时看到inode可用并尝试声明它。 Iget（*kernel/fs.c*:243）在inode缓存中查找具有所需设备和inode编号的活动条目（ip-\u003eref \u003e 0）。如果找到一个，它将返回对该incode的新引用（*kernel/fs.c*:252-256）。在iget扫描时，它会记录第一个空槽（*kernel/fs.c*:257-258）的位置，如果需要分配缓存项，它会使用这个槽。 在读取或写入inode的元数据或内容之前，代码必须使用ilock锁定inode。Ilock（kernel/fs.c:289）为此使用睡眠锁。一旦ilock以独占方式访问inode，它将根据需要从磁盘（更可能是buffer cache）读取inode。函数iunlock（*kernel/fs.c*:317）释放睡眠锁，这可能会导致任何睡眠进程被唤醒。 Iput（*kernel/fs.c*:333）通过减少引用计数（*kernel/fs.c*:356）释放指向inode的C指针。如果这是最后一次引用，inode缓存中该inode的槽现在将是空闲的，可以重用于其他inode。 如果iput发现没有指向inode的C指针引用，并且inode没有指向它的链接（发生于无目录），则必须释放inode及其数据块。Iput调用itrunc将文件截断为零字节，释放数据块；将索引节点类型设置为0（未分配）；并将inode写入磁盘（*kernel/fs.c*:338）。 iput中释放inode的锁定协议值得仔细研究。一个危险是并发线程可能正在ilock中等待使用该inode（例如，读取文件或列出目录），并且不会做好该inode已不再被分配的准备。这不可能发生，因为如果缓存的inode没有链接，并且ip-\u003eref为1，那么系统调用就无法获取指向该inode的指针。那一个引用是调用iput的线程所拥有的引用。的确，iput在icache.lock的临界区域之外检查引用计数是否为1，但此时已知链接计数为零，因此没有线程会尝试获取新引用。另一个主要危险是，对ialloc的并发调用可能会选择iput正在释放的同一个inode。这只能在iupdate写入磁盘以使inode的type为零后发生。这个争用是良性的：分配线程将客气地等待获取inode的睡眠锁，然后再读取或写入inode，此时iput已完成。 iput()可以写入磁盘。这意味着任何使用文件系统的系统调用都可能写入磁盘，因为系统调用可能是最后一个引用该文件的系统调用。即使像read()这样看起来是只读的调用，也可能最终调用iput()。这反过来意味着，即使是只读系统调用，如果它们使用文件系统，也必须在事务中进行包装。 iput()和崩溃之间存在一种具有挑战性的交互。iput()不会在文件的链接计数降至零时立即截断文件，因为某些进程可能仍在内存中保留对inode的引用：进程可能仍在读取和写入该文件，因为它已成功打开该文件。但是，如果在最后一个进程关闭该文件的文件描述符之前发生崩溃，则该文件将被标记为已在磁盘上分配，但没有目录项指向它。（磁盘已经没有目录项指向它，内存中的最后一个进程关闭该文件时发生的崩溃，那么nlink为0，ref为1） 文件系统以两种方式之一处理这种情况。简单的解决方案用于恢复时：重新启动后，文件系统会扫描整个文件系统，以查找标记为已分配但没有指向它们的目录项的文件。如果存在任何此类文件，接下来可以将其释放。 第二种解决方案不需要扫描文件系统。在此解决方案中，文件系统在磁盘（例如在超级块中）上记录链接计数降至零但引用计数不为零的文件的i-number。如果文件系统在其引用计数达到0时删除该文件，则会通过从列表中删除该inode来更新磁盘列表。恢复时，文件系统将释放列表中的任何文件。（将ref！=0，nlink=0的全部释放） Xv6没有实现这两种解决方案，这意味着inode可能被标记为已在磁盘上分配，即使它们不再使用。这意味着随着时间的推移，xv6可能会面临磁盘空间不足的风险。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:9:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.10 代码： Inode包含内容 磁盘上的inode结构体struct dinode包含一个size和一个块号数组（见图8.3）。inode数据可以在dinode的addrs数组列出的块中找到。前面的NDIRECT个数据块被列在数组中的前NDIRECT个元素中；这些块称为直接块（direct blocks）。接下来的NINDIRECT个数据块不在inode中列出，而是在称为间接块（indirect block）的数据块中列出。addrs数组中的最后一个元素给出了间接块的地址。因此，可以从inode中列出的块加载文件的前12 kB（NDIRECT x BSIZE）字节，而只有在查阅间接块后才能加载下一个256 kB（NINDIRECT x BSIZE）字节。这是一个很好的磁盘表示，但对于客户端来说较复杂。函数bmap管理这种表示，以便实现我们将很快看到的如readi和writei这样的更高级例程。bmap(struct inode *ip, uint bn)返回索引结点ip的第bn个数据块的磁盘块号。如果ip还没有这样的块，bmap会分配一个。 函数bmap（*kernel/fs.c*:378）从简单的情况开始：前面的NDIRECT个块在inode本身中列出（*kernel/fs.c*:383-387）中。下面NINDIRECT个块在ip-\u003eaddrs[NDIRECT]的间接块中列出。Bmap读取间接块（*kernel/fs.c*:394），然后从块内的正确位置（*kernel/fs.c*:395）读取块号。如果块号超过NDIRECT+NINDIRECT，则bmap调用panic崩溃；writei包含防止这种情况发生的检查（*kernel/fs.c*:490）。 Bmap根据需要分配块。ip-\u003eaddrs[]或间接块中条目为零表示未分配块。当bmap遇到零时，它会用按需分配的新块（*kernel/fs.c*:384-385）（*kernel/fs.c*:392-393）替换它们。 itrunc释放文件的块，将inode的size重置为零。Itrunc（*kernel/fs.c*:410）首先释放直接块（*kernel/fs.c*:416-421），然后释放间接块中列出的块（*kernel/fs.c*:426-429），最后释放间接块本身（*kernel/fs.c*:431-432）。 Bmap使readi和writei很容易获取inode的数据。Readi（*kernel/fs.c*:456）首先确保偏移量和计数不超过文件的末尾。开始于超过文件末尾的地方读取将返回错误（*kernel/fs.c*:461-462），而从文件末尾开始或穿过文件末尾的读取返回的字节数少于请求的字节数（*kernel/fs.c*:463-464）。主循环处理文件的每个块，将数据从缓冲区复制到dst（*kernel/fs.c*:466-474）。writei（*kernel/fs.c*:483）与readi相同，但有三个例外：从文件末尾开始或穿过文件末尾的写操作会使文件增长到最大文件大小（*kernel/fs.c*:490-491）；循环将数据复制到缓冲区而不是输出（kernel/fs.c:36）；如果写入扩展了文件，writei必须更新其大小（*kernel/fs.c*:504-511）。 readi和writei都是从检查ip-\u003etype == T_DEV开始的。这种情况处理的是数据不在文件系统中的特殊设备；我们将在文件描述符层返回到这种情况。 函数stati（*kernel/fs.c*:442）将inode元数据复制到stat结构体中，该结构通过stat系统调用向用户程序公开。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:10:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.11 代码：目录层 目录的内部实现很像文件。其inode的type为T_DIR，其数据是一系列目录条目（directory entries）。每个条目（entry）都是一个struct dirent（*kernel/fs.h*:56），其中包含一个名称name和一个inode编号inum。名称最多为DIRSIZ（14）个字符；如果较短，则以NUL（0）字节终止。inode编号为零的条目是空的。 函数dirlookup（*kernel/fs.c*:527）在目录中搜索具有给定名称的条目。如果找到一个，它将返回一个指向相应inode的指针，解开锁定，并将*poff设置为目录中条目的字节偏移量，以满足调用方希望对其进行编辑的情形。如果dirlookup找到具有正确名称的条目，它将更新*poff并返回通过iget获得的未锁定的inode。Dirlookup是iget返回未锁定indoe的原因。调用者已锁定dp，因此，如果对.，当前目录的别名，进行查找，则在返回之前尝试锁定indoe将导致重新锁定dp并产生死锁(还有更复杂的死锁场景，涉及多个进程和..，父目录的别名。.不是唯一的问题。）调用者可以解锁dp，然后锁定ip，确保它一次只持有一个锁。 函数dirlink（*kernel/fs.c*:554）将给定名称和inode编号的新目录条目写入目录dp。如果名称已经存在，dirlink将返回一个错误（*kernel/fs.c*:560-564）。主循环读取目录条目，查找未分配的条目。当找到一个时，它会提前停止循环（*kernel/fs.c*:538-539），并将off设置为可用条目的偏移量。否则，循环结束时会将off设置为dp-\u003esize。无论哪种方式，dirlink都会通过在偏移off处写入（*kernel/fs.c*:574-577）来向目录添加一个新条目。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:11:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.12 代码：路径名 路径名查找涉及一系列对dirlookup的调用，每个路径组件调用一个。Namei（*kernel/fs.c*:661）计算path并返回相应的inode。函数nameiparent是一个变体：它在最后一个元素之前停止，返回父目录的inode并将最后一个元素复制到name中。两者都调用通用函数namex来完成实际工作。 Namex（*kernel/fs.c*:626）首先决定路径计算的开始位置。如果路径以斜线开始，则计算从根目录开始；否则，从当前目录开始（*kernel/fs.c*:630-633）。然后，它使用skipelem依次考察路径的每个元素（*kernel/fs.c*:635）。循环的每次迭代都必须在当前索引结点ip中查找name。迭代首先给ip上锁并检查它是否是一个目录。如果不是，则查找失败（*kernel/fs.c*:636-640）(锁定ip是必要的，不是因为ip-\u003etype可以被更改，而是因为在ilock运行之前，ip-\u003etype不能保证已从磁盘加载。）如果调用是nameiparent，并且这是最后一个路径元素，则根据nameiparent的定义，循环会提前停止；最后一个路径元素已经复制到name中，因此namex只需返回解锁的ip（*kernel/fs.c*:641-645）。最后，循环将使用dirlookup查找路径元素，并通过设置ip = next（*kernel/fs.c*:646-651）为下一次迭代做准备。当循环用完路径元素时，它返回ip。 namex过程可能需要很长时间才能完成：它可能涉及多个磁盘操作来读取路径名中所遍历目录的索引节点和目录块（如果它们不在buffer cache中）。Xv6经过精心设计，如果一个内核线程对namex的调用在磁盘I/O上阻塞，另一个查找不同路径名的内核线程可以同时进行。Namex分别锁定路径中的每个目录，以便在不同目录中进行并行查找。 这种并发性带来了一些挑战。例如，当一个内核线程正在查找路径名时，另一个内核线程可能正在通过取消目录链接来更改目录树。一个潜在的风险是，查找可能正在搜索已被另一个内核线程删除且其块已被重新用于另一个目录或文件的目录。 Xv6避免了这种竞争。例如，在namex中执行dirlookup时，lookup线程持有目录上的锁，dirlookup返回使用iget获得的inode。Iget增加索引节点的引用计数。只有在从dirlookup接收inode之后，namex才会释放目录上的锁。现在，另一个线程可以从目录中取消inode的链接，但是xv6还不会删除inode，因为inode的引用计数仍然大于零。 另一个风险是死锁。例如，查找“.”时，next指向与ip相同的inode。在释放ip上的锁之前锁定next将导致死锁。为了避免这种死锁，namex在获得下一个目录的锁之前解锁该目录。这里我们再次看到为什么iget和ilock之间的分离很重要。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:12:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.13 文件描述符层 Unix界面的一个很酷的方面是，Unix中的大多数资源都表示为文件，包括控制台、管道等设备，当然还有真实文件。文件描述符层是实现这种一致性的层。 正如我们在第1章中看到的，Xv6为每个进程提供了自己的打开文件表或文件描述符。每个打开的文件都由一个struct file（*kernel/file.h*:1）表示，它是inode或管道的封装，加上一个I/O偏移量。每次调用open都会创建一个新的打开文件（一个新的struct file）：如果多个进程独立地打开同一个文件，那么不同的实例将具有不同的I/O偏移量。另一方面，单个打开的文件（同一个struct file）可以多次出现在一个进程的文件表中，也可以出现在多个进程的文件表中。如果一个进程使用open打开文件，然后使用==dup创建别名==，或使用fork与子进程共享，就会发生这种情况。引用计数跟踪对特定打开文件的引用数。可以打开文件进行读取或写入，也可以同时进行读取和写入。readable和writable字段可跟踪此操作。 系统中所有打开的文件都保存在全局文件表ftable中。文件表具有分配文件（filealloc）、创建重复引用（filedup）、释放引用（fileclose）以及读取和写入数据（fileread和filewrite）的函数。 前三个函数遵循现在熟悉的形式。Filealloc（*kernel/file.c*:30）扫描文件表以查找未引用的文件（f-\u003eref == 0），并返回一个新的引用；filedup（*kernel/file.c*:48）增加引用计数；fileclose（*kernel/file.c*:60）将其递减。当文件的引用计数达到零时，fileclose会根据type释放底层管道或inode。 函数filestat、fileread和filewrite实现对文件的stat、read和write操作。Filestat（*kernel/file.c*:88）只允许在inode上操作并且调用了stati。Fileread和filewrite检查打开模式是否允许该操作，然后将调用传递给管道或inode的实现。如果文件表示inode，fileread和filewrite使用I/O偏移量作为操作的偏移量，然后将文件指针前进该偏移量（*kernel/file.c*:122-123）（*kernel/file.c*:153-154）。管道没有偏移的概念。回想一下，inode的函数要求调用方处理锁（*kernel/file.c*:94-96）（*kernel/file.c*:121-124）（*kernel/file.c*:163-166）。inode锁定有一个方便的副作用，即读取和写入偏移量以原子方式更新，因此，对同一文件的同时多次写入不能覆盖彼此的数据，尽管他们的写入最终可能是交错的。 ","date":"2023-10-31","objectID":"/6.s081_lab9/:13:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"8.14 代码：系统调用 通过使用底层提供的函数，大多数系统调用的实现都很简单（请参阅*kernel/sysfile.c*）。有几个调用值得仔细看看。 函数sys_link和sys_unlink编辑目录，创建或删除索引节点的引用。它们是使用事务能力的另一个很好的例子。sys_link（*kernel/sysfile.c*:120）从获取其参数开始，两个字符串分别是old和new（*kernel/sysfile.c*:125）。假设old存在并且不是一个目录（*kernel/sysfile.c*:129-132），sys_link会增加其ip-\u003enlink计数。然后sys_link调用nameiparent来查找new（*kernel/sysfile.c*:145）的父目录和最终路径元素，并创建一个指向old的inode（*kernel/sysfile.c*:148）的新目录条目。new的父目录必须存在并且与现有inode位于同一设备上：inode编号在一个磁盘上只有唯一的含义。如果出现这样的错误，sys_link必须返回并减少ip-\u003enlink。 事务简化了实现，因为它需要更新多个磁盘块，但我们不必担心更新的顺序。他们要么全部成功，要么什么都不做。例如在没有事务的情况下，在创建一个链接之前更新ip-\u003enlink会使文件系统暂时处于不安全状态，而在这两者之间发生的崩溃可能会造成严重破坏。对于事务，我们不必担心这一点 Sys_link为现有inode创建一个新名称。函数create（*kernel/sysfile.c*:242）为新inode创建一个新名称。它是三个文件创建系统调用的泛化：带有O_CREATE标志的open生成一个新的普通文件，mkdir生成一个新目录，mkdev生成一个新的设备文件。与sys_link一样，create从调用nameiparent开始，以获取父目录的inode。然后调用dirlookup检查名称是否已经存在（*kernel/sysfile.c*:252）。如果名称确实存在，create的行为取决于它用于哪个系统调用：open的语义与mkdir和mkdev不同。如果create是代表open（type == T_FILE）使用的，并且存在的名称本身是一个常规文件，那么open会将其视为成功，create也会这样做（*kernel/sysfile.c*:256）。否则，这是一个错误（*kernel/sysfile.c*:257-258）。如果名称不存在，create现在将使用ialloc（*kernel/sysfile.c*:261）分配一个新的inode。如果新inode是目录，create将使用.和..条目对它进行初始化。最后，既然数据已正确初始化，create可以将其链接到父目录（*kernel/sysfile.c*:274）。Create与sys_link一样，同时持有两个inode锁：ip和dp。不存在死锁的可能性，因为索引结点ip是新分配的：系统中没有其他进程会持有ip的锁，然后尝试锁定dp。 使用create，很容易实现sys_open、sys_mkdir和sys_mknod。Sys_open（*kernel/sysfile.c*:287）是最复杂的，因为创建一个新文件只是它能做的一小部分。如果open被传递了O_CREATE标志，它将调用create（*kernel/sysfile.c*:301）。否则，它将调用namei（*kernel/sysfile.c*:307）。Create返回一个锁定的inode，但namei不锁定，因此sys_open必须锁定inode本身。这提供了一个方便的地方来检查目录是否仅为读取打开，而不是写入。假设inode是以某种方式获得的，sys_open分配一个文件和一个文件描述符（*kernel/sysfile.c*:325），然后填充该文件（*kernel/sysfile.c*:337-342）。请注意，没有其他进程可以访问部分初始化的文件，因为它仅位于当前进程的表中。 在我们还没有文件系统之前，第7章就研究了管道的实现。函数sys_pipe通过提供创建管道对的方法将该实现连接到文件系统。它的参数是一个指向两个整数的指针，它将在其中记录两个新的文件描述符。然后分配管道并安装文件描述符。 lab9 ","date":"2023-10-31","objectID":"/6.s081_lab9/:14:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"Large files(moderate) 在本作业中，您将增加xv6文件的最大大小。目前，xv6文件限制为268个块或268*BSIZE字节（在xv6中BSIZE为1024）。此限制来自以下事实：一个xv6 inode包含12个“直接”块号和一个“间接”块号，“一级间接”块指一个最多可容纳256个块号的块，总共12+256=268个块。 bigfile命令可以创建最长的文件，并报告其大小： $ bigfile .. wrote 268 blocks bigfile: file is too small $ 测试失败，因为bigfile希望能够创建一个包含65803个块的文件，但未修改的xv6将文件限制为268个块。 您将更改xv6文件系统代码，以支持每个inode中可包含256个一级间接块地址的“二级间接”块，每个一级间接块最多可以包含256个数据块地址。结果将是一个文件将能够包含多达65803个块，或256*256+256+11个块（11而不是12，因为我们将为二级间接块牺牲一个直接块号）。 修改ip-\u003eaddrs[]中NDIRECT的宏定义改为11，因为需要1个一级索引，一个二级索引。之后修改dinode和inode中的addrs[NDIRECT+1+1] #define NDIRECT 11 #define NINDIRECT (BSIZE / sizeof(uint)) #define SECNINDIRECT ((BSIZE / sizeof(uint)) * NINDIRECT) #define MAXFILE (NDIRECT + NINDIRECT + SECNINDIRECT 修改bmap的二级索引，仿照一级索引来写 if (bn \u003c NINDIRECT) { // Load indirect block, allocating if necessary. if ((addr = ip-\u003eaddrs[NDIRECT]) == 0) // 如果第12项为0那么就为addr[11]分配空间，将该block的地址存入addrs[11] ip-\u003eaddrs[NDIRECT] = addr = balloc(ip-\u003edev); bp = bread(ip-\u003edev, addr); a = (uint *)bp-\u003edata; if ((addr = a[bn]) == 0) { // 分配一个block的data，将该block的地址赋值给a[bn] a[bn] = addr = balloc(ip-\u003edev); log_write(bp); } brelse(bp); return addr; } 一级索引如上所示,先去看addrs[NDIRECT]这个数组元素是否有地址，如果有地址那么就代表已经分配了一个块即indirect block来存储数据的地址，如果没有那么就分配一个块indirect block并把块地址放入addr[NDIRECT]。 先判断数据块是否已经分配（看a[bn]是否存了块的地址），如果没有分配就分配一个数据块，并将数据块data的地址放在addr[NDIRECT]指向的数据块indirect block的address n的位置，然后将buffer cache的数据bp写入data数据块中 二级索引 bn -= NINDIRECT; //二级索引 if (bn \u003c SECNINDIRECT){ // Load double indirect block, allocating if necessary. if ((addr = ip-\u003eaddrs[NDIRECT+1]) == 0) // 如果第13项为0那么就为addr[12]分配空间，将该block的地址存入addrs[12] ip-\u003eaddrs[NDIRECT+1] = addr = balloc(ip-\u003edev); bp = bread(ip-\u003edev, addr); a = (uint *)bp-\u003edata; uint addr2; if ((addr2 = a[bn/NINDIRECT]) == 0) { // 分配一个block的data，将该block的地址赋值给a[bn] a[bn/NINDIRECT] = addr2 = balloc(ip-\u003edev); log_write(bp); } brelse(bp); bp = bread(ip-\u003edev, addr2); a = (uint *)bp-\u003edata; if ((addr = a[bn%NINDIRECT]) == 0) { // 分配一个block的data，将该block的地址赋值给a[bn] a[bn%NINDIRECT] = addr = balloc(ip-\u003edev); log_write(bp); } brelse(bp); return addr; } 之后仿照程序释放一级索引块的方法释放二级索引块。释放一级索引块：先释放数据块，然后释放存储数据块地址的块，最后释放addrs数组 void itrunc(struct inode *ip) { int i, j; struct buf *bp; uint *a; for (i = 0; i \u003c NDIRECT; i++) { if (ip-\u003eaddrs[i]) { bfree(ip-\u003edev, ip-\u003eaddrs[i]); ip-\u003eaddrs[i] = 0; } } if (ip-\u003eaddrs[NDIRECT]) { bp = bread(ip-\u003edev, ip-\u003eaddrs[NDIRECT]); a = (uint *)bp-\u003edata; for (j = 0; j \u003c NINDIRECT; j++) { if (a[j]) bfree(ip-\u003edev, a[j]); } brelse(bp); bfree(ip-\u003edev, ip-\u003eaddrs[NDIRECT]); ip-\u003eaddrs[NDIRECT] = 0; } if(ip-\u003eaddrs[NDIRECT+1]){ bp = bread(ip-\u003edev, ip-\u003eaddrs[NDIRECT+1]); a = (uint *)bp-\u003edata; for (j = 0; j \u003c NINDIRECT; j++) { if (a[j]) { struct buf *bp2 = bread(ip-\u003edev, a[j]); uint *a2 = (uint *)bp2-\u003edata; for (int k = 0; k \u003c NINDIRECT; k++) { if (a2[k]) bfree(ip-\u003edev, a2[k]); } brelse(bp2); bfree(ip-\u003edev, a[j]); } } brelse(bp); bfree(ip-\u003edev, ip-\u003eaddrs[NDIRECT+1]); ip-\u003eaddrs[NDIRECT+1] = 0; } ip-\u003esize = 0; iupdate(ip); } ","date":"2023-10-31","objectID":"/6.s081_lab9/:15:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"Symbolic links(moderate) 在本练习中，您将向xv6添加符号链接。符号链接（或软链接）是指按路径名链接的文件；当一个符号链接打开时，内核跟随该链接指向引用的文件。符号链接类似于硬链接，但硬链接仅限于指向同一磁盘上的文件，而符号链接可以跨磁盘设备。尽管xv6不支持多个设备，但实现此系统调用是了解路径名查找工作原理的一个很好的练习。 YOUR JOB 您将实现symlink(char *target, char *path)系统调用，该调用在引用由target命名的文件的路径处创建一个新的符号链接。有关更多信息，请参阅symlink手册页（注：执行man symlink）。要进行测试，请将symlinktest添加到***Makefile***并运行它。当测试产生以下输出（包括usertests运行成功）时，您就完成本作业了。 $ symlinktest Start: test symlinks test symlinks: ok Start: test concurrent symlinks test concurrent symlinks: ok $ usertests ... ALL TESTS PASSED $ 提示： 首先，为symlink创建一个新的系统调用号，在*user/usys.pl*、**user/user.h*中添加一个条目，并在kernel/sysfile.c***中实现一个空的sys_symlink。 向***kernel/stat.h***添加新的文件类型（T_SYMLINK）以表示符号链接。 在k*ernel/fcntl.h*中添加一个新标志（O_NOFOLLOW），该标志可用于open系统调用。请注意，传递给open的标志使用按位或运算符组合，因此新标志不应与任何现有标志重叠。一旦将user/symlinktest.c*添加到Makefile***中，您就可以编译它。 实现symlink(target, path)系统调用，以在path处创建一个新的指向target的符号链接。请注意，系统调用的成功不需要target已经存在。您需要选择存储符号链接目标路径的位置，例如在inode的数据块中。symlink应返回一个表示成功（0）或失败（-1）的整数，类似于link和unlink。 修改open系统调用以处理路径指向符号链接的情况。如果文件不存在，则打开必须失败。当进程向open传递O_NOFOLLOW标志时，open应打开符号链接（而不是跟随符号链接）。 如果链接文件也是符号链接，则必须递归地跟随它，直到到达非链接文件为止。如果链接形成循环，则必须返回错误代码。你可以通过以下方式估算存在循环：通过在链接深度达到某个阈值（例如10）时返回错误代码。 其他系统调用（如link和unlink）不得跟随符号链接；这些系统调用对符号链接本身进行操作。 您不必处理指向此实验的目录的符号链接。 (1). 配置系统调用的常规操作，如在*user/usys.pl*、user/user.h*中添加一个条目，在kernel/syscall.c*、***kernel/syscall.h***中添加相关内容 (2). 添加提示中的相关定义，T_SYMLINK以及O_NOFOLLOW // fcntl.h #define O_NOFOLLOW 0x004 // stat.h #define T_SYMLINK 4 (3). 在***kernel/sysfile.c***中实现sys_symlink，这里需要注意的是create返回已加锁的inode，此外iunlockput既对inode解锁，还将其引用计数减1，计数为0时回收此inode uint64 sys_symlink(void) { char target[MAXPATH], path[MAXPATH]; struct inode* ip_path; if(argstr(0, target, MAXPATH) \u003c 0 || argstr(1, path, MAXPATH) \u003c 0) { return -1; } begin_op(); // 分配一个inode结点，create返回锁定的inode ip_path = create(path, T_SYMLINK, 0, 0); if(ip_path == 0) { end_op(); return -1; } // 向inode数据块中写入target路径 if(writei(ip_path, 0, (uint64)target, 0, MAXPATH) \u003c MAXPATH) { iunlockput(ip_path); end_op(); return -1; } iunlockput(ip_path); end_op(); return 0; } (4). 修改sys_open支持打开符号链接 // 处理符号链接 // FOLLOW代表跟随符号链接即打开符号链接指向的文件，NOFOLLOW代表就是打开符号链接本身 if (ip-\u003etype == T_SYMLINK \u0026\u0026 !(omode \u0026 O_NOFOLLOW)) { // 若符号链接指向的仍然是符号链接，则递归的跟随它 // 直到找到真正指向的文件 // 但深度不能超过MAX_SYMLINK_DEPTH for (int i = 0; i \u003c MAX_SYMLINK_DEPTH; ++i) { // 读出符号链接指向的路径 if (readi(ip, 0, (uint64)path, 0, MAXPATH) != MAXPATH) { iunlockput(ip); end_op(); return -1; } // 释放当前符号链接的inode，并释放锁 iunlockput(ip); // get inode of the path ip = namei(path); if (ip == 0) { end_op(); return -1; } // get new inode lock ilock(ip); if (ip-\u003etype != T_SYMLINK) break; } // 超过最大允许深度后仍然为符号链接，则返回错误 if (ip-\u003etype == T_SYMLINK) { iunlockput(ip); end_op(); return -1; } } ","date":"2023-10-31","objectID":"/6.s081_lab9/:16:0","tags":null,"title":"6.s081_lab9","uri":"/6.s081_lab9/"},{"categories":["6.s081"],"content":"lab8 ","date":"2023-10-31","objectID":"/6.s081_lab8/:0:0","tags":null,"title":"6.s081_lab8","uri":"/6.s081_lab8/"},{"categories":["6.s081"],"content":"Memory allocator 本实验主要是解决多CPU竞争内存的问题，我们通过为每个CPU分配一个kmem锁来解决该问题，为每个CPU都维护一个空闲列表，初始时将所有的空闲内存分配到某个CPU，此后各个CPU需要内存时，如果当前CPU的空闲列表上没有，则窃取其他CPU的。例如，所有的空闲内存初始分配到CPU0，当CPU1需要内存时就会窃取CPU0的，而使用完成后就挂在CPU1的空闲列表，此后CPU1再次需要内存时就可以从自己的空闲列表中取。 (1). 将kmem定义为一个数组，包含NCPU个元素，即每个CPU对应一个 struct { struct spinlock lock; struct run *freelist; } kmem[NCPU]; (2). 修改kinit，为所有锁初始化以“kmem”开头的名称，该函数只会被一个CPU调用，freerange调用kfree将所有空闲内存挂在该CPU的空闲列表上 void kinit() { char lockname[8]; for(int i = 0;i \u003c NCPU; i++) { snprintf(lockname, sizeof(lockname), \"kmem_%d\", i); initlock(\u0026kmem[i].lock, lockname); } freerange(end, (void*)PHYSTOP); } (3). 修改kfree，使用cpuid()和它返回的结果时必须关中断，请参考《XV6使用手册》第7.4节 void kfree(void *pa) { struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa \u003c end || (uint64)pa \u003e= PHYSTOP) panic(\"kfree\"); // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); r = (struct run*)pa; push_off(); // 关中断 int id = cpuid(); acquire(\u0026kmem[id].lock); r-\u003enext = kmem[id].freelist; kmem[id].freelist = r; release(\u0026kmem[id].lock); pop_off(); //开中断 } (4). 修改kalloc，使得在当前CPU的空闲列表没有可分配内存时窃取其他内存的 void * kalloc(void) { struct run *r; push_off();// 关中断 int id = cpuid(); acquire(\u0026kmem[id].lock); r = kmem[id].freelist; if(r) kmem[id].freelist = r-\u003enext; else { int antid; // another id // 遍历所有CPU的空闲列表 for(antid = 0; antid \u003c NCPU; ++antid) { if(antid == id) continue; acquire(\u0026kmem[antid].lock); r = kmem[antid].freelist; if(r) { kmem[antid].freelist = r-\u003enext; release(\u0026kmem[antid].lock); break; } release(\u0026kmem[antid].lock); } } release(\u0026kmem[id].lock); pop_off(); //开中断 if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r; } ","date":"2023-10-31","objectID":"/6.s081_lab8/:1:0","tags":null,"title":"6.s081_lab8","uri":"/6.s081_lab8/"},{"categories":["6.s081"],"content":"Buffer cache 该实验是解决多个进程竞争缓冲区的问题，按照上一个实验为每个CPU分配锁已经不起作用，因为一个CPU也会存在多个进程竞争锁 (1). 定义哈希桶结构，并在bcache中删除全局缓冲区链表，改为使用素数个散列桶 #define NBUCKET 13 #define HASH(id) (id % NBUCKET) struct hashbuf { struct buf head; // 头节点 struct spinlock lock; // 锁 }; struct { struct buf buf[NBUF]; struct hashbuf buckets[NBUCKET]; // 散列桶 } bcache; (2). 在binit中，（1）初始化散列桶的锁，（2）将所有散列桶的head-\u003eprev、head-\u003enext都指向自身表示为空，（3）将所有的缓冲区挂载到bucket[0]桶上，代码如下 void binit(void) { struct buf* b; char lockname[16]; for(int i = 0; i \u003c NBUCKET; ++i) { // 初始化散列桶的自旋锁 snprintf(lockname, sizeof(lockname), \"bcache_%d\", i); initlock(\u0026bcache.buckets[i].lock, lockname); // 初始化散列桶的头节点 bcache.buckets[i].head.prev = \u0026bcache.buckets[i].head; bcache.buckets[i].head.next = \u0026bcache.buckets[i].head; } // Create linked list of buffers for(b = bcache.buf; b \u003c bcache.buf + NBUF; b++) { // 利用头插法初始化缓冲区列表,全部放到散列桶0上 b-\u003enext = bcache.buckets[0].head.next; b-\u003eprev = \u0026bcache.buckets[0].head; initsleeplock(\u0026b-\u003elock, \"buffer\"); bcache.buckets[0].head.next-\u003eprev = b; bcache.buckets[0].head.next = b; } } (3). 在***buf.h***中增加新字段timestamp，这里来理解一下这个字段的用途：在原始方案中，每次brelse都将被释放的缓冲区挂载到链表头，禀明这个缓冲区最近刚刚被使用过，在bget中分配时从链表尾向前查找，这样符合条件的第一个就是最久未使用的。而在提示中建议使用时间戳作为LRU判定的法则，这样我们就无需在brelse中进行头插法更改结点位置 struct buf { ... ... uint timestamp; // 时间戳 }; (4). 更改brelse，不再获取全局锁 void brelse(struct buf* b) { if(!holdingsleep(\u0026b-\u003elock)) panic(\"brelse\"); int bid = HASH(b-\u003eblockno); releasesleep(\u0026b-\u003elock); acquire(\u0026bcache.buckets[bid].lock); b-\u003erefcnt--; // 更新时间戳 // 由于LRU改为使用时间戳判定，不再需要头插法 acquire(\u0026tickslock); b-\u003etimestamp = ticks; release(\u0026tickslock); release(\u0026bcache.buckets[bid].lock); } (5). 更改bget，当没有找到指定的缓冲区时进行分配，分配方式是优先从当前列表遍历，找到一个没有引用且timestamp最小的缓冲区，如果没有就申请下一个桶的锁，并遍历该桶，找到后将该缓冲区从原来的桶移动到当前桶中，最多将所有桶都遍历完。在代码中要注意锁的释放 static struct buf* bget(uint dev, uint blockno) { struct buf* b; int bid = HASH(blockno); acquire(\u0026bcache.buckets[bid].lock); // Is the block already cached? for(b = bcache.buckets[bid].head.next; b != \u0026bcache.buckets[bid].head; b = b-\u003enext) { if(b-\u003edev == dev \u0026\u0026 b-\u003eblockno == blockno) { b-\u003erefcnt++; // 记录使用时间戳 acquire(\u0026tickslock); b-\u003etimestamp = ticks; release(\u0026tickslock); release(\u0026bcache.buckets[bid].lock); acquiresleep(\u0026b-\u003elock); return b; } } // Not cached. b = 0; struct buf* tmp; // Recycle the least recently used (LRU) unused buffer. // 从当前散列桶开始查找 for(int i = bid, cycle = 0; cycle != NBUCKET; i = (i + 1) % NBUCKET) { ++cycle; // 如果遍历到当前散列桶，则不重新获取锁 if(i != bid) { if(!holding(\u0026bcache.buckets[i].lock)) acquire(\u0026bcache.buckets[i].lock); else continue; } for(tmp = bcache.buckets[i].head.next; tmp != \u0026bcache.buckets[i].head; tmp = tmp-\u003enext) // 使用时间戳进行LRU算法，而不是根据结点在链表中的位置 if(tmp-\u003erefcnt == 0 \u0026\u0026 (b == 0 || tmp-\u003etimestamp \u003c b-\u003etimestamp)) b = tmp; if(b) { // 如果是从其他散列桶窃取的，则将其以头插法插入到当前桶 if(i != bid) { b-\u003enext-\u003eprev = b-\u003eprev; b-\u003eprev-\u003enext = b-\u003enext; release(\u0026bcache.buckets[i].lock); b-\u003enext = bcache.buckets[bid].head.next; b-\u003eprev = \u0026bcache.buckets[bid].head; bcache.buckets[bid].head.next-\u003eprev = b; bcache.buckets[bid].head.next = b; } b-\u003edev = dev; b-\u003eblockno = blockno; b-\u003evalid = 0; b-\u003erefcnt = 1; acquire(\u0026tickslock); b-\u003etimestamp = ticks; release(\u0026tickslock); release(\u0026bcache.buckets[bid].lock); acquiresleep(\u0026b-\u003elock); return b; } else { // 在当前散列桶中未找到，则直接释放锁 if(i != bid) release(\u0026bcache.buckets[i].lock); } } panic(\"bget: no buffers\"); } (6). 最后将末尾的两个函数也改一下 void bpin(struct buf* b) { int bid = HASH(b-\u003eblockno); acquire(\u0026bcache.buckets[bid].lock); b-\u003erefcnt++; release(\u0026bcache.buckets[bid].lock); } void bunpin(struct buf* b) { int bid = HASH(b-\u003eblockno); acquire(\u0026bcache.buckets[bid].lock); b-\u003erefcnt--; release(\u0026bcache.buckets[bid].lock); } ","date":"2023-10-31","objectID":"/6.s081_lab8/:2:0","tags":null,"title":"6.s081_lab8","uri":"/6.s081_lab8/"},{"categories":["6.s081"],"content":"Chapter 7 ","date":"2023-10-31","objectID":"/6.s081_lab7/:0:0","tags":null,"title":"6.s081_lab7","uri":"/6.s081_lab7/"},{"categories":["6.s081"],"content":"7.1 多路复用 Xv6通过在两种情况下将每个CPU从一个进程切换到另一个进程来实现多路复用（Multiplexing）。第一：当进程等待设备或管道I/O完成，或等待子进程退出，或在sleep系统调用中等待时，xv6使用睡眠（sleep）和唤醒（wakeup）机制切换。第二：xv6周期性地强制切换以处理长时间计算而不睡眠的进程。这种多路复用产生了每个进程都有自己的CPU的错觉，就像xv6使用内存分配器和硬件页表来产生每个进程都有自己内存的错觉一样。 实现多路复用带来了一些挑战。首先，如何从一个进程切换到另一个进程？尽管上下文切换的思想很简单，但它的实现是xv6中最不透明的代码之一。第二，如何以对用户进程透明的方式强制切换？Xv6使用标准技术，通过定时器中断驱动上下文切换。第三，许多CPU可能同时在进程之间切换，使用一个用锁方案来避免争用是很有必要的。第四，进程退出时必须释放进程的内存以及其他资源，但它不能自己完成所有这一切，因为（例如）它不能在仍然使用自己内核栈的情况下释放它。第五，多核机器的每个核心必须记住它正在执行哪个进程，以便系统调用正确影响对应进程的内核状态。最后，sleep允许一个进程放弃CPU，wakeup允许另一个进程唤醒第一个进程。需要小心避免导致唤醒通知丢失的竞争。Xv6试图尽可能简单地解决这些问题，但结果代码很复杂。 ","date":"2023-10-31","objectID":"/6.s081_lab7/:1:0","tags":null,"title":"6.s081_lab7","uri":"/6.s081_lab7/"},{"categories":["6.s081"],"content":"7.2 代码：上下文切换 图7.1概述了从一个用户进程（旧进程）切换到另一个用户进程（新进程）所涉及的步骤：一个到旧进程内核线程的用户-内核转换（系统调用或中断），一个到当前CPU调度程序线程的上下文切换，一个到新进程内核线程的上下文切换，以及一个返回到用户级进程的陷阱。调度程序在旧进程的内核栈上执行是不安全的：其他一些核心可能会唤醒进程并运行它，而在两个不同的核心上使用同一个栈将是一场灾难，因此xv6调度程序在每个CPU上都有一个专用线程（保存寄存器和栈）。在本节中，我们将研究在内核线程和调度程序线程之间切换的机制。 从一个线程切换到另一个线程需要保存旧线程的CPU寄存器，并恢复新线程先前保存的寄存器；栈指针和程序计数器被保存和恢复的事实意味着CPU将切换栈和执行中的代码。 函数swtch为内核线程切换执行保存和恢复操作。swtch对线程没有直接的了解；它只是保存和恢复寄存器集，称为上下文（contexts）。当某个进程要放弃CPU时，该进程的内核线程调用swtch来保存自己的上下文并返回到调度程序的上下文。每个上下文都包含在一个struct context（*kernel/proc.h*:2）中，这个结构体本身包含在一个进程的struct proc或一个CPU的struct cpu中。Swtch接受两个参数：struct context *old和struct context *new。它将当前寄存器保存在old中，从new中加载寄存器，然后返回。 让我们跟随一个进程通过swtch进入调度程序。我们在第4章中看到，中断结束时的一种可能性是usertrap调用了yield。依次地：Yield调用sched，sched调用swtch将当前上下文保存在p-\u003econtext中，并切换到先前保存在cpu-\u003escheduler（*kernel/proc.c*:517）中的调度程序上下文。 注：当前版本的XV6中调度程序上下文是cpu-\u003econtext Swtch（*kernel/swtch.S*:3）只保存被调用方保存的寄存器（callee-saved registers）；调用方保存的寄存器（caller-saved registers）通过调用C代码保存在栈上（如果需要）。Swtch知道struct context中每个寄存器字段的偏移量。它不保存程序计数器。但swtch保存ra寄存器，该寄存器保存调用swtch的返回地址。现在，swtch从新进程的上下文中恢复寄存器，该上下文保存前一个swtch保存的寄存器值。当swtch返回时，它返回到由ra寄存器指定的指令，即新线程以前调用swtch的指令。另外，它在新线程的栈上返回。 注：关于callee-saved registers和caller-saved registers请回看视频课程LEC5以及文档《Calling Convention》 Note 这里不太容易理解，这里举个课程视频中的例子： 以cc切换到ls为例，且ls此前运行过 XV6将cc程序的内核线程的内核寄存器保存在一个context对象中 因为要切换到ls程序的内核线程，那么ls 程序现在的状态必然是RUNABLE ，表明ls程序之前运行了一半。这同时也意味着： a. ls程序的用户空间状态已经保存在了对应的trapframe中 b. ls程序的内核线程对应的内核寄存器已经保存在对应的context对象中 所以接下来，XV6会恢复ls程序的内核线程的context对象，也就是恢复内核线程的寄存器。 之后ls会继续在它的内核线程栈上，完成它的中断处理程序 恢复ls程序的trapframe中的用户进程状态，返回到用户空间的ls程序中 最后恢复执行ls 在我们的示例中，sched调用swtch切换到cpu-\u003escheduler，即每个CPU的调度程序上下文。调度程序上下文之前通过scheduler对swtch（*kernel/proc.c*:475）的调用进行了保存。当我们追踪swtch到返回时，他返回到scheduler而不是sched，并且它的栈指针指向当前CPU的调用程序栈（scheduler stack） lab7 ","date":"2023-10-31","objectID":"/6.s081_lab7/:2:0","tags":null,"title":"6.s081_lab7","uri":"/6.s081_lab7/"},{"categories":["6.s081"],"content":"Uthread: switching between threads (moderate) 在xv6 book的chapter7中讲的是用户态进程切换到另一个用户态进程，通过在内核中的调度程序去进行切换。在这个实验中我们需要去写一个用户态的调度程序（其中切换寄存器需要在内核中运行）去模拟内核中的调度程序去实现用户态线程之间的切换，所以我们需要自己定义属于thread的context去保存寄存器的值。 定义用户态的上下文结构体tcontext // 用户线程的上下文结构体 struct tcontext { uint64 ra; uint64 sp; // callee-saved uint64 s0; uint64 s1; uint64 s2; uint64 s3; uint64 s4; uint64 s5; uint64 s6; uint64 s7; uint64 s8; uint64 s9; uint64 s10; uint64 s11; }; 修改thread结构体，添加context字段 struct thread { char stack[STACK_SIZE]; /* the thread's stack */ int state; /* FREE, RUNNING, RUNNABLE */ struct tcontext context; /* 用户进程上下文 */ }; 模仿kernel/swtch.S，在kernel/uthread_switch.S中写入以下代码 .text /* * save the old thread's registers, * restore the new thread's registers. */ .globl thread_switch thread_switch: /* YOUR CODE HERE */ sd ra, 0(a0) sd sp, 8(a0) sd s0, 16(a0) sd s1, 24(a0) sd s2, 32(a0) sd s3, 40(a0) sd s4, 48(a0) sd s5, 56(a0) sd s6, 64(a0) sd s7, 72(a0) sd s8, 80(a0) sd s9, 88(a0) sd s10, 96(a0) sd s11, 104(a0) ld ra, 0(a1) ld sp, 8(a1) ld s0, 16(a1) ld s1, 24(a1) ld s2, 32(a1) ld s3, 40(a1) ld s4, 48(a1) ld s5, 56(a1) ld s6, 64(a1) ld s7, 72(a1) ld s8, 80(a1) ld s9, 88(a1) ld s10, 96(a1) ld s11, 104(a1) ret /* return to ra */ 修改thread_scheduler，添加线程切换语句 ... if (current_thread != next_thread) { /* switch threads? */ ... /* YOUR CODE HERE */ thread_switch((uint64)\u0026t-\u003econtext, (uint64)\u0026current_thread-\u003econtext); } else next_thread = 0; 在thread_create中对thread结构体做一些初始化设定，主要是ra返回地址和sp栈指针，其他的都不重要，将回调函数放在thread的返回地址上来让第一次调用thread_scheduler时可以调用该线程的回调函数 // YOUR CODE HERE t-\u003econtext.ra = (uint64)func; // 设定函数返回地址 t-\u003econtext.sp = (uint64)t-\u003estack + STACK_SIZE; // 设定栈指针 ","date":"2023-10-31","objectID":"/6.s081_lab7/:3:0","tags":null,"title":"6.s081_lab7","uri":"/6.s081_lab7/"},{"categories":["6.s081"],"content":"Using threads 来看一下程序的运行过程：设定了五个散列桶，根据键除以5的余数决定插入到哪一个散列桶中，插入方法是头插法，下面是图示 不支持在 Docs 外粘贴 block 这个实验比较简单，首先是问为什么为造成数据丢失： 假设现在有两个线程T1和T2，两个线程都走到put函数，且假设两个线程中key%NBUCKET相等，即要插入同一个散列桶中。两个线程同时调用insert(key, value, \u0026table[i], table[i])，insert是通过头插法实现的。如果先insert的线程还未返回另一个线程就开始insert，那么前面的数据会被覆盖 因此只需要对插入操作上锁即可 为每个散列桶定义一个锁，将五个锁放在一个数组中，并进行初始化 pthread_mutex_t lock[NBUCKET] = { PTHREAD_MUTEX_INITIALIZER }; // 每个散列桶一把锁 (2). 在put函数中对insert上锁 if(e){ // update the existing key. e-\u003evalue = value; } else { pthread_mutex_lock(\u0026lock[i]); // the new is new. insert(key, value, \u0026table[i], table[i]); pthread_mutex_unlock(\u0026lock[i]); } Barrier 保证在所有线程到达之前barrier之前不会有线程先退出barrier，否则会导致断言函数abort static void barrier() { // 申请持有锁 pthread_mutex_lock(\u0026bstate.barrier_mutex); bstate.nthread++; if(bstate.nthread == nthread) { // 所有线程已到达 bstate.round++; bstate.nthread = 0; pthread_cond_broadcast(\u0026bstate.barrier_cond); } else { // 等待其他线程 // 调用pthread_cond_wait时，mutex必须已经持有 pthread_cond_wait(\u0026bstate.barrier_cond, \u0026bstate.barrier_mutex); } // 释放锁 pthread_mutex_unlock(\u0026bstate.barrier_mutex); } ","date":"2023-10-31","objectID":"/6.s081_lab7/:4:0","tags":null,"title":"6.s081_lab7","uri":"/6.s081_lab7/"},{"categories":["6.s081"],"content":"Implement copy-on write (hard) YOUR JOB 您的任务是在xv6内核中实现copy-on-write fork。如果修改后的内核同时成功执行cowtest和usertests程序就完成了。 为了帮助测试你的实现方案，我们提供了一个名为cowtest的xv6程序（源代码位于*user/cowtest.c*）。cowtest运行各种测试，但在未修改的xv6上，即使是第一个测试也会失败。因此，最初您将看到： $ cowtest simple: fork() failed $ “simple”测试分配超过一半的可用物理内存，然后执行一系列的fork()。fork失败的原因是没有足够的可用物理内存来为子进程提供父进程内存的完整副本。 完成本实验后，内核应该通过cowtest和usertests中的所有测试。即： $ cowtest simple: ok simple: ok three: zombie! ok three: zombie! ok three: zombie! ok file: ok ALL COW TESTS PASSED $ usertests ... ALL TESTS PASSED $ 这是一个合理的攻克计划： 修改uvmcopy()将父进程的物理页映射到子进程，而不是分配新页。在子进程和父进程的PTE中清除PTE_W标志。 修改usertrap()以识别页面错误。当COW页面出现页面错误时，使用kalloc()分配一个新页面，并将旧页面复制到新页面，然后将新页面添加到PTE中并设置PTE_W。 确保每个物理页在最后一个PTE对它的引用撤销时被释放——而不是在此之前。这样做的一个好方法是为每个物理页保留引用该页面的用户页表数的“引用计数”。当kalloc()分配页时，将页的引用计数设置为1。当fork导致子进程共享页面时，增加页的引用计数；每当任何进程从其页表中删除页面时，减少页的引用计数。kfree()只应在引用计数为零时将页面放回空闲列表。可以将这些计数保存在一个固定大小的整型数组中。你必须制定一个如何索引数组以及如何选择数组大小的方案。例如，您可以用页的物理地址除以4096对数组进行索引，并为数组提供等同于***kalloc.c***中kinit()在空闲列表中放置的所有页面的最高物理地址的元素数。 修改copyout()在遇到COW页面时使用与页面错误相同的方案。 提示： lazy page allocation实验可能已经让您熟悉了许多与copy-on-write相关的xv6内核代码。但是，您不应该将这个实验室建立在您的lazy allocation解决方案的基础上；相反，请按照上面的说明从一个新的xv6开始。 有一种可能很有用的方法来记录每个PTE是否是COW映射。您可以使用RISC-V PTE中的RSW（reserved for software，即为软件保留的）位来实现此目的。 usertests检查cowtest不测试的场景，所以别忘两个测试都需要完全通过。 ***kernel/riscv.h***的末尾有一些有用的宏和页表标志位的定义。 如果出现COW页面错误并且没有可用内存，则应终止进程。 (1). 在***kernel/riscv.h***中选取PTE中的保留位定义标记一个页面是否为COW Fork页面的标志位 // 记录应用了COW策略后fork的页面 #define PTE_F (1L \u003c\u003c 8) (2). 在***kalloc.c***中进行如下修改 定义引用计数的全局变量ref，其中包含了一个自旋锁和一个引用计数数组，由于ref是全局变量，会被自动初始化为全0。 这里使用自旋锁是考虑到这种情况：进程P1和P2共用内存M，M引用计数为2，此时CPU1要执行fork产生P1的子进程，CPU2要终止P2，那么假设两个CPU同时读取引用计数为2，执行完成后CPU1中保存的引用计数为3，CPU2保存的计数为1，那么后赋值的语句会覆盖掉先赋值的语句，从而产生错误 struct ref_stru { struct spinlock lock; int cnt[PHYSTOP / PGSIZE]; // 引用计数 } ref; 在kinit中初始化ref的自旋锁 void kinit() { initlock(\u0026kmem.lock, \"kmem\"); initlock(\u0026ref.lock, \"ref\"); freerange(end, (void*)PHYSTOP); } 修改kalloc和kfree函数，在kalloc中初始化内存引用计数为1，在kfree函数中对内存引用计数减1，如果引用计数为0时才真正删除 void kfree(void *pa) { struct run *r; if(((uint64)pa % PGSIZE) != 0 || (char*)pa \u003c end || (uint64)pa \u003e= PHYSTOP) panic(\"kfree\"); // 只有当引用计数为1了才回收空间，--后为0 // 否则只是将引用计数减1 acquire(\u0026ref.lock); if(--ref.cnt[(uint64)pa / PGSIZE] == 0) { release(\u0026ref.lock); r = (struct run*)pa; // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); acquire(\u0026kmem.lock); r-\u003enext = kmem.freelist; kmem.freelist = r; release(\u0026kmem.lock); } else { release(\u0026ref.lock); } } void * kalloc(void) { struct run *r; acquire(\u0026kmem.lock); r = kmem.freelist; if(r) { kmem.freelist = r-\u003enext; acquire(\u0026ref.lock); ref.cnt[(uint64)r / PGSIZE] = 1; // 将引用计数初始化为1 release(\u0026ref.lock); } release(\u0026kmem.lock); if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r; } 添加如下四个函数，详细说明已在注释中，这些函数中用到了walk，记得在defs.h*中添加声明，最后也需要将这些函数的声明添加到defs.h*，在cowalloc中，读取内存引用计数，如果为1，说明只有当前进程引用了该物理内存（其他进程此前已经被分配到了其他物理页面），就只需要改变PTE使能PTE_W；否则就分配物理页面，并将原来的内存引用计数减1。该函数需要返回物理地址，这将在copyout中使用到。 /** * @brief cowpage 判断一个页面是否为COW页面 * @param pagetable 指定查询的页表 * @param va 虚拟地址 * @return 0 是 -1 不是 */ int cowpage(pagetable_t pagetable, uint64 va) { if(va \u003e= MAXVA) return -1; pte_t* pte = walk(pagetable, va, 0); if(pte == 0) return -1; if((*pte \u0026 PTE_V) == 0) return -1; return (*pte \u0026 PTE_F ? 0 : -1); } /** * @brief cowalloc copy-on-write分配器 * @param pagetable 指定页表 * @param va 指定的虚拟地址,必须页面对齐 * @return 分配后va对应的物理地址，如果返回0则分配失败 */ void* cowalloc(pagetable_t pagetable, uint64 va) { if(va % PGSIZE != 0) return 0; uint64 pa = walkaddr(pagetable, va); // 获取对应的物理地址 if(pa == 0) return 0; pte_t* pte = walk(pagetable, va, 0); // 获取对应的PTE if(krefcnt((char*)pa) == 1) { // 只剩一个进程对此物理地址存在引用 // 则直接修改对应的PTE即可 *pte |= PTE_W; *pte \u0026= ~PTE_F; return (void*)pa; } else { // 多个进程对物理内存存在引用 // 需要分配新的页面，并拷贝旧页面的内容 char* mem = kalloc(); if(mem == 0) return 0; // 复制旧页面内容到新页 memmove(mem, (char*)pa, PGSIZE); // 清除PTE_V，否则在mappagges中会判定为remap，因为mappages只为PTE_V为0的page添加映射 *pte \u0026= ~PTE_V; // 为新页面添加映射 if(mappages(pagetable, va, PGSIZE, (uint64)mem, (PTE_FLAGS(*pte) | PTE_W) \u0026 ~PTE_F) != 0) ","date":"2023-10-31","objectID":"/6.s081_lab6/:0:0","tags":null,"title":"6.s081_lab6","uri":"/6.s081_lab6/"},{"categories":["6.s081"],"content":"lab5 ","date":"2023-10-31","objectID":"/6.s081_lab5/:0:0","tags":null,"title":"6.s081_lab5","uri":"/6.s081_lab5/"},{"categories":["6.s081"],"content":"Eliminate allocation from sbrk 将sys_sbrk()中的growproc函数调用删除，因为其是给新增加的堆空间分配内存的，我们现在需要惰性分配，所以并不真正分配空间。 uint64 sys_sbrk(void) { int addr; int n; if(argint(0, \u0026n) \u003c 0) return -1; addr = myproc()-\u003esz; // lazy allocation myproc()-\u003esz += n; return addr; } 未分配的页表比如512项中只分配了两项，那么其余的均为0x00000000000000000。 ","date":"2023-10-31","objectID":"/6.s081_lab5/:1:0","tags":null,"title":"6.s081_lab5","uri":"/6.s081_lab5/"},{"categories":["6.s081"],"content":"Lazy allocation (1). 修改usertrap()(*kernel/trap.c*)函数，使用r_scause()判断是否为页面错误，在页面错误处理的过程中，先判断发生错误的虚拟地址（r_stval()读取）是否位于栈空间之上，进程大小（虚拟地址从0开始，进程大小表征了进程的最高虚拟地址）之下，然后分配物理内存并添加映射 uint64 cause = r_scause(); if(cause == 8) { ... } else if((which_dev = devintr()) != 0) { // ok } else if(cause == 13 || cause == 15) { // 处理页面错误 uint64 fault_va = r_stval(); // 产生页面错误的虚拟地址 char* pa; // 分配的物理地址 if(PGROUNDUP(p-\u003etrapframe-\u003esp) - 1 \u003c fault_va \u0026\u0026 fault_va \u003c p-\u003esz \u0026\u0026 (pa = kalloc()) != 0) { memset(pa, 0, PGSIZE); if(mappages(p-\u003epagetable, PGROUNDDOWN(fault_va), PGSIZE, (uint64)pa, PTE_R | PTE_W | PTE_X | PTE_U) != 0) { kfree(pa); p-\u003ekilled = 1; } } else { // printf(\"usertrap(): out of memory!\\n\"); p-\u003ekilled = 1; } } else { ... } (2). 修改uvmunmap()(*kernel/vm.c*)，之所以修改这部分代码是因为lazy allocation中首先并未实际分配内存，所以当解除映射关系的时候对于这部分内存要略过，而不是使系统崩溃，这部分在课程视频中已经解答。 void uvmunmap(pagetable_t pagetable, uint64 va, uint64 npages, int do_free) { ... for(a = va; a \u003c va + npages*PGSIZE; a += PGSIZE){ if((pte = walk(pagetable, a, 0)) == 0) panic(\"uvmunmap: walk\"); if((*pte \u0026 PTE_V) == 0) continue; ... } } ","date":"2023-10-31","objectID":"/6.s081_lab5/:2:0","tags":null,"title":"6.s081_lab5","uri":"/6.s081_lab5/"},{"categories":["6.s081"],"content":"Lazytests and Usertests (1). 处理sbrk()参数为负数的情况，参考之前sbrk()调用的growproc()程序，如果为负数，就调用uvmdealloc()函数，但需要限制缩减后的内存空间不能小于0 uint64 sys_sbrk(void) { int addr; int n; if(argint(0, \u0026n) \u003c 0) return -1; struct proc* p = myproc(); addr = p-\u003esz; uint64 sz = p-\u003esz; if(n \u003e 0) { // lazy allocation p-\u003esz += n; } else if(sz + n \u003e 0) { sz = uvmdealloc(p-\u003epagetable, sz, sz + n); p-\u003esz = sz; } else { return -1; } return addr; } (2). 正确处理fork的内存拷贝：fork调用了uvmcopy进行内存拷贝，所以修改uvmcopy如下 int uvmcopy(pagetable_t old, pagetable_t new, uint64 sz) { ... for(i = 0; i \u003c sz; i += PGSIZE){ if((pte = walk(old, i, 0)) == 0) continue; if((*pte \u0026 PTE_V) == 0) continue; ... } ... } (3). 还需要继续修改uvmunmap，否则会运行出错，关于为什么要使用两个continue，请看本文最下面 void uvmunmap(pagetable_t pagetable, uint64 va, uint64 npages, int do_free) { ... for(a = va; a \u003c va + npages*PGSIZE; a += PGSIZE){ if((pte = walk(pagetable, a, 0)) == 0) continue; if((*pte \u0026 PTE_V) == 0) continue; ... } } (4). 处理通过sbrk申请内存后还未实际分配就传给系统调用使用的情况，系统调用的处理会陷入内核，scause寄存器存储的值是8，如果此时传入的地址还未实际分配，就不能走到上文usertrap中判断scause是13或15后进行内存分配的代码，syscall执行就会失败 系统调用流程： 陷入内核**==\u003eusertrap中r_scause()==8的分支==\u003esyscall()==\u003e**回到用户空间 页面错误流程： 陷入内核**==\u003eusertrap中r_scause()==13||r_scause()==15的分支==\u003e分配内存==\u003e**回到用户空间 因此就需要找到在何时系统调用会使用这些地址，将地址传入系统调用后，会通过argaddr函数(*kernel/syscall.c*)从寄存器中读取，因此在这里添加物理内存分配的代码 int argaddr(int n, uint64 *ip) { *ip = argraw(n); struct proc* p = myproc(); // 处理向系统调用传入lazy allocation地址的情况 if(walkaddr(p-\u003epagetable, *ip) == 0) { if(PGROUNDUP(p-\u003etrapframe-\u003esp) - 1 \u003c *ip \u0026\u0026 *ip \u003c p-\u003esz) { char* pa = kalloc(); if(pa == 0) return -1; memset(pa, 0, PGSIZE); if(mappages(p-\u003epagetable, PGROUNDDOWN(*ip), PGSIZE, (uint64)pa, PTE_R | PTE_W | PTE_X | PTE_U) != 0) { kfree(pa); return -1; } } else { return -1; } } return 0; } ","date":"2023-10-31","objectID":"/6.s081_lab5/:3:0","tags":null,"title":"6.s081_lab5","uri":"/6.s081_lab5/"},{"categories":["6.s081"],"content":"Chapter 4 ","date":"2023-10-31","objectID":"/6.s081_lab4/:0:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"4.1RISC-V陷入机制 每个RISC-V CPU都有一组控制寄存器，内核通过向这些寄存器写入内容来告诉CPU如何处理陷阱，内核可以读取这些寄存器来明确已经发生的陷阱。RISC-V文档包含了完整的内容。*riscv.h*(*kernel/riscv.h*:1)包含在xv6中使用到的内容的定义。以下是最重要的一些寄存器概述： stvec：内核在这里写入其陷阱处理程序的地址；RISC-V跳转到这里处理陷阱。 sepc：当发生陷阱时，RISC-V会在这里保存程序计数器pc（因为pc会被stvec覆盖）。sret（从陷阱返回）指令会将sepc复制到pc。内核可以写入sepc来控制sret的去向。 scause： RISC-V在这里放置一个描述陷阱原因的数字。 sscratch：内核在这里放置了一个值，这个值在陷阱处理程序一开始就会派上用场。 sstatus：其中的SIE位控制设备中断是否启用。如果内核清空SIE，RISC-V将推迟设备中断，直到内核重新设置SIE。SPP位指示陷阱是来自用户模式还是管理模式，并控制sret返回的模式。 上述寄存器都用于在管理模式下处理陷阱，在用户模式下不能读取或写入。在机器模式下处理陷阱有一组等效的控制寄存器，xv6仅在计时器中断的特殊情况下使用它们。 多核芯片上的每个CPU都有自己的这些寄存器集，并且在任何给定时间都可能有多个CPU在处理陷阱。 当需要强制执行陷阱时，RISC-V硬件对所有陷阱类型（计时器中断除外）执行以下操作： 如果陷阱是设备中断，并且状态SIE位被清空，则不执行以下任何操作。 清除SIE以禁用中断。 将pc复制到sepc。 将当前模式（用户或管理）保存在状态的SPP位中。 设置scause以反映产生陷阱的原因。 将模式设置为管理模式。 将stvec复制到pc。 在新的pc上开始执行。 请注意，CPU不会切换到内核页表，不会切换到内核栈，也不会保存除pc之外的任何寄存器。内核软件必须执行这些任务。CPU在陷阱期间执行尽可能少量工作的一个原因是为软件提供灵活性；例如，一些操作系统在某些情况下不需要页表切换，这可以提高性能。 你可能想知道CPU硬件的陷阱处理顺序是否可以进一步简化。例如，假设CPU不切换程序计数器。那么陷阱可以在仍然运行用户指令的情况下切换到管理模式。但因此这些用户指令可以打破用户/内核的隔离机制，例如通过修改satp寄存器(保存页表根地址的寄存器)来指向允许访问所有物理内存的页表。因此，CPU使用专门的寄存器切换到内核指定的指令地址，即stvec，是很重要的。 ","date":"2023-10-31","objectID":"/6.s081_lab4/:1:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"4.2从用户空间陷入 如果用户程序发出系统调用（ecall指令），或者做了一些非法的事情，或者设备中断，那么在用户空间中执行时就可能会产生陷阱。来自用户空间的陷阱的高级路径是uservec (*kernel/trampoline.S*:16)，然后是usertrap (*kernel/trap.c*:37)；返回时，先是usertrapret (*kernel/trap.c*:90)，然后是userret (*kernel/trampoline.S*:16)。 来自用户代码的陷阱比来自内核的陷阱更具挑战性，因为satp指向不映射内核的用户页表，栈指针可能包含无效甚至恶意的值。 由于RISC-V硬件在陷阱期间不会切换页表，所以用户页表必须包括uservec（stvec指向的陷阱向量指令）的映射。uservec必须切换satp以指向内核页表；为了在切换后继续执行指令，uservec必须在内核页表中与用户页表中映射相同的地址。 xv6使用包含uservec的蹦床页面（trampoline page）来满足这些约束。xv6将蹦床页面映射到内核页表和每个用户页表中相同的虚拟地址。这个虚拟地址是TRAMPOLINE（如图2.3和图3.3所示）。蹦床内容在***trampoline.S***中设置，并且（当执行用户代码时）stvec设置为uservec (*kernel/trampoline.S*:16)。 当uservec启动时，所有32个寄存器都包含被中断代码所拥有的值。但是uservec需要能够修改一些寄存器，以便设置satp并生成保存寄存器的地址。RISC-V以sscratch寄存器的形式提供了帮助。uservec开始时的csrrw指令交换了a0和sscratch的内容。现在用户代码的a0被保存了；uservec有一个寄存器（a0）可以使用；a0包含内核以前放在sscratch中的值。 uservec的下一个任务是保存用户寄存器。在进入用户空间之前，内核先前将sscratch设置为指向一个每个进程的trapframe，该帧（除此之外）具有保存所有用户寄存器的空间(*kernel/proc.h*:44)。因为satp仍然指向用户页表，所以uservec需要将trapframe映射到用户地址空间中。每当创建一个进程时，xv6就为该进程的trapframe分配一个页面，并安排它始终映射在用户虚拟地址TRAPFRAME，该地址就在TRAMPOLINE下面。尽管使用物理地址，该进程的p-\u003etrapframe仍指向trapframe，这样内核就可以通过内核页表使用它。 因此在交换a0和sscratch之后，a0持有指向当前进程trapframe的指针。uservec现在保存那里的所有用户寄存器，包括从sscratch读取的用户的a0。 陷阱帧包含指向当前进程内核栈的指针、当前CPU的hartid、usertrap的地址和内核页表的地址。uservec取得这些值，将satp切换到内核页表，并调用usertrap。 usertrap的任务是确定陷阱的原因，处理并返回(*kernel/trap.c*:37)。如上所述，它首先改变stvec，这样内核中的陷阱将由kernelvec处理。它保存了sepc（保存的用户程序计数器），再次保存是因为usertrap中可能有一个进程切换，可能导致sepc被覆盖。如果陷阱来自系统调用，syscall会处理它；如果是设备中断，devintr会处理；否则它是一个异常，内核会杀死错误进程。系统调用路径在保存的用户程序计数器pc上加4，因为在系统调用的情况下，RISC-V会留下指向ecall指令的程序指针（返回后需要执行ecall之后的下一条指令）。在退出的过程中，usertrap检查进程是已经被杀死还是应该让出CPU（如果这个陷阱是计时器中断）。 返回用户空间的第一步是调用usertrapret (*kernel/trap.c*:90)。该函数设置RISC-V控制寄存器，为将来来自用户空间的陷阱做准备。这涉及到将stvec更改为指向uservec，准备uservec所依赖的陷阱帧字段，并将sepc设置为之前保存的用户程序计数器。最后，usertrapret在用户和内核页表中都映射的蹦床页面上调用userret；原因是userret中的汇编代码会切换页表。 usertrapret对userret的调用将指针传递到a0中的进程用户页表和a1中的TRAPFRAME (*kernel/trampoline.S*:88)。userret将satp切换到进程的用户页表。回想一下，用户页表同时映射蹦床页面和TRAPFRAME，但没有从内核映射其他内容。同样，蹦床页面映射在用户和内核页表中的同一个虚拟地址上的事实允许用户在更改satp后继续执行。userret复制陷阱帧保存的用户a0到sscratch，为以后与TRAPFRAME的交换做准备。从此刻开始，userret可以使用的唯一数据是寄存器内容和陷阱帧的内容。下一个userret从陷阱帧中恢复保存的用户寄存器，做a0与sscratch的最后一次交换来恢复用户a0并为下一个陷阱保存TRAPFRAME，并使用sret返回用户空间。 ","date":"2023-10-31","objectID":"/6.s081_lab4/:2:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"4.3 代码：调用系统调用 第2章以***initcode.S***调用exec系统调用（*user/initcode.S*:11）结束。让我们看看用户调用是如何在内核中实现exec系统调用的。 用户代码将exec需要的参数放在寄存器a0和a1中，并将系统调用号放在a7中。系统调用号与syscalls数组中的条目相匹配，syscalls数组是一个函数指针表（*kernel/syscall.c*:108）。ecall指令陷入(trap)到内核中，执行uservec、usertrap和syscall，和我们之前看到的一样。 syscall（*kernel/syscall.c*:133）从陷阱帧（trapframe）中保存的a7中检索系统调用号（p-\u003etrapframe-\u003ea7），并用它索引到syscalls中，对于第一次系统调用，a7中的内容是SYS_exec（*kernel/syscall. h*:8），导致了对系统调用接口函数sys_exec的调用。 当系统调用接口函数返回时，syscall将其返回值记录在p-\u003etrapframe-\u003ea0中。这将导致原始用户空间对exec()的调用返回该值，因为RISC-V上的C调用约定将返回值放在a0中。系统调用通常返回负数表示错误，返回零或正数表示成功。如果系统调用号无效，syscall打印错误并返回-1。 ","date":"2023-10-31","objectID":"/6.s081_lab4/:3:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"4.4 系统调用参数 内核中的系统调用接口需要找到用户代码传递的参数。因为用户代码调用了系统调用封装函数，所以参数最初被放置在RISC-V C调用所约定的地方：寄存器。内核陷阱代码将用户寄存器保存到当前进程的陷阱框架中，内核代码可以在那里找到它们。函数artint、artaddr和artfd从陷阱框架中检索第n个系统调用参数并以整数、指针或文件描述符的形式保存。他们都调用argraw来检索相应的保存的用户寄存器（*kernel/syscall.c*:35）。 有些系统调用传递指针作为参数，内核必须使用这些指针来读取或写入用户内存。例如：exec系统调用传递给内核一个指向用户空间中字符串参数的指针数组。这些指针带来了两个挑战。首先，用户程序可能有缺陷或恶意，可能会传递给内核一个无效的指针，或者一个旨在欺骗内核访问内核内存而不是用户内存的指针。其次，xv6内核页表映射与用户页表映射不同，因此内核不能使用普通指令从用户提供的地址加载或存储。 内核实现了安全地将数据传输到用户提供的地址和从用户提供的地址传输数据的功能。fetchstr是一个例子（*kernel/syscall.c*:25）。文件系统调用，如exec，使用fetchstr从用户空间检索字符串文件名参数。fetchstr调用copyinstr来完成这项困难的工作。 copyinstr（*kernel/vm.c*:406）从用户页表页表中的虚拟地址srcva复制max字节到dst。它使用walkaddr（它又调用walk）在软件中遍历页表，以确定srcva的物理地址pa0。由于内核将所有物理RAM地址映射到同一个内核虚拟地址，copyinstr可以直接将字符串字节从pa0复制到dst。walkaddr（*kernel/vm.c*:95）检查用户提供的虚拟地址是否为进程用户地址空间的一部分，因此程序不能欺骗内核读取其他内存。一个类似的函数copyout，将数据从内核复制到用户提供的地址。 ","date":"2023-10-31","objectID":"/6.s081_lab4/:4:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"4.5 从内核空间陷入 xv6根据执行的是用户代码还是内核代码，对CPU陷阱寄存器的配置有所不同。当在CPU上执行内核时，内核将stvec指向kernelvec(*kernel/kernelvec.S*:10)的汇编代码。由于xv6已经在内核中，kernelvec可以依赖于设置为内核页表的satp，以及指向有效内核栈的栈指针。kernelvec保存所有寄存器，以便被中断的代码最终可以不受干扰地恢复。 kernelvec将寄存器保存在被中断的内核线程的栈上，这是有意义的，因为寄存器值属于该线程。如果陷阱导致切换到不同的线程，那这一点就显得尤为重要——在这种情况下，陷阱将实际返回到新线程的栈上，将被中断线程保存的寄存器安全地保存在其栈上。 Kernelvec在保存寄存器后跳转到kerneltrap(*kernel/trap.c*:134)。kerneltrap为两种类型的陷阱做好了准备：设备中断和异常。它调用devintr(*kernel/trap.c*:177)来检查和处理前者。如果陷阱不是设备中断，则必定是一个异常，内核中的异常将是一个致命的错误；内核调用panic并停止执行。 如果由于计时器中断而调用了kerneltrap，并且一个进程的内核线程正在运行（而不是调度程序线程），kerneltrap会调用yield，给其他线程一个运行的机会。在某个时刻，其中一个线程会让步，让我们的线程和它的kerneltrap再次恢复。第7章解释了yield中发生的事情。 当kerneltrap的工作完成后，它需要返回到任何被陷阱中断的代码。因为一个yield可能已经破坏了保存的sepc和在sstatus中保存的前一个状态模式，因此kerneltrap在启动时保存它们。它现在恢复这些控制寄存器并返回到kernelvec(*kernel/kernelvec.S*:48)。kernelvec从栈中弹出保存的寄存器并执行sret，将sepc复制到pc并恢复中断的内核代码。 值得思考的是，如果内核陷阱由于计时器中断而调用yield，陷阱返回是如何发生的。 当CPU从用户空间进入内核时，xv6将CPU的stvec设置为kernelvec；您可以在usertrap(*kernel/trap.c*:29)中看到这一点。内核执行时有一个时间窗口，但stvec设置为uservec，在该窗口中禁用设备中断至关重要。幸运的是，RISC-V总是在开始设置陷阱时禁用中断，xv6在设置stvec之前不会再次启用中断 ","date":"2023-10-31","objectID":"/6.s081_lab4/:5:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"4.6 页面错误异常 Xv6对异常的响应相当无趣: 如果用户空间中发生异常，内核将终止故障进程。如果内核中发生异常，则内核会崩溃。真正的操作系统通常以更有趣的方式做出反应。 例如，许多内核使用页面错误来实现写时拷贝版本的fork——copy on write (COW) fork。要解释COW fork，请回忆第3章内容：xv6的fork通过调用uvmcopy(*kernel/vm.c*:309) 为子级分配物理内存，并将父级的内存复制到其中，使子级具有与父级相同的内存内容。如果父子进程可以共享父级的物理内存，则效率会更高。然而武断地实现这种方法是行不通的，因为它会导致父级和子级通过对共享栈和堆的写入来中断彼此的执行。 由页面错误驱动的COW fork可以使父级和子级安全地共享物理内存。当CPU无法将虚拟地址转换为物理地址时，CPU会生成页面错误异常。Risc-v有三种不同的页面错误: 加载页面错误 (当加载指令无法转换其虚拟地址时)，存储页面错误 (当存储指令无法转换其虚拟地址时) 和指令页面错误 (当指令的地址无法转换时)。scause寄存器中的值指示页面错误的类型，stval寄存器包含无法翻译的地址。 COW fork中的基本计划是让父子最初共享所有物理页面，但将它们映射为只读。因此，当子级或父级执行存储指令时，risc-v CPU引发页面错误异常。为了响应此异常，内核复制了包含错误地址的页面。它在子级的地址空间中映射一个权限为读/写的副本，在父级的地址空间中映射另一个权限为读/写的副本。更新页表后，内核会在导致故障的指令处恢复故障进程的执行。由于内核已经更新了相关的PTE以允许写入，所以错误指令现在将正确执行。 COW策略对fork很有效，因为通常子进程会在fork之后立即调用exec，用新的地址空间替换其地址空间。在这种常见情况下，子级只会触发很少的页面错误，内核可以避免拷贝父进程内存完整的副本。此外，COW fork是透明的: 无需对应用程序进行任何修改即可使其受益。 除COW fork以外，页表和页面错误的结合还开发出了广泛有趣的可能性。另一个广泛使用的特性叫做惰性分配——*lazy allocation。*它包括两部分内容：首先，当应用程序调用sbrk时，内核增加地址空间，但在页表中将新地址标记为无效。其次，对于包含于其中的地址的页面错误，内核分配物理内存并将其映射到页表中。由于应用程序通常要求比他们需要的更多的内存，惰性分配可以称得上一次胜利: 内核仅在应用程序实际使用它时才分配内存。像COW fork一样，内核可以对应用程序透明地实现此功能。 利用页面故障的另一个广泛使用的功能是从磁盘分页。如果应用程序需要比可用物理RAM更多的内存，内核可以换出一些页面: 将它们写入存储设备 (如磁盘)，并将它们的PTE标记为无效。如果应用程序读取或写入被换出的页面，则CPU将触发页面错误。然后内核可以检查故障地址。如果该地址属于磁盘上的页面，则内核分配物理内存页面，将该页面从磁盘读取到该内存，将PTE更新为有效并引用该内存，然后恢复应用程序。为了给页面腾出空间，内核可能需要换出另一个页面。此功能不需要对应用程序进行更改，并且如果应用程序具有引用的地址 (即，它们在任何给定时间仅使用其内存的子集)，则该功能可以很好地工作。 结合分页和页面错误异常的其他功能包括自动扩展栈空间和内存映射文件。 lab4 ","date":"2023-10-31","objectID":"/6.s081_lab4/:6:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"RISC-V assembly 理解一点RISC-V汇编是很重要的，你应该在6.004中接触过。xv6仓库中有一个文件user/call.c*。执行make fs.img编译它，并在**user/call.asm***中生成可读的汇编版本。 阅读**call.asm*中函数g、f和main的代码。RISC-V的使用手册在参考页上。以下是您应该回答的一些问题（将答案存储在answers-traps.txt***文件中）： 哪些寄存器保存函数的参数？例如，在main对printf的调用中，哪个寄存器保存13？ main的汇编代码中对函数f的调用在哪里？对g的调用在哪里(提示：编译器可能会将函数内联） printf函数位于哪个地址？ 在main中printf的jalr之后的寄存器ra中有什么值？ 运行以下代码。 unsigned int i = 0x00646c72; printf(\"H%x Wo%s\", 57616, \u0026i); 程序的输出是什么？这是将字节映射到字符的ASCII码表。 输出取决于RISC-V小端存储的事实。如果RISC-V是大端存储，为了得到相同的输出，你会把i设置成什么？是否需要将57616更改为其他值？ 这里有一个小端和大端存储的描述和一个更异想天开的描述。 在下面的代码中，“y=”之后将打印什么(注：答案不是一个特定的值）？为什么会发生这种情况？ printf(\"x=%d y=%d\", 3); 在a0-a7中存放参数，13存放在a2中 在C代码中，main调用f，f调用g。而在生成的汇编中，main函数进行了内联优化处理。从代码li a1,12可以看出，main直接计算出了结果并储存 在0x616 auipc(Add Upper Immediate to PC)：auipc rd imm，将高位立即数加到PC上，从下面的指令格式可以看出，该指令将20位的立即数左移12位之后（右侧补0）加上PC的值，将结果保存到dest位置，图中为rd`寄存器 ​ 下面来看jalr (jump and link register)：jalr rd, offset(rs1)跳转并链接寄存器。jalr指令会将当前PC+4 保存在rd中，然后跳转到指定的偏移地址offset(rs1)。 30: 00000097 auipc ra,0x0 34: 5e6080e7 jalr 1510(ra) # 616 \u003cprintf\u003e 第一列代表pc的值，第二列代表指令，最后一列是详细指令。 第一行pc值为0x30，指令是00000097H=00...0 0000 1001 0111B，对比指令格式，可见imm=0，dest=00001，opcode=0010111，对比汇编指令可知，auipc的操作码是0010111，ra寄存器代码是00001。这行代码将0x0左移12位（还是0x0）加到PC（当前为0x30）上并存入ra中，即ra中保存的是0x30。 第2行pc值为0x34，指令是5e6080e7H=0101 1110 0110 0000 1000 0000 1110 0111B，offset=0101 1110 0110，rs1=00001，rd=00001。rs1和rd都为寄存器ra。因此现在pc的值为x[ra]+offset即0x30+0x5e6=0x616,即printf的地址。并将PC+4=0x34+4=0x38保存在ra中 ​ 57616=0xE110，0x00646c72小端存储为72-6c-64-00，对照ASCII码表 72:r 6c:l 64:d 00:充当字符串结尾标识 因此输出为：HE110 World 若为大端存储，i应改为0x726c6400，不需改变57616 取决于寄存器a2保存的值 ","date":"2023-10-31","objectID":"/6.s081_lab4/:7:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"Backtrace 在kernel/defs.h中添加backtrace的原型。 GCC编译器将当前正在执行的函数的帧指针保存在s0寄存器，将下面的函数添加到*kernel/riscv.h* static inline uint64 r_fp() { uint64 x; asm volatile(\"mv %0, s0\" : \"=r\" (x) ); return x; } 并在backtrace中调用此函数来读取当前的帧指针。这个函数使用内联汇编来读取s0 最后完成backtrace函数 首先先来认识下帧栈（stack frame） 每一次函数的调用,都会在调用栈(call stack)上维护一个独立的栈帧(stack frame).每个独立的栈帧一般包括: 函数的返回地址和参数 临时变量: 包括函数的非静态局部变量以及编译器自动生成的其他临时变量 函数调用的上下文 栈是从高地址向低地址延伸,一个函数的栈帧用ebp 和 esp 这两个寄存器来划定范围.ebp 指向当前的栈帧的底部,esp 始终指向栈帧的顶部; ebp 寄存器又被称为帧指针(Frame Pointer); esp 寄存器又被称为栈指针(Stack Pointer); 在xv6中，返回地址位于frame pointer（fp）固定偏移-8的位置（fp-8,fp）。并且保存的前一个fp指针在固定偏移-16的位置（fp-16,fp-8） xv6在内核中以页面对齐的地址为每个栈分配一个页面，所以可以通过PGROUNDUP/DOWN函数来判断帧栈页面是否有效。 如果有效，那么分别获取返回地址(fp-8)和前一个fp指针的地址(fp-16) 最终可以打印函数返回地址 /** * @brief backtrace 回溯函数调用的返回地址 */ void backtrace(void) { printf(\"backtrace:\\n\"); // 读取当前帧指针 uint64 fp = r_fp(); while (PGROUNDUP(fp) - PGROUNDDOWN(fp) == PGSIZE) { // 返回地址保存在-8偏移的位置 uint64 ret_addr = *(uint64*)(fp - 8); printf(\"%p\\n\", ret_addr); // 前一个帧指针保存在-16偏移的位置 fp = *(uint64*)(fp - 16); } } ","date":"2023-10-31","objectID":"/6.s081_lab4/:8:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"Alarm ","date":"2023-10-31","objectID":"/6.s081_lab4/:9:0","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"test0 程序计数器的过程是这样的： ecall指令中将PC保存到SEPC 在usertrap中将SEPC保存到p-\u003etrapframe-\u003eepc p-\u003etrapframe-\u003eepc加4指向下一条指令 执行系统调用 在usertrapret中将SEPC改写为p-\u003etrapframe-\u003eepc中的值 在sret中将PC设置为SEPC的值 可见执行系统调用后返回到用户空间继续执行的指令地址是由p-\u003etrapframe-\u003eepc决定的，因此在usertrap中主要就是完成它的设置工作。 (1). 在struct proc中增加字段，同时记得在allocproc中将它们初始化为0，并在freeproc中也设为0 int alarm_interval; // 报警间隔 void (*alarm_handler)(); // 报警处理函数 int ticks_count; // 两次报警间的滴答计数 (2). 在sys_sigalarm中读取参数 uint64 sys_sigalarm(void) { if(argint(0, \u0026myproc()-\u003ealarm_interval) \u003c 0 || argaddr(1, (uint64*)\u0026myproc()-\u003ealarm_handler) \u003c 0) return -1; return 0; } (3). 修改usertrap()，将函数指针赋值给trapframe-\u003eepc，然后将ticks_count改为0 // give up the CPU if this is a timer interrupt. if(which_dev == 2) { if(++p-\u003eticks_count == p-\u003ealarm_interval) { // 更改陷阱帧中保留的程序计数器 p-\u003etrapframe-\u003eepc = (uint64)p-\u003ealarm_handler; p-\u003eticks_count = 0; } yield(); } ","date":"2023-10-31","objectID":"/6.s081_lab4/:9:1","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"Test1\u0026test2 考虑一下没有alarm时运行的大致过程 进入内核空间，保存用户寄存器到进程陷阱帧 陷阱处理过程 恢复用户寄存器，返回用户空间 而当添加了alarm后，变成了以下过程 进入内核空间，保存用户寄存器到进程陷阱帧 陷阱处理过程 恢复用户寄存器，返回用户空间，但此时返回的并不是进入陷阱时的程序地址，而是处理函数handler的地址（因为修改了trapframe的epc寄存器），而handler可能会改变用户寄存器（比如trapframe的epc寄存器） 因此我们要在usertrap中再次保存用户寄存器，当handler调用sigreturn时将其恢复，并且要防止在handler执行过程中重复调用，过程如下 (1). 再在struct proc中新增两个字段 int is_alarming; // 是否正在执行告警处理函数 struct trapframe* alarm_trapframe; // 告警陷阱帧 (2). 在allocproc和freeproc中设定好相关分配，回收内存的代码 /** * allocproc.c */ // 初始化告警字段 if((p-\u003ealarm_trapframe = (struct trapframe*)kalloc()) == 0) { freeproc(p); release(\u0026p-\u003elock); return 0; } p-\u003eis_alarming = 0; p-\u003ealarm_interval = 0; p-\u003ealarm_handler = 0; p-\u003eticks_count = 0; /** * freeproc.c */ if(p-\u003ealarm_trapframe) kfree((void*)p-\u003ealarm_trapframe); p-\u003ealarm_trapframe = 0; p-\u003eis_alarming = 0; p-\u003ealarm_interval = 0; p-\u003ealarm_handler = 0; p-\u003eticks_count = 0; (3). 更改usertrap函数，保存进程陷阱帧p-\u003etrapframe到p-\u003ealarm_trapframe // give up the CPU if this is a timer interrupt. if(which_dev == 2) { if(p-\u003ealarm_interval != 0 \u0026\u0026 ++p-\u003eticks_count == p-\u003ealarm_interval \u0026\u0026 p-\u003eis_alarming == 0) { // 保存寄存器内容 memmove(p-\u003ealarm_trapframe, p-\u003etrapframe, sizeof(struct trapframe)); // 更改陷阱帧中保留的程序计数器，注意一定要在保存寄存器内容后再设置epc p-\u003etrapframe-\u003eepc = (uint64)p-\u003ealarm_handler; p-\u003eticks_count = 0; p-\u003eis_alarming = 1; } yield(); } (4). 更改sys_sigreturn，恢复陷阱帧 uint64 sys_sigreturn(void) { memmove(myproc()-\u003etrapframe, myproc()-\u003ealarm_trapframe, sizeof(struct trapframe)); myproc()-\u003eis_alarming = 0; return 0; } ","date":"2023-10-31","objectID":"/6.s081_lab4/:9:2","tags":null,"title":"6.s081_lab4","uri":"/6.s081_lab4/"},{"categories":["6.s081"],"content":"Chapter 3 ","date":"2023-10-31","objectID":"/6.s081_lab3/:0:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.1页式硬件 该树的根是一个4096字节的页表页，其中包含512个PTE，每个PTE中包含该树下一级页表页的物理地址（PPN）。这些页中的每一个PTE都包含该树最后一级的512个PTE。分别使用virtual address 的L2,L1,L0各9位（2^9=512）去确定页表页的哪一个PTE。最终的Physical address是由最后一级的PPN和Virtual address的最开始的12位offset组成。 每个PTE包含10位的标志位。这些标志位告诉分页硬件允许如何使用关联的虚拟地址。PTE_V指示PTE是否存在：如果它没有被设置，对页面的引用会导致异常（即不允许）。PTE_R控制是否允许指令读取到页面。PTE_W控制是否允许指令写入到页面。PTE_X控制CPU是否可以将页面内容解释为指令并执行它们。PTE_U控制用户模式下的指令是否被允许访问页面；如果没有设置PTE_U，PTE只能在管理模式下使用。图3.2显示了它是如何工作的。标志和所有其他与页面硬件相关的结构在（*kernel/riscv.h*）中定义。 为了告诉硬件使用页表，内核必须将根页表页的物理地址写入到satp寄存器中（satp的作用是存放根页表页在物理内存中的地址）。每个CPU都有自己的satp，一个CPU将使用自己的satp指向的页表转换后续指令生成的所有地址。每个CPU都有自己的satp，因此不同的CPU就可以运行不同的进程，每个进程都有自己的页表描述的私有地址空间。 ","date":"2023-10-31","objectID":"/6.s081_lab3/:1:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.2内核地址空间 Xv6为每个进程维护一个页表，用以描述每个进程的用户地址空间，外加一个单独描述内核地址空间的页表。内核配置其地址空间的布局，以允许自己以可预测的虚拟地址访问物理内存和各种硬件资源。图3.3显示了这种布局如何将内核虚拟地址映射到物理地址。文件(*kernel/memlayout.h*) 声明了xv6内核内存布局的常量。 QEMU模拟了一台计算机，它包括从物理地址0x80000000开始并至少到0x86400000结束的RAM（物理内存），xv6称结束地址为PHYSTOP。QEMU模拟还包括I/O设备，如磁盘接口。QEMU将设备接口作为内存映射控制寄存器暴露给软件，这些寄存器位于物理地址空间0x80000000以下。内核可以通过读取/写入这些特殊的物理地址与设备交互；这种读取和写入与设备硬件而不是RAM通信。第4章解释了xv6如何与设备进行交互。 内核使用“直接映射”获取内存和内存映射设备寄存器；也就是说，将资源映射到等于物理地址的虚拟地址。例如，内核本身在虚拟地址空间和物理内存中都位于KERNBASE=0x80000000。直接映射简化了读取或写入物理内存的内核代码。例如，当fork为子进程分配用户内存时，分配器返回该内存的物理地址；fork在将父进程的用户内存复制到子进程时直接将该地址用作虚拟地址。 有几个内核虚拟地址不是直接映射： 蹦床页面(trampoline page)。它映射在虚拟地址空间的顶部；用户页表具有相同的映射。第4章讨论了蹦床页面的作用，但我们在这里看到了一个有趣的页表用例；一个物理页面（持有蹦床代码）在内核的虚拟地址空间中映射了两次：一次在虚拟地址空间的顶部，一次直接映射。 内核栈页面。每个进程都有自己的内核栈，它将映射到偏高一些的地址，这样xv6在它之下就可以留下一个未映射的保护页(guard page)。保护页的PTE是无效的（也就是说PTE_V没有设置），所以如果内核溢出内核栈就会引发一个异常，内核触发panic。如果没有保护页，栈溢出将会覆盖其他内核内存，引发错误操作。恐慌崩溃（panic crash）是更可取的方案。（注：Guard page不会浪费物理内存，它只是占据了虚拟地址空间的一段靠后的地址，但并不映射到物理地址空间。） ","date":"2023-10-31","objectID":"/6.s081_lab3/:2:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.3创建一个地址空间 大多数用于操作地址空间和页表的xv6代码都写在 vm.c* (kernel/vm.c:1) 中。其核心数据结构是pagetable_t，它实际上是指向RISC-V根页表页的指针；一个pagetable_t可以是内核页表，也可以是一个进程页表。最核心的函数是walk和mappages，前者为虚拟地址找到PTE，后者为新映射装载PTE。名称以kvm开头的函数操作内核页表；以uvm开头的函数操作用户页表；其他函数用于二者。copyout和copyin复制数据到用户虚拟地址或从用户虚拟地址复制数据，这些虚拟地址作为系统调用参数提供; 由于它们需要显式地翻译这些地址，以便找到相应的物理内存，故将它们写在**vm.c***中。 在启动的初期，main会调用kvminit，kvminit首先会分配一个物理内存页用来装根页表，后续调用kvmmap去安装内核需要的内存，这些转换包括 内核的指令和数据，到PHYSTOP的物理内存，以及实际上是设备的内存范围。实际上是设备。（即上图中的CLINT PLIC UARTO VIRTIO KERNBASE PHYSTOP TRAMPOLINE） kvmmap(*kernel/vm.c*:127)调用mappages(*kernel/vm.c*:138)，mappages将范围虚拟地址到同等范围物理地址的映射装载到一个页表中。它以页面大小为间隔，为范围内的每个虚拟地址单独执行此操作。对于要映射的每个虚拟地址，mappages调用walk来查找该地址的PTE地址。然后，它初始化PTE以保存相关的物理页号、所需权限（PTE_W、PTE_X和/或PTE_R）以及用于标记PTE有效的PTE_V(*kernel/vm.c*:153)。先通过walk找到PTE，然后通过mappages装载PTE 在查找PTE中的虚拟地址（参见图3.2）时，walk(*kernel/vm.c*:72)模仿RISC-V分页硬件。walk一次从3级页表中获取9个比特位。它使用上一级的9位虚拟地址来查找下一级页表或最终页面的PTE (*kernel/vm.c*:78)。如果PTE无效，则所需的页面还没有分配；如果设置了alloc参数，walk就会分配一个新的页表页面，并将其物理地址放在PTE中。它返回树中最低一级的PTE地址(*kernel/vm.c*:88)。 上面的代码依赖于直接映射到内核虚拟地址空间中的物理内存。例如，当walk降低页表的级别时，它从PTE (*kernel/vm.c*:80)中提取下一级页表的（物理）地址，然后使用该地址作为虚拟地址来获取下一级的PTE (*kernel/vm.c*:78)。 for(int level = 2; level \u003e 0; level--) { pte_t *pte = \u0026pagetable[PX(level, va)]; if(*pte \u0026 PTE_V) { pagetable = (pagetable_t)PTE2PA(*pte); } else { if(!alloc || (pagetable = (pde_t*)kalloc()) == 0) return 0; memset(pagetable, 0, PGSIZE); *pte = PA2PTE(pagetable) | PTE_V; } } 由于虚拟地址和物理地址直接映射，在这个函数中把va先当做虚拟地址获取第一个9位来获取根页表的PTE，获取到二级页表的物理地址后（每页4096B，所以低12位为0，获取pte之后由于pte的低10位是标志位，pte»10«12便是物理地址），将这个地址转换为虚拟地址即是二级页表的虚拟地址，然后便可以在通过va»12+level*9获取到PTE进而找到最后一级页表 ","date":"2023-10-31","objectID":"/6.s081_lab3/:3:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.4物理内存分配 内核必须在运行时为页表、用户内存、内核栈和管道缓冲区分配和释放物理内存。xv6使用内核末尾到PHYSTOP之间的物理内存进行运行时分配。它一次分配和释放整个4096字节的页面。它使用链表的数据结构将空闲页面记录下来。分配时需要从链表中删除页面；释放时需要将释放的页面添加到链表中。 ","date":"2023-10-31","objectID":"/6.s081_lab3/:4:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.5代码：物理内存分配 分配器(allocator)位于*kalloc.c*(*kernel/kalloc.c*:1)中。分配器的数据结构是可供分配的物理内存页的空闲列表。每个空闲页的列表元素是一个struct run(*kernel/kalloc.c*:17)。分配器从哪里获得内存来填充该数据结构呢？它将每个空闲页的run结构存储在空闲页本身，因为在那里没有存储其他东西。 main函数调用kinit(*kernel/kalloc.c*:27)来初始化分配器。kinit初始化空闲列表以保存从内核结束到PHYSTOP之间的每一页。xv6应该通过解析硬件提供的配置信息来确定有多少物理内存可用。然而，xv6假设机器有128兆字节的RAM。kinit调用freerange将内存添加到空闲列表中，在freerange中每页都会调用kfree。PTE只能引用在4096字节边界上对齐的物理地址（是4096的倍数），所以freerange使用PGROUNDUP来确保它只释放对齐的物理地址。分配器开始时没有内存；这些对kfree的调用给了它一些管理空间。 分配器有时将地址视为整数，以便对其执行算术运算（例如，在freerange中遍历所有页面），有时将地址用作读写内存的指针（例如，操纵存储在每个页面中的run结构）；这种地址的双重用途是分配器代码充满C类型转换的主要原因。另一个原因是释放和分配从本质上改变了内存的类型。 函数kfree (*kernel/kalloc.c*:47)首先将内存中的每一个字节设置为1。这将导致使用释放后的内存的代码（使用“悬空引用”）读取到垃圾信息而不是旧的有效内容，从而希望这样的代码更快崩溃。然后kfree将页面前置（头插法）到空闲列表中：它将pa转换为一个指向struct run的指针r，在r-\u003enext中记录空闲列表的旧开始，并将空闲列表设置为等于r。 kalloc删除并返回空闲列表中的第一个元素。 ","date":"2023-10-31","objectID":"/6.s081_lab3/:5:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.6进程地址空间 每个进程都有一个单独的页表，当xv6在进程之间切换时，也会更改页表。如图2.3所示，一个进程的用户内存从虚拟地址零开始，可以增长到MAXVA (*kernel/riscv.h*:348)，原则上允许一个进程内存寻址空间为256G。 当进程向xv6请求更多的用户内存时，xv6首先使用kalloc来分配物理页面。然后，它将PTE添加到进程的页表中，指向新的物理页面。Xv6在这些PTE中设置PTE_W、PTE_X、PTE_R、PTE_U和PTE_V标志。大多数进程不使用整个用户地址空间；xv6在未使用的PTE中留空PTE_V。 我们在这里看到了一些使用页表的很好的例子。首先，不同进程的页表将用户地址转换为物理内存的不同页面，这样每个进程都拥有私有内存。第二，每个进程看到的自己的内存空间都是以0地址起始的连续虚拟地址，而进程的物理内存可以是非连续的。第三，内核在用户地址空间的顶部映射一个带有蹦床（trampoline）代码的页面，这样在所有地址空间都可以看到一个单独的物理内存页面。 图3.4更详细地显示了xv6中执行态进程的用户内存布局。栈是单独一个页面，显示的是由exec创建后的初始内容。包含命令行参数的字符串以及指向它们的指针数组位于栈的最顶部。再往下是允许程序在main处开始启动的值（即main的地址、argc、argv），这些值产生的效果就像刚刚调用了main(argc, argv)一样。 为了检测用户栈是否溢出了所分配栈内存，xv6在栈正下方放置了一个无效的保护页（guard page）。如果用户栈溢出并且进程试图使用栈下方的地址，那么由于映射无效（PTE_V为0）硬件将生成一个页面故障异常。当用户栈溢出时，实际的操作系统可能会自动为其分配更多内存。 ","date":"2023-10-31","objectID":"/6.s081_lab3/:6:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.7 sbrk sbrk是一个用于进程减少或增长其内存的系统调用。这个系统调用由函数growproc实现(*kernel/proc.c*:239)。growproc根据n是正的还是负的调用uvmalloc或uvmdealloc。uvmalloc(*kernel/vm.c*:229)用kalloc分配物理内存，并用mappages将PTE添加到用户页表中。uvmdealloc调用uvmunmap(*kernel/vm.c*:174)，uvmunmap使用walk来查找对应的PTE，并使用kfree来释放PTE引用的物理内存。 XV6使用进程的页表，不仅是告诉硬件如何映射用户虚拟地址，也是明晰哪一个物理页面已经被分配给该进程的唯一记录。这就是为什么释放用户内存（在uvmunmap中）需要检查用户页表的原因。 在heap中获取内存 ","date":"2023-10-31","objectID":"/6.s081_lab3/:7:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"3.8 exec Stack是单独一个页面，显示的是由exec创建后的初始内容 lab3 allocproc分配进程，而在procinit中所有的内核栈都在其中设置，把这个功能迁移到allocproc中，为proc中新增的字段kernel_pagetable赋值 lab3 ","date":"2023-10-31","objectID":"/6.s081_lab3/:8:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"Print a page table 定义一个名为vmprint()的函数。它应当接收一个pagetable_t作为参数，并以下面描述的格式打印该页表。在exec.c中的return argc之前插入if(p-\u003epid==1) vmprint(p-\u003epagetable)，以打印第一个进程的页表。如果你通过了pte printout测试的make grade，你将获得此作业的满分。 首先现在exec.c的return argc之前插入if(p-\u003epid==1) vmprint(p-\u003epagetable) 然后观察kernel/vm.c中的freewalk方法 // Recursively free page-table pages. // All leaf mappings must already have been removed. void freewalk(pagetable_t pagetable) { // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i \u003c 512; i++){ pte_t pte = pagetable[i]; if((pte \u0026 PTE_V) \u0026\u0026 (pte \u0026 (PTE_R|PTE_W|PTE_X)) == 0){ // this PTE points to a lower-level page table. uint64 child = PTE2PA(pte); freewalk((pagetable_t)child); pagetable[i] = 0; } else if(pte \u0026 PTE_V){ panic(\"freewalk: leaf\"); } } kfree((void*)pagetable); } 首先会遍历第一级页表，当遇到有效的页表并且不是最后一级，就会递归。RWX均为0表示不是最后一层，因为最后一层页表中的页表项至少有一个为1 根据freewalk函数便可去仿照写出vmprint函数 /** * @param pagetable 所要打印的页表 * @param level 页表的层级 */ void _vmprint(pagetable_t pagetable, int level){ // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i \u003c 512; i++){ pte_t pte = pagetable[i]; // PTE_V is a flag for whether the page table is valid if(pte \u0026 PTE_V){ for (int j = 0; j \u003c level; j++){ if (j) printf(\" \"); printf(\"..\"); } uint64 child = PTE2PA(pte); printf(\"%d: pte %p pa %p\\n\", i, pte, child); if((pte \u0026 (PTE_R|PTE_W|PTE_X)) == 0){ // this PTE points to a lower-level page table. _vmprint((pagetable_t)child, level + 1); } } } } /** * @brief vmprint 打印页表 * @param pagetable 所要打印的页表 */ void vmprint(pagetable_t pagetable){ printf(\"page table %p\\n\", pagetable); _vmprint(pagetable, 1); } 在_vmprint中，首先判断是否有效，并根据level打印出对应的..表示第几层，如果其还有下层页表，那么就递归调用_vmprint 最后在kernel/defs.h加上新增函数的声明 void vmprint(pagetable_t); ","date":"2023-10-31","objectID":"/6.s081_lab3/:9:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"A kernel page table per process Xv6有一个单独的用于在内核中执行程序时的内核页表。内核页表直接映射（恒等映射）到物理地址，也就是说内核虚拟地址x映射到物理地址仍然是x。Xv6还为每个进程的用户地址空间提供了一个单独的页表，只包含该进程用户内存的映射，从虚拟地址0开始。因为内核页表不包含这些映射，所以用户地址在内核中无效。因此，当内核需要使用在系统调用中传递的用户指针（例如，传递给write()的缓冲区指针）时，内核必须首先将指针转换为物理地址。本节和下一节的目标是允许内核直接解引用用户指针。 你的第一项工作是修改内核来让每一个进程在内核中执行时使用它自己的内核页表的副本。修改struct proc来为每一个进程维护一个内核页表，修改调度程序使得切换进程时也切换内核页表。对于这个步骤，每个进程的内核页表都应当与现有的的全局内核页表完全一致。如果你的usertests程序正确运行了，那么你就通过了这个实验。 首先在kernel/proc.h的proc结构体中添加进程的内核页表成员变量 pagetable_t kernelpt; // Kernel page table 需要初始化进程的内核页表，在vm.c中添加一个新的函数proc_kpt_init，用于在allocproc中，另外需要一个辅助函数uvmmap与kvmmap类似，kvmmap用于对内核的内核页表进行映射，而uvmmap用于对进程的内核页表进行映射。 // Just follow the kvmmap on vm.c void uvmmap(pagetable_t pagetable, uint64 va, uint64 pa, uint64 sz, int perm) { if(mappages(pagetable, va, sz, pa, perm) != 0) panic(\"uvmmap\"); } // Create a kernel page table for the process pagetable_t proc_kpt_init(){ pagetable_t kernelpt = uvmcreate(); if (kernelpt == 0) return 0; uvmmap(kernelpt, UART0, UART0, PGSIZE, PTE_R | PTE_W); uvmmap(kernelpt, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W); uvmmap(kernelpt, CLINT, CLINT, 0x10000, PTE_R | PTE_W); uvmmap(kernelpt, PLIC, PLIC, 0x400000, PTE_R | PTE_W); uvmmap(kernelpt, KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X); uvmmap(kernelpt, (uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W); uvmmap(kernelpt, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X); return kernelpt; } 然后在kernel/proc.c里面的allocproc调用。 ... // An empty user page table. p-\u003epagetable = proc_pagetable(p); if(p-\u003epagetable == 0){ freeproc(p); release(\u0026p-\u003elock); return 0; } // Init the kernal page table p-\u003ekernelpt = proc_kpt_init(); if(p-\u003ekernelpt == 0){ freeproc(p); release(\u0026p-\u003elock); return 0; } ... 为了确保每一个进程的内核页表都关于该进程的内核栈有一个映射。我们需要将procinit方法中相关的代码迁移到allocproc方法中。很明显就是下面这段代码，将其剪切到上述内核页表初始化的代码后。 // Allocate a page for the process's kernel stack. // Map it high in memory, followed by an invalid // guard page. char *pa = kalloc(); if(pa == 0) panic(\"kalloc\"); uint64 va = KSTACK((int) (p - proc)); uvmmap(p-\u003ekernelpt, va, (uint64)pa, PGSIZE, PTE_R | PTE_W); p-\u003ekstack = va; 修改scheduler()来讲进程的内核页表加载进SATP寄存器中。 // Switch h/w page table register to the kernel's page table, // and enable paging. void kvminithart() { w_satp(MAKE_SATP(kernel_pagetable)); sfence_vma(); } kvminithart是用于原先的内核页表，我们将进程的内核页表传进去就可以。在vm.c里面添加一个新方法proc_inithart。 // Store kernel page table to SATP register void proc_inithart(pagetable_t kpt){ w_satp(MAKE_SATP(kpt)); sfence_vma(); } 然后在scheduler()内调用即可，但在结束的时候，需要切换回原先的kernel_pagetable。直接调用调用上面的kvminithart()就能把Xv6的内核页表加载回去。 ... p-\u003estate = RUNNING; c-\u003eproc = p; // Store the kernal page table into the SATP proc_inithart(p-\u003ekernelpt); swtch(\u0026c-\u003econtext, \u0026p-\u003econtext); // Come back to the global kernel page table kvminithart(); ... 在freeproc中释放进程的内核页表，首先需要释放页表内的内核栈，调用uvmunmap可以解除虚拟地址到物理地址的映射并将物理内存释放 // free the kernel stack in the RAM uvmunmap(p-\u003ekernelpt, p-\u003ekstack, 1, 1); p-\u003ekstack = 0; 然后释放进程的内核页表，先在kernel/proc.c里面添加一个方法proc_freekernelpt。如下，历遍整个内核页表，然后将所有有效的页表项清空为零。如果这个页表项不在最后一层的页表上，需要继续进行递归，每次遍历完页表之后就会将其物理空间释放。 void proc_freekernelpt(pagetable_t kernelpt) { // similar to the freewalk method // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i \u003c 512; i++){ pte_t pte = kernelpt[i]; if(pte \u0026 PTE_V){ kernelpt[i] = 0; if ((pte \u0026 (PTE_R|PTE_W|PTE_X)) == 0){ uint64 child = PTE2PA(pte); proc_freekernelpt((pagetable_t)child); } } } kfree((void*)kernelpt); } 修改vm.c中的kvmpa，将原先的kernel_pagetable改成myproc()-\u003ekernelpt，使用进程的内核页表。 #include \"spinlock.h\" #include \"proc.h\" uint64 kvmpa(uint64 va) { uint64 off = va % PGSIZE; pte_t *pte; uint64 pa; pte = walk(myproc()-\u003ekernelpt, va, 0); // 修改这里 if(pte == 0) panic(\"kvmpa\"); if((*pte \u0026 PTE_V) == 0) panic(\"kvmpa\"); pa = PTE2PA(*pte); return pa+off; } 最后讲定义的函数加入kernel/defs.h void proc_freekernelpt(pagetable_t ); void uvmmap(pagetable_t, uint64, uint64, uint64, int); pagetable_t proc_kpt_init(void); // 用于内核页表的初始化 void proc_inithart(pagetable_t); // 将进程的内核页表保存到SATP寄存器 ","date":"2023-10-31","objectID":"/6.s081_lab3/:10:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"copyin/copyinstr 第三个实验是讲用户空间的映射添加到内核页表，目的就是用户空间传递的地址内核可以直接解引用，如果不这么做那么用户态传递的指针，内核需要转换为物理地址之后才能够进行memmove，修改之后memmove直接可以用两个地址即可。 该方案依赖于用户虚拟地址范围不与内核用于其自己的指令和数据的虚拟地址范围重叠。xv6 为用户地址空间使用从零开始的虚拟地址，幸运的是内核的内存从更高的地址开始。但是，该方案确实将用户进程的最大大小限制为小于内核的最低虚拟地址。内核启动后，该地址在 xv6 中为0xC000000，即 PLIC 寄存器的地址； 此图是内核空间布局图 下图是用户空间布局图，其虚拟地址不可超过PLIC 首先添加复制函数。需要注意的是，在内核模式下，无法访问设置了PTE_U的页面，所以我们要将其移除。 void u2kvmcopy(pagetable_t pagetable, pagetable_t kernelpt, uint64 oldsz, uint64 newsz){ pte_t *pte_from, *pte_to; oldsz = PGROUNDUP(oldsz); for (uint64 i = oldsz; i \u003c newsz; i += PGSIZE){ if((pte_from = walk(pagetable, i, 0)) == 0) panic(\"u2kvmcopy: src pte does not exist\"); if((pte_to = walk(kernelpt, i, 1)) == 0) panic(\"u2kvmcopy: pte walk failed\"); uint64 pa = PTE2PA(*pte_from); uint flags = (PTE_FLAGS(*pte_from)) \u0026 (~PTE_U); *pte_to = PA2PTE(pa) | flags; } } 然后在内核更改进程的用户映射的每一处 （fork(), exec(), 和sbrk()），都复制一份到进程的内核页表。 exec()： int exec(char *path, char **argv){ ... sp = sz; stackbase = sp - PGSIZE; // 添加复制逻辑 u2kvmcopy(pagetable, p-\u003ekernelpt, 0, sz); // Push argument strings, prepare rest of stack in ustack. for(argc = 0; argv[argc]; argc++) { ... } fork(): int fork(void){ ... // Copy user memory from parent to child. if(uvmcopy(p-\u003epagetable, np-\u003epagetable, p-\u003esz) \u003c 0){ freeproc(np); release(\u0026np-\u003elock); return -1; } np-\u003esz = p-\u003esz; ... // 复制到新进程的内核页表 u2kvmcopy(np-\u003epagetable, np-\u003ekernelpt, 0, np-\u003esz); ... } sbrk()， 在kernel/sysproc.c里面找到sys_sbrk(void)，可以知道只有growproc是负责将用户内存增加或缩小 n 个字节。以防止用户进程增长到超过PLIC的地址，我们需要给它加个限制。 int growproc(int n) { uint sz; struct proc *p = myproc(); sz = p-\u003esz; if(n \u003e 0){ // 加上PLIC限制 if (PGROUNDUP(sz + n) \u003e= PLIC){ return -1; } if((sz = uvmalloc(p-\u003epagetable, sz, sz + n)) == 0) { return -1; } // 复制一份到内核页表 u2kvmcopy(p-\u003epagetable, p-\u003ekernelpt, sz - n, sz); } else if(n \u003c 0){ sz = uvmdealloc(p-\u003epagetable, sz, sz + n); } p-\u003esz = sz; return 0; } 更改userinit和copyin、copyinstr p-\u003esz = PGSIZE; u2kvmcopy(p-\u003epagetable, p-\u003ekernelpt, 0, p-\u003esz); // Copy from user to kernel. // Copy len bytes to dst from virtual address srcva in a given page table. // Return 0 on success, -1 on error. int copyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len) { return copyin_new(pagetable, dst, srcva, len); } // Copy a null-terminated string from user to kernel. // Copy bytes to dst from virtual address srcva in a given page table, // until a '\\0', or max. // Return 0 on success, -1 on error. int copyinstr(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max) { return copyinstr_new(pagetable, dst, srcva, max); } 最后将copyin、copyinstr添加至kernel/defs.h // vmcopyin.c int copyin_new(pagetable_t, char *, uint64, uint64); int copyinstr_new(pagetable_t, char *, uint64, uint64); ","date":"2023-10-31","objectID":"/6.s081_lab3/:11:0","tags":null,"title":"6.s081_lab3","uri":"/6.s081_lab3/"},{"categories":["6.s081"],"content":"chapter 2 ","date":"2023-10-31","objectID":"/6.s081_lab2/:1:0","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"2.2用户态、核心态、系统调用 RISC-V分为三个模式machine mode,supervisor mode and user mode 在机器模式下执行的指令具有完全权限；CPU以机器模式启动。机器模式主要用于配置计算机。Xv6在机器模式下执行几行，然后切换到管理模式。 ","date":"2023-10-31","objectID":"/6.s081_lab2/:1:1","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"2.3内核组织 整个os都驻留在内核中，所有系统调用的实现都是以supervisor mode运行，这个被称为amonolithic内核（宏内核） 为了降低内核出错的风险，操作系统设计者可以最大限度地减少在监督模式下运行的操作系统代码的数量，并在用户模式下执行大部分操作系统。这个内核组织被称为amicrokernel（微内核） ","date":"2023-10-31","objectID":"/6.s081_lab2/:1:2","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"2.5进程概述 Risc-v的指针是64位的，硬件仅使用低39位；而xv6仅使用这39个比特中的38个。因此，最大地址为238−1=0x3fffffffff xv6内核为每个进程维护许多状态片段，并将它们聚集到一个proc(*kernel/proc.h*:86)结构体中。一个进程最重要的内核状态片段是它的页表、内核栈区和运行状态。我们将使用符号p-\u003exxx来引用proc结构体的元素；例如，p-\u003epagetable是一个指向该进程页表的指针。 线程的大部分状态（局部变量、函数调用返回地址）都存储在线程的堆栈中。每个进程有两个堆栈：一个用户堆栈和一个内核堆栈（p-\u003ekstack）。当进程执行用户指令时，只有它的用户堆栈在使用，而它的内核堆栈是空的。当进程进入内核（用于系统调用或中断）时，内核代码在进程的内核堆栈上执行；当进程在内核中时，它的用户堆栈仍然包含保存的数据，但没有被实际使用。进程的线程在主动使用其用户堆栈和内核堆栈之间交替。内核堆栈是独立的（并受用户代码保护），因此即使进程破坏了其用户堆栈，内核也可以执行。 ","date":"2023-10-31","objectID":"/6.s081_lab2/:1:3","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"2.6xv6启动和运行第一个线程的概述 为了使xv6更加具体，我们将概述内核如何启动和运行第一个进程。接下来的章节将更详细地描述本概述中显示的机制。 当RISC-V计算机上电时，它会初始化自己并运行一个存储在只读内存中的引导加载程序。引导加载程序将xv6内核加载到内存中。然后，在机器模式下，中央处理器从_entry (*kernel/entry.S*:6)开始运行xv6。Xv6启动时页式硬件（paging hardware）处于禁用模式：也就是说虚拟地址将直接映射到物理地址。 加载程序将xv6内核加载到物理地址为0x80000000的内存中。它将内核放在0x80000000而不是0x0的原因是地址范围0x0:0x80000000包含I/O设备。 _entry的指令设置了一个栈区，这样xv6就可以运行C代码。Xv6在**start. c (kernel/start.c:11)*文件中为初始栈stack0***声明了空间。由于RISC-V上的栈是向下扩展的，所以_entry的代码将栈顶地址stack0+4096加载到栈顶指针寄存器sp中。现在内核有了栈区，_entry便调用C代码start(*kernel/start.c*:21)。 函数start执行一些仅在机器模式下允许的配置，然后切换到管理模式。RISC-V提供指令mret以进入管理模式，该指令最常用于将管理模式切换到机器模式的调用中返回。而start并非从这样的调用返回，而是执行以下操作：它在寄存器mstatus中将先前的运行模式改为管理模式，它通过将main函数的地址写入寄存器mepc将返回地址设为main，它通过向页表寄存器satp写入0来在管理模式下禁用虚拟地址转换，并将所有的中断和异常委托给管理模式。 在进入管理模式之前，start还要执行另一项任务：对时钟芯片进行编程以产生计时器中断。清理完这些“家务”后，start通过调用mret“返回”到管理模式。这将导致程序计数器（PC）的值更改为main(*kernel/main.c*:11)函数地址。 TIPS 注：mret执行返回，返回到先前状态，由于start函数将前模式改为了管理模式且返回地址改为了main,因此mret将返回到main函数，并以管理模式运行 在main(*kernel/main.c*:11)初始化几个设备和子系统后，便通过调用userinit (*kernel/proc.c*:212)创建第一个进程，第一个进程执行一个用RISC-V程序集写的小型程序：*initcode. S* (***user/initcode.S:***1)，它通过调用exec系统调用重新进入内核。正如我们在第1章中看到的，exec用一个新程序（本例中为 /init）替换当前进程的内存和寄存器。一旦内核完成exec，它就返回/init进程中的用户空间。如果需要，init(*user/init.c*:15)将创建一个新的控制台设备文件，然后以文件描述符0、1和2打开它。然后它在控制台上启动一个shell。系统就这样启动了。 ","date":"2023-10-31","objectID":"/6.s081_lab2/:1:4","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"lab2 ","date":"2023-10-31","objectID":"/6.s081_lab2/:2:0","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"system call tracing trace系统调用有一个参数，这个参数是一个整数“掩码”（mask），它的比特位指定要跟踪的系统调用。例如，要跟踪fork系统调用，程序调用trace(1 \u003c\u003c SYS_fork)，其中SYS_fork是kernel/syscall.h中的系统调用编号，这个掩码是01也即2。32是read的系统调用，因为1«5为32，100000。1111……1共31个1，即2147483647，这个掩码代表追踪所有的系统调用。 我们提供了一个用户级程序版本的trace，它运行另一个启用了跟踪的程序（参见user/trace.c）。完成后，您应该看到如下输出： $ trace 32 grep hello README 3: syscall read -\u003e 1023 3: syscall read -\u003e 966 3: syscall read -\u003e 70 3: syscall read -\u003e 0 $ $ trace 2147483647 grep hello README 4: syscall trace -\u003e 0 4: syscall exec -\u003e 3 4: syscall open -\u003e 3 4: syscall read -\u003e 1023 4: syscall read -\u003e 966 4: syscall read -\u003e 70 4: syscall read -\u003e 0 4: syscall close -\u003e 0 $ $ grep hello README $ $ trace 2 usertests forkforkfork usertests starting test forkforkfork: 407: syscall fork -\u003e 408 408: syscall fork -\u003e 409 409: syscall fork -\u003e 410 410: syscall fork -\u003e 411 409: syscall fork -\u003e 412 410: syscall fork -\u003e 413 409: syscall fork -\u003e 414 411: syscall fork -\u003e 415 ... $ 在上面的第一个例子中，trace调用grep，仅跟踪了read系统调用。32是1\u003c。在第二个示例中，trace在运行grep时跟踪所有系统调用；2147483647将所有31个低位置为1。在第三个示例中，程序没有被跟踪，因此没有打印跟踪输出。在第四个示例中，在usertests中测试的forkforkfork中所有子孙进程的fork`系统调用都被追踪。如果程序的行为如上所示，则解决方案是正确的（尽管进程ID可能不同）。 解决步骤 在Makefile的UPROGS中添加$U/_trace 在user/user.h中添加原型 int trace(int); 添加存根到user/usys.pl entry(\"trace\"); 添加系统调用号到kernel/syscall.h #define SYS_trace 22 在syscall.c中添加trace相关的定义 extern uint64 sys_trace(void); [SYS_trace] sys_trace, 现在make qemu可以通过编译，但是内核中还没有实现系统调用，执行测试trace 32 grep hello README将失败 在kernel/sysproc.c中添加一个sys_trace()函数，它通过将参数保存到proc结构体（请参见kernel/proc.h）里的一个新变量中来实现新的系统调用。从用户空间检索系统调用参数的函数在kernel/syscall.c中。 在proc结构体中添加成员变量mask // Per-process state struct proc { struct spinlock lock; // p-\u003elock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent's wait int pid; // Process ID // these are private to the process, so p-\u003elock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) int mask; // mask for system call }; 通过argint获取到第一个参数，将其赋值给proc的mask成员变量 // realize trace // save params into struct proc uint64 sys_trace(void) { int mask; if (argint(0, \u0026mask) \u003c 0) { return -1; } myproc()-\u003emask = mask; return 0; } 修改fork()（请参阅kernel/proc.c）将跟踪掩码从父进程复制到子进程。在fork函数中将parent process的mask赋值给child process safestrcpy(np-\u003ename, p-\u003ename, sizeof(p-\u003ename)); pid = np-\u003epid; np-\u003estate = RUNNABLE; np-\u003emask = p-\u003emask; release(\u0026np-\u003elock); return pid; 修改kernel/syscall.c中的syscall()函数以打印跟踪输出。您将需要添加一个系统调用名称数组以建立索引。当调用trace的时候才会有mask属性，因此将掩码和系统调用号相与，为1则按要求输出即可。syscall（*kernel/syscall.c*:133）从陷阱帧（trapframe）中保存的a7中检索系统调用号（p-\u003etrapframe-\u003ea7），并用它索引到syscalls中，对于第一次系统调用，a7中的内容是SYS_exec（*kernel/syscall. h*:8），导致了对系统调用接口函数sys_exec的调用。 当系统调用接口函数返回时，syscall将其返回值记录在p-\u003etrapframe-\u003ea0中。这将导致原始用户空间对exec()的调用返回该值，因为RISC-V上的C调用约定将返回值放在a0中。系统调用通常返回负数表示错误，返回零或正数表示成功。如果系统调用号无效，syscall打印错误并返回-1 void syscall(void) { int num; struct proc *p = myproc();//return current process num = p-\u003etrapframe-\u003ea7;//system call number if(num \u003e 0 \u0026\u0026 num \u003c NELEM(syscalls) \u0026\u0026 syscalls[num]) { p-\u003etrapframe-\u003ea0 = syscalls[num]();//return number,if err return fushu if(1\u003c\u003cnum \u0026 p-\u003emask){ printf(\"%d: syscall %s -\u003e %d\\n\",p-\u003epid,sysname[num],p-\u003etrapframe-\u003ea0); } } else { printf(\"%d %s: unknown sys call %d\\n\", p-\u003epid, p-\u003ename, num); p-\u003etrapframe-\u003ea0 = -1;//if syscall number is unknown return -1 } } ","date":"2023-10-31","objectID":"/6.s081_lab2/:2:1","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"Sysinfo 将添加一个系统调用sysinfo，它收集有关正在运行的系统的信息。系统调用采用一个参数：一个指向struct sysinfo的指针（参见kernel/sysinfo.h）。内核应该填写这个结构的字段：freemem字段应该设置为空闲内存的字节数，nproc字段应该设置为state字段不为UNUSED的进程数。我们提供了一个测试程序sysinfotest；如果输出“sysinfotest: OK”则通过。 解决步骤 在Makefile的UPROGS中添加$U/_sysinfotest 在user/user.h中添加原型 int sysinfo(int); 添加存根到user/usys.pl entry(\"sysinfo\"); 添加系统调用号到kernel/syscall.h #define SYS_sysinfo 23 在syscall.c中添加trace相关的定义 extern uint64 sys_sysinfo(void); [SYS_sysinfo] sys_sysinfo, 现在make qemu可以通过编译，但是内核中还没有实现系统调用，执行测试trace 32 grep hello README将失败 sysinfo需要将一个struct sysinfo复制回用户空间；请参阅sys_fstat()(kernel/sysfile.c)和filestat()(kernel/file.c)以获取如何使用copyout()执行此操作的示例。 先定义一个指向用户态struct sysinfo的指针 我们在用户态时会传递一个地址，通过argaddr获取该地址存入sysinfo指针中 定义一个内核态的sysinfo结构体（加头文件），接下来我我们需要通过两个函数为sysinfo的两个成员变量赋值。 最后通过copyout函数将内核态的sysinfo结构体复制到用户态sysinfo指针指向的地址 // Copy from kernel to user. // Copy len bytes from src to virtual address dstva in a given page table. // Return 0 on success, -1 on error. // int // copyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len) // { uint64 sys_sysinfo(void) { uint64 sysinfo; // user pointer to struct sysinfo // get the address of sysinfo in user space if (argaddr(0, \u0026sysinfo) \u003c 0) return -1; struct proc *p = myproc(); //struct sysinfo in kernel struct sysinfo sys_info; //kernel need to fill the struct sysinfo //copy the struct sysinfo in kernel to the pointer to sysinfo in user space if (copyout(p-\u003epagetable, sysinfo, (char *)\u0026sys_info, sizeof(sys_info)) \u003c 0) return -1; return 0; } 要获取空闲内存量，请在kernel/kalloc.c中添加一个函数。定义一个指针p指向记录空闲内存的链表的表头，每次遍历到一个num就+1，最后返回num*PAGESIZE即可 uint64 free_memory(void) { struct run *p = kmem.freelist; uint64 num = 0; while (p) { num++; p = p-\u003enext; } return num * PGSIZE; } 要获取进程数，请在kernel/proc.c中添加一个函数。遍历proc[NPROC]，如果p的状态为UNUSED就为n加一，最后的n即为空闲的process数量。 uint64 free_proc(void) { uint64 n = 0; struct proc *p; for (p = proc; p \u003c \u0026proc[NPROC]; p++) { acquire(\u0026p-\u003elock); if (p-\u003estate != UNUSED) n++; release(\u0026p-\u003elock); } return n; } 最后将刚才的sysinfo系统调用补充完整，同时需要在kernel/defs.h添加上刚才所写的两个函数，才可使用。 //kernel need to fill the struct sysinfo sys_info.freemem=free_memory(); sys_info.nproc=free_proc(); ","date":"2023-10-31","objectID":"/6.s081_lab2/:2:2","tags":null,"title":"6.s081_lab2","uri":"/6.s081_lab2/"},{"categories":["6.s081"],"content":"lab1 ","date":"2023-10-31","objectID":"/6.s081_lab1/:1:0","tags":null,"title":"6.s081_lab1","uri":"/6.s081_lab1/"},{"categories":["6.s081"],"content":"sleep 思路 直接将argv[1]赋值给sleep系统调用即可 #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" int main(int argc, char *argv[]) { char *sleeptime_char; int sleeptime; if (argc \u003c= 1) { printf(\"sleep need one parm\"); exit(1); } if (argc \u003e 2) { printf(\"too many parm\"); exit(1); /* code */ } sleeptime_char = argv[1]; sleeptime = atoi(sleeptime_char); sleep(sleeptime); exit(0); } ","date":"2023-10-31","objectID":"/6.s081_lab1/:1:1","tags":null,"title":"6.s081_lab1","uri":"/6.s081_lab1/"},{"categories":["6.s081"],"content":"pingpong 在pingpong.c中，有两种思路 思路一 只创建一个管道，父子都通过这个管道进行读写，父进程在写入之后必须wait等待子进程读取并在写入之后，在进行读取操作。 #include \"kernel/types.h\" #include \"user.h\" int main(int argc,char* argv[]){ //创建两个管道，分别实现ping、pong的读写 int p[2]; pipe(p); char readtext[10];//作为父进程和子进程的读出容器 //子程序读出 int pid = fork(); if(pid==0){ read(p[0],readtext,10); printf(\"%d: received %s\\n\",getpid(),readtext); write(p[1],\"pong\",10); exit(0);//子进程一定要退出 } //父程序写入 else{ write(p[1],\"ping\",10); wait(0);//父进程阻塞，等待子进程读取 read(p[0],readtext,10); printf(\"%d: received %s\\n\",getpid(),readtext); exit(0);//父进程一定要退出 } return 0; } 思路二 需要建立两个管道分别用到作为读写管道。管道一头只作为读，另一头只作为写。当父子进程都需要读写时，创建两个管道，一个父读子写，一个父写子读。 #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" int main(int argc, char *argv[]) { //0 read 1 write int child_fd[2]; int parent_fd[2]; pipe(child_fd); pipe(parent_fd); if (fork()==0) { char buf[80]; close(child_fd[1]); read(child_fd[0],buf,sizeof(buf)); close(child_fd[0]); printf(\"%d: received p%sng\\n\",getpid(),buf); close(parent_fd[0]); write(parent_fd[1],\"o\",1); close(parent_fd[1]); exit(0); } else { char buf[80]; close(child_fd[0]); write(child_fd[1],\"i\",1); close(child_fd[1]); close(parent_fd[1]); read(parent_fd[0],buf,sizeof(buf)); close(parent_fd[0]); printf(\"%d: received p%sng\\n\",getpid(),buf); exit(0); } return 0; } 其中思路二更好理解，两个管道各司其职 ","date":"2023-10-31","objectID":"/6.s081_lab1/:1:2","tags":null,"title":"6.s081_lab1","uri":"/6.s081_lab1/"},{"categories":["6.s081"],"content":"primes 思路 main函数中负责创建一个管道，将2-35全部写入管道中。在child process中负责调用一个创建子进程的函数。 该函数主要做以下几件事： 读取parent process送入的每个数字，将其打印prime i。不用担心main函数的顺序，因为如果main函数没有write数字也没有关闭写描述符，那么child process的read函数会阻塞。 继续读取parent process管道中的数字，如果没有数字代表不需要在创建child process，退出。如果仍有数字，继续创建child process。 通过fork函数创建child process，在当前的process中需要判断该数字是否为素数，为素数则写入子进程的管道中。只有在数字2创建的进程需要一一判断，后续进程皆为素数产生的进程。 Tips:如果有指向管道写端的文件描述符没有关闭，而持有写端的进程也没有向管道内写入数据的时候，那么管道剩余的数据被读取后，再次read会被阻塞，之后有数据可读会再次返回。 如果所有的写端均被关闭，那么再次read会直接返回0，就像读到了文件末尾一样。 如果管道内没有其他prime，则不再创建子进程 注意wait，必须等到所有child process都结束了parent process才能结束 #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" int isprime(int n) { int j, flag = 1; for (j = 2; j \u003c= n / 2; ++j) { if (n % j == 0) { flag = 0; break; } } return flag; } int create_process(int parent_fd[2]) { char bytes[4]; close(parent_fd[1]); /*如果有指向管道写端的文件描述符没有关闭，而持有写端的进程也没有向管道内 写入数据的时候，那么管道剩余的数据被读取后，再次read会被阻塞，之后有数据 可读会再次返回。 如果所有的写端均被关闭，那么再次read会直接返回0，就像读到了文件末尾一样。 */ read(parent_fd[0], bytes, sizeof(bytes)); int prime = *(int *)bytes; printf(\"prime %d\\n\", prime); /*如果管道内没有其他prime，则不再创建子进程*/ if (read(parent_fd[0], bytes, sizeof(bytes)) == 0) { close(parent_fd[0]); exit(0); } int child_fd[2]; pipe(child_fd); int pid = fork(); if (pid \u003c 0) { printf(\"fork error\"); exit(1); } else if (pid == 0) { create_process(child_fd); exit(0); } else { close(child_fd[0]); do { int selectprime = *(int *)bytes; if (isprime(selectprime)) { write(child_fd[1], bytes, sizeof(bytes)); } } while (read(parent_fd[0], bytes, sizeof(bytes))); close(parent_fd[0]); close(child_fd[1]); //必须等到所有子进程都结束了才能exit wait((int *)0); exit(0); } } int main(int argc, char *argv[]) { int parent_fd[2]; pipe(parent_fd); int count = 35; int pid = fork(); if (pid \u003c 0) { printf(\"fork error\"); exit(1); } else if (pid == 0) { create_process(parent_fd); exit(0); } else { close(parent_fd[0]); for (int i = 2; i \u003c= count; i++) { char bytes[4]; bytes[3] = (i \u003e\u003e 24) \u0026 0xff; bytes[2] = (i \u003e\u003e 16) \u0026 0xff; bytes[1] = (i \u003e\u003e 8) \u0026 0xff; bytes[0] = i \u0026 0xff; write(parent_fd[1], bytes, sizeof(bytes)); } close(parent_fd[1]); wait((int *)0); exit(0); } } ","date":"2023-10-31","objectID":"/6.s081_lab1/:1:3","tags":null,"title":"6.s081_lab1","uri":"/6.s081_lab1/"},{"categories":["6.s081"],"content":"find 思路 find(*path, *filename)获取想要查找的目录路径，和目标文件名 创建缓冲区和一系列变量，将路径存储到缓冲区buf中，指针p指向最后一个/的位置 open打开路径path到句柄fd stat将fd的stat读取到st中 循环体while，判断条件为是否成功从句柄fd中读取dirent结构（目录层），使用read()函数将读取到的dirent存储到de中 忽略de.inum==0的项 拼接de.name到buf末尾，获得fd指向的目录下的一个文件完整路径 stat访问完整路径，存入st对st.type进行判断，进入switch结构 当st.type为T_FILE即文件时 对比当前文件名（de.name）和目标文件名（targent）是否一致 如果一致，输出buf中的完整目录 break 当st.type为T_DIR即目录时 迭代find(buf,targent) break #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"user/user.h\" #include \"kernel/fs.h\" int find(char *path, char *filename) { char buf[512], *p; int fd; struct dirent de; struct stat st; int findflag = 0; if ((fd = open(path, 0)) \u003c 0) { fprintf(2, \"find: cannot open %s\\n\", path); return -1; } if (fstat(fd, \u0026st) \u003c 0) { fprintf(2, \"find: cannot stat %s\\n\", path); close(fd); return -1; } // 将P指针指向path的最后buf+1的位置赋值\"/\" strcpy(buf, path); p = buf + strlen(buf); *p++ = '/'; while (read(fd, \u0026de, sizeof(de)) == sizeof(de)) { if (de.inum == 0) { continue; } if (strcmp(de.name, \".\") == 0 || strcmp(de.name, \"..\") == 0) { continue; } // 将指定路径目录下的dirent的name赋值给p，由此buf获取完整的路径 memmove(p, de.name, DIRSIZ); *(p + DIRSIZ) = 0; // 可以通过完整的文件名获取文件的stat，由此可以获取其type if (stat(buf, \u0026st) \u003c 0) { printf(\"find: cannot stat %s\\n\", buf); continue; } switch (st.type) { case T_FILE: if (strcmp(de.name, filename) == 0) { printf(\"%s\\n\", buf); findflag = 1; } break; case T_DIR: findflag = find(buf, filename); break; } } close(fd); return findflag; } int main(int argc, char *argv[]) { int findflag = 0; if (argc \u003c 2) { printf(\"find need para\\n\"); exit(1); } else if (argc == 2) { findflag = find(\".\", argv[1]); } else if (argc == 3) { findflag = find(argv[1], argv[2]); } else { printf(\"too many para\\n\"); exit(1); } if (findflag == 0) { printf(\"can not find the file\\n\"); exit(1); } exit(0); } ","date":"2023-10-31","objectID":"/6.s081_lab1/:1:4","tags":null,"title":"6.s081_lab1","uri":"/6.s081_lab1/"},{"categories":["6.s081"],"content":"xargs 思路 以find . | xargs grep hello为例 xargs之需要处理｜之后的参数，而｜之前的参数保存在line中，创建一个字符容量为4的字符数组，1:cmd即例子中的grep，2:argv[2]即例子中的hello，3:line即例子中find .的结果，4:0代表结束。通过while持续读一个字符到line字符数组中，直到读到换行符结束。之后在子进程中调用exec系统调用，参数即为cmd和容量为4的字符数组。 #include \"kernel/types.h\" #include \"kernel/stat.h\" #include \"kernel/param.h\" #include \"user/user.h\" #define MAXLINE 32 int main(int argc, char *argv[]) { if (argc \u003c 3) { printf(\"xargs need at least three params\"); exit(1); } char *cmd = argv[1]; char line[MAXLINE]; // 管道前产生的结果放入line中 memset(line, 0, sizeof(line)); // 读入的每一个char int i = 0; char ch; while (read(0, \u0026ch, sizeof(ch)) != 0) { if (ch == '\\n') { char *child_argv[4]; // 1: cmd 2: argv[2] 3:line 4:0表示结束 child_argv[0] = cmd; child_argv[1] = argv[2]; child_argv[2] = line; child_argv[3] = 0; if (fork() == 0) { exec(cmd, child_argv); } else { wait((int *)0); // parent process need to wait child process } memset(line,0,sizeof(line)); i=0; } else { line[i++] = ch; } } exit(0); } Tips:字符之间可以用==比较，可以用=赋值 字符串之间不可以用==比较，要用strcmp 字符串比较 int main() { char *str1=\"hello\"; char str2[]=\"hello\"; printf(\"%d\\n\",str1==\"hello\"); printf(\"%d\\n\",str2==\"hello\"); printf(\"%d\\n\",strcmp(str1,\"hello\")); printf(\"%d\\n\",strcmp(str2,\"hello\")); ​ return 0； } 输出结果为1 0 0 0 1.字符串变量比较不能直接用==，但是可以用变量地址和字符串用==比较，如果地址相同，字符串会相等 char *str1 = “hello”;和”hello”的地址是相同的，所以返回结果相等 str2 == “hello”地址不相等。char str2[] = “hello”; 这里str2并不是指针，类型里已经说明它是一个数组，所以这会是另一个内存地址，于是str2与”hello”的地址是不同的。 综上：字符串的比较不能用== 2.字符串比较用strcmp函数 strcmp(str1,”hello”)，strcmp(str2,”hello”)都是成立的 由于”hello”是字符串常量，编译器会进行优化： 所有的”hello”都是相同的，整个程序中只需要有一个”hello”字符串。 然后所有引用”hello”这个字符串的“指针变量”都赋值成相同的地址。 3.字符串赋值不能用= ","date":"2023-10-31","objectID":"/6.s081_lab1/:1:5","tags":null,"title":"6.s081_lab1","uri":"/6.s081_lab1/"},{"categories":["结构体读写"],"content":"关于结构体的读写 ","date":"2023-10-31","objectID":"/read_write_struct/:0:0","tags":null,"title":"Read_write_struct","uri":"/read_write_struct/"},{"categories":["结构体读写"],"content":"user space 可以通过fread和rwrite进行结构体的读写。 size_t fread(void *ptr, size_t size, size_t nmemb, FILE *stream) size_t fread(void *ptr, size_t size, size_t nmemb, FILE *stream) ptr − This is the pointer to a block of memory with a minimum size of size*nmemb bytes. size − This is the size in bytes of each element to be read. nmemb − This is the number of elements, each one with a size of size bytes. stream − This is the pointer to a FILE object that specifies an input stream. 代码示例 #include \u003cstdio.h\u003e /* 定义结构体, 存储一个字符串和年龄 */ struct student { char name[20]; int age; }; int main() { // 要写入文件的结构体 struct student s1 = {\"Tom\", 18}; // 打开要写入的文件 FILE *p = fopen(\"D:/File/student.dat\", \"w\"); // 打开失败直接退出 if(p == NULL) return 0; // 将结构体写出到文件中 fwrite(\u0026s1, sizeof (struct student), 1 ,p); // 关闭文件 fclose(p); // 读取文件中的结构体 // 存储读取到的结构体数据 struct student s2 = {0}; // 打开文件 FILE *p2 = fopen(\"D:/File/student.dat\", \"r\"); // 如果打开失败, 退出 if(p2 == NULL) return 0; // 从文件中读取结构体信息 fread(\u0026s2, sizeof (struct student), 1 ,p2); // 关闭文件 fclose(p2); // 打印数据 printf(\"student : name=%s, age=%d\\n\", s2.name, s2.age); return 0; } ","date":"2023-10-31","objectID":"/read_write_struct/:1:0","tags":null,"title":"Read_write_struct","uri":"/read_write_struct/"},{"categories":["结构体读写"],"content":"kernel kernel中并不像user space中那么简单。 打开文件 filp_open()在kernel中可以打开文件，其原形如下： strcut file* filp_open(const char* filename, int open_mode, int mode); 该函数返回strcut file*结构指针，供后继函数操作使用，该返回值用IS_ERR()来检验其有效性。 参数说明 filename： 表明要打开或创建文件的名称(包括路径部分)。在内核中打开的文件时需要注意打开的时机，很容易出现需要打开文件的驱动很早就加载并打开文件，但需要打开的文件所在设备还不有挂载到文件系统中，而导致打开失败。 open_mode： 文件的打开方式，其取值与标准库中的open相应参数类似，可以取O_CREAT,O_RDWR,O_RDONLY等。 mode： 创建文件时使用，设置创建文件的读写权限，其它情况可以匆略设为0 读写文件 kernel中文件的读写操作可以使用vfs_read()和vfs_write，在使用这两个函数前需要说明一下get_fs()和 set_fs()这两个函数。 vfs_read() vfs_write()两函数的原形如下： ssize_t vfs_read(struct file* filp, char __user* buffer, size_t len, loff_t* pos); ssize_t vfs_write(struct file* filp, const char __user* buffer, size_t len, loff_t* pos); 注意这两个函数的第二个参数buffer，前面都有__user修饰符，这就要求这两个buffer指针都应该指向用空的内存，如果对该参数传递kernel空间的指针，这两个函数都会返回失败-EFAULT。但在Kernel中，我们一般不容易生成用户空间的指针，或者不方便独立使用用户空间内存。要使这两个读写函数使用kernel空间的buffer指针也能正确工作，需要使用set_fs()函数或宏(set_fs()可能是宏定义)，如果为函数，其原形如下： void set_fs(mm_segment_t fs); 该函数的作用是改变kernel对内存地址检查的处理方式，其实该函数的参数fs只有两个取值：USER_DS，KERNEL_DS，分别代表用户空间和内核空间，默认情况下，kernel取值为USER_DS，即对用户空间地址检查并做变换。那么要在这种对内存地址做检查变换的函数中使用内核空间地址，就需要使用set_fs(KERNEL_DS)进行设置。get_fs()一般也可能是宏定义，它的作用是取得当前的设置，这两个函数的一般用法为： mm_segment_t old_fs; old_fs = get_fs(); set_fs(KERNEL_DS); ...... //与内存有关的操作 set_fs(old_fs); 还有一些其它的内核函数也有用__user修饰的参数，在kernel中需要用kernel空间的内存代替时，都可以使用类似办法。 使用vfs_read()和vfs_write()最后需要注意的一点是最后的参数loff_t * pos，pos所指向的值要初始化，表明从文件的什么地方开始读写。 在新版的kernel中，通过kernel_read和kernel_write封装上述烦琐的过程。 ssize_t kernel_read(struct file *file, void *buf, size_t count, loff_t *pos) { mm_segment_t old_fs; ssize_t result; old_fs = get_fs(); set_fs(KERNEL_DS); /* The cast to a user pointer is valid due to the set_fs() */ result = vfs_read(file, (void __user *)buf, count, pos); set_fs(old_fs); return result; } 在将结构体写入文件时，由于指针所指向的空间已经被内容填充，直接调用函数即可。 writefile = filp_open(\"~/Desktop/file_and_path.txt\", O_RDWR | O_APPEND | O_CREAT, 0); if (IS_ERR(writefile)) { printk(KERN_ERR \"open dir %s error.\\n\", path); return -1; } kernel_write(writefile, file, sizeof(struct file), \u0026wpos); 在读入文件时，定义读入的指针时需要通过kmalloc分配空间，否则会出现空指针的BUG。 struct file *readfile = NULL; struct file *testfile = kmalloc(sizeof(struct file), GFP_KERNEL); loff_t rpos = 0; readfile = filp_open(\"/home/zxj/Desktop/file_and_path.txt\", O_RDWR | O_CREAT, 0); if (IS_ERR(readfile)) { printk(KERN_ERR \"open dir %s error.\\n\", path); return -1; } kernel_read(readfile, testfile, sizeof(struct file), \u0026rpos); ","date":"2023-10-31","objectID":"/read_write_struct/:2:0","tags":null,"title":"Read_write_struct","uri":"/read_write_struct/"},{"categories":["6.828"],"content":"For GCC 7 or later, after switching to lab3 branch an error like kernel panic at kern/pmap.c:147: PADDR called with invalid kva 00000000 will occur. This is a bug caused by the linker script, modify kern/kernel.ld as follow will fix it. --- a/kern/kernel.ld +++ b/kern/kernel.ld @@ -50,6 +50,7 @@ SECTIONS .bss : { PROVIDE(edata = .); *(.bss) + *(COMMON) PROVIDE(end = .); BYTE(0) } 切换进程但是其内核页表不进行切换并非直接不切换，而是因为每个进程都对内核页表进行了赋值，并将UVPT这个虚拟地址映射到页表的物理地址上 ","date":"2023-10-31","objectID":"/6.828_lab3/:0:0","tags":null,"title":"6.828_lab3","uri":"/6.828_lab3/"},{"categories":["6.828"],"content":"中断描述符 一共有三种必须将控制由用户程序转移到内核的情形：系统调用、异常和中断。系统调用发生在用户程序请求一项操作系统的服务时。异常发生在用户程序想要执行某种非法的操作时，如除以零或者访问不存在的页表项。中断发生在某个外部设备需要操作系统的留意时，比如时钟芯片会定时产生一个中断提醒操作系统可以将硬件资源切换给下一个进程使用一会儿了。在大多数处理器上，这三种情形都是由同一种硬件机制来处理的。对于x86，系统调用和异常本质上也是生成一个中断，因此操作系统只需要提供一套针对中断的处理策略就可以了。 异常和中断都是“受保护的控制传输”，这会导致处理器从用户模式切换到内核模式 (CPL=0)，而不会给用户模式代码任何干扰内核或其他环境运行的机会。在 Intel 的术语中，中断是一种受保护的控制传输，它由通常在处理器外部的异步事件引起，例如外部设备 I/O 活动的通知。相反，异常是由当前运行的代码同步引起的受保护的控制传输，例如由于被零除或无效的内存访问。 为了确保这些受保护的控制传输真正受到保护，处理器的中断/异常机制被设计成当中断或异常发生时当前正在运行的代码 无法随意选择进入内核的位置或方式。相反，处理器确保只有在仔细控制的条件下才能进入内核。在 x86 上，有两种机制共同提供这种保护： 中断描述符表IDT。处理器确保中断和异常只能导致内核在由内核本身确定的几个特定的、定义明确的入口点进入 ，而不是在中断或异常发生时运行的代码。 x86 允许多达 256 个不同的中断或异常入口点进入内核，每个都有不同的中断向量。向量是介于 0 和 255 之间的数字。中断的向量由中断源决定：不同的设备、错误条件和对内核的应用程序请求会生成具有不同向量的中断。CPU 使用向量作为处理器中断描述符表(IDT) 的索引，内核在内核专用内存中设置该表，与 GDT 非常相似。处理器从该表中的适当==条目==（EIP和CS）加载： 加载到指令指针 ( EIP ) 寄存器 的值，指向指定用于处理该类型异常的内核代码。 要加载到代码段 ( CS ) 寄存器 中的值，它在位 0-1 中包含异常处理程序运行的特权级别。（在 JOS 中，所有异常都在内核模式下处理，权限级别为 0。） 任务状态段TSS。 处理器需要一个地方来保存中断或异常发生之前的旧处理器状态，例如处理器调用异常处理程序之前的EIP和CS的原始值，以便异常处理程序稍后可以恢复旧状态并恢复被中断的状态从它停止的地方开始的代码。但是这个旧处理器状态的保存区域必须反过来保护不受非特权用户模式代码的影响（保存的旧状态不可以被用户模式的代码所访问或者影响，因此需要切换到内核态来保存）；否则错误或恶意的用户代码可能会危及内核。 因此，当 x86 处理器发生中断或陷阱导致特权级别从用户模式更改为内核模式时，它也会切换到内核内存中的堆栈。称为任务状态段(TSS)的结构指定了该堆栈所在的段选择器和地址。具体地，JOS只会用到TSS中的esp0和ss0字段，JOS 不使用任何其他 TSS 字段。 处理器在读取IDT中的中断描述符之前，会先从TSS中读取将要切换的栈的ss0和esp0，并切换到新栈；接着往新栈压入原来进程的ss、esp、eflags、cs、eip、err code（如有的话）等信息；然后，处理器访问IDT，取出中断处理程序的cs、eip并跳转到中断处理程序。这些都是由处理器自动完成的工作，接下来便是操作系统（中断处理程序）的任务了。 尽管 TSS 很大并且可能有多种用途，但 JOS 仅使用它来定义处理器在从用户模式转移到内核模式时应该切换到的内核堆栈。 所有中断处理程序的入口都位于trapentry.S中，我们要通过其中定义的两个宏来设置需要用到的中断处理程序。JOS为多个入口设计了同一个处理函数trap，然后在trap中将处理任务分发给具体的中断处理函数。TRAPHANDLER和TRAPHANDLER_NOEC两个宏分别用于处理器会自动压入err code与不会自动压入的情形。我们可以直接对照xv6知道哪些中断号会压入错误代码而哪些不会。xv6中是通过一段perl脚本生成256个中断号入口的，JOS中暂时用不到这么多中断号，我们通过定义好的两个宏可以逐个生成需要用到的入口程序，其余的中断号会在后面的实验过程中创建。具体如下： TRAPHANDLER_NOEC(t_divide, T_DIVIDE) TRAPHANDLER_NOEC(t_debug, T_DEBUG) TRAPHANDLER_NOEC(t_nmi, T_NMI) TRAPHANDLER_NOEC(t_brkpt, T_BRKPT) TRAPHANDLER_NOEC(t_oflow, T_OFLOW) TRAPHANDLER_NOEC(t_bound, T_BOUND) TRAPHANDLER_NOEC(t_illop, T_ILLOP) TRAPHANDLER_NOEC(t_device, T_DEVICE) TRAPHANDLER(t_dblflt, T_DBLFLT) TRAPHANDLER(t_tss, T_TSS) TRAPHANDLER(t_segnp, T_SEGNP) TRAPHANDLER(t_stack, T_STACK) TRAPHANDLER(t_gpflt, T_GPFLT) TRAPHANDLER(t_pgflt, T_PGFLT) TRAPHANDLER_NOEC(t_fperr, T_FPERR) TRAPHANDLER(t_align, T_ALIGN) TRAPHANDLER_NOEC(t_mchk, T_MCHK) TRAPHANDLER_NOEC(t_simderr, T_SIMDERR) 这些入口程序都会跳转到紧随其后的_alltraps过程，该过程的任务其实就是继续保存用户进程其余的现场信息。而且JOS让它压入新栈的信息与之前处理器自动压入的信息（ss、esp、eflags、cs、eip、err code）最终成为一个Trapframe结构，并将结构体的地址传给trap函数。这份状态信息将会在之后用于恢复现场。也就是在这里，Trapframe成功地完成了原本属于TSS的任务。_allotraps的实现比较简单，和xv6不同，JOS的trap函数是不会返回的，所以在调用完trap之后没必要再执行任何操作。 _alltraps: # push values to make the stack look like a struct Trapframe pushl %ds pushl %es pushal # load GD_KD into %ds and %es movw $GD_KD, %ax movw %ax, %ds movw %ax, %es # pushl %esp to pass a pointer to the Trapframe as an argument to trap() pushl %esp call trap ","date":"2023-10-31","objectID":"/6.828_lab3/:1:0","tags":null,"title":"6.828_lab3","uri":"/6.828_lab3/"},{"categories":["6.828"],"content":"lab2 Virtual memory map /* * Virtual memory map: Permissions * kernel/user * * 4 Gig --------\u003e +------------------------------+ * | | RW/-- * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ * : . : * : . : * : . : * |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| RW/-- * | | RW/-- * | Remapped Physical Memory | RW/-- * | | RW/-- * KERNBASE, ----\u003e +------------------------------+ 0xf0000000 --+ * KSTACKTOP | CPU0's Kernel Stack | RW/-- KSTKSIZE | * | - - - - - - - - - - - - - - -| | * | Invalid Memory (*) | --/-- KSTKGAP | * +------------------------------+ | * | CPU1's Kernel Stack | RW/-- KSTKSIZE | * | - - - - - - - - - - - - - - -| PTSIZE * | Invalid Memory (*) | --/-- KSTKGAP | * +------------------------------+ | * : . : | * : . : | * MMIOLIM ------\u003e +------------------------------+ 0xefc00000 --+ * | Memory-mapped I/O | RW/-- PTSIZE * ULIM, MMIOBASE --\u003e +------------------------------+ 0xef800000 * | Cur. Page Table (User R-) | R-/R- PTSIZE * UVPT ----\u003e +------------------------------+ 0xef400000 * | RO PAGES | R-/R- PTSIZE * UPAGES ----\u003e +------------------------------+ 0xef000000 * | RO ENVS | R-/R- PTSIZE * UTOP,UENVS ------\u003e +------------------------------+ 0xeec00000 * UXSTACKTOP -/ | User Exception Stack | RW/RW PGSIZE * +------------------------------+ 0xeebff000 * | Empty Memory (*) | --/-- PGSIZE * USTACKTOP ---\u003e +------------------------------+ 0xeebfe000 * | Normal User Stack | RW/RW PGSIZE * +------------------------------+ 0xeebfd000 * | | * | | * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ * . . * . . * . . * |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| * | Program Data \u0026 Heap | * UTEXT --------\u003e +------------------------------+ 0x00800000 * PFTEMP -------\u003e | Empty Memory (*) | PTSIZE * | | * UTEMP --------\u003e +------------------------------+ 0x00400000 --+ * | Empty Memory (*) | | * | - - - - - - - - - - - - - - -| | * | User STAB Data (optional) | PTSIZE * USTABDATA ----\u003e +------------------------------+ 0x00200000 | * | Empty Memory (*) | | * 0 ------------\u003e +------------------------------+ --+ * * (*) Note: The kernel ensures that \"Invalid Memory\" is *never* mapped. * \"Empty Memory\" is normally unmapped, but user programs may map pages * there if desired. JOS user programs map pages temporarily at UTEMP. */ Physical Memory We will now dive into a bit more detail about how a PC starts up. A PC's physical address space is hard-wired to have the following general layout: +------------------+ \u003c- 0xFFFFFFFF (4GB) | 32-bit | | memory mapped | | devices | | | /\\/\\/\\/\\/\\/\\/\\/\\/\\/\\ /\\/\\/\\/\\/\\/\\/\\/\\/\\/\\ | | | Unused | | | +------------------+ \u003c- depends on amount of RAM | | | | | Extended Memory | | | | | +------------------+ \u003c- 0x00100000 (1MB) | BIOS ROM | 64KB +------------------+ \u003c- 0x000F0000 (960KB) | 16-bit devices, | | expansion ROMs | +------------------+ \u003c- 0x000C0000 (768KB) | VGA Display | 128KB +------------------+ \u003c- 0x000A0000 (640KB) | | | Low Memory | | | +------------------+ \u003c- 0x00000000 ","date":"2023-10-31","objectID":"/6.828_lab2/:0:0","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"Part1 Physical Page Management ","date":"2023-10-31","objectID":"/6.828_lab2/:1:0","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"exercise1 补全在kern/pmap.c下的几个函数。 boot_alloc() mem_init() (only up to the call to check_page_free_list(1)) page_init() page_alloc() page_free() 首先我们要知道一点，在代码中，所有的变量的地址都是虚拟地址，JOS中虚拟地址到物理地址的转换很简单: 虚拟地址 = 物理地址 + KERNBASE(0xF0000000) 在JOS中，一开始的物理内存布局如下图所示 由lab1我们可以得知，内核代码从0x100000处开始，也就是物理地址1MB的地方，第一条指令从0x10000c处开始，随后内核主要做如下几件事情 开启分页(entry.S 62行) 设置栈指针(entry.S 77行) 调用i386_init(entry.S 80行) I386_init()会调用mem_init()。 mem_init会先去调用i386_detect_memory();去找到这个机器有多少内存，也就是确定npages和npages_basemem。其中npages表示一共有多少物理内存页，而npages_basemem则表示物理内存中的Low Memory一共有多少物理内存页。我们知道，在4G的物理内存中，0x000A000(640KB)~0x00100000(1MB)存在一个IO洞，这个IO洞用于VGA和BIOS等，这在分配空闲页面时是绝对不能使用的 // Find out how much memory the machine has (npages \u0026 npages_basemem). i386_detect_memory(); 随后mem_init()调用boot_alloc分配一个PASIZE大小的页用于存放页表，将页表的起始地址存放在kern_pgdir中，本实验中地址是0xf0117000~0xf0118000，并通过memset初始化该页面 kern_pgdir = (pde_t *)boot_alloc(PGSIZE); memset(kern_pgdir, 0, PGSIZE); 随后给页表添加映射，从UVPT虚拟地址映射到kern_pgdir的物理地址，在UVPT处形成一个虚拟页表 //递归地将PD作为一个页表插入自身，以形成 // 在虚拟地址UVPT处形成一个虚拟页表。 kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P; 之后为PageInfo分配空间，用于管理物理页面。从0xf0118000开始即是通过boot_alloc()函数来分配npages个PageInfo结构体来管理物理页面。 pages = (struct PageInfo *)boot_alloc(npages * sizeof(struct PageInfo)); memset(pages, 0, sizeof(struct PageInfo) * npages); 之后对数据结构初始化，详细见page_init() page_init(); boot_alloc() boot_alloc()中维护了一个静态指针nextfree，初始值是在/kern/kernel.ld中定义的符号，指向bss段末尾。 if (!nextfree) { extern char end[]; nextfree = ROUNDUP((char *)end, PGSIZE); cprintf(\"kern code end ROUNDUP 4k: %x\\n\", nextfree); } 可以看到0xf0117000是end（4K对齐）的末尾，随后的4K页面用于存储页表即0xf0117000~0xf0118000。 boot_alloc()函数主要做的工作就是当申请n字节大小空间的内存时，将当前nextfree保存在result当做函数返回值，然后将其向后移动ROUNDUP(n, PGSIZE)，此时[result, nextfree)的空间就分配出来了。因为是按页管理内存，所以分配的内存大小需要页对齐。特别地，如果n是0，那么直接返回nextfree。 if (0 == n) return nextfree; n = ROUNDUP(n, PGSIZE); if ((uint32_t)nextfree + n \u003e KERNBASE + npages * PGSIZE) panic(\"boot_alloc: out of memory\\n\"); result = nextfree; nextfree += n; return result; mem_init() 只需要完成到调用check_page_free_list(1)之前 在内核代码中每个物理页都由一个PageInfo的数据结构来标识，一共有npages个物理页。所有的PageInfo组成一个pages数组。所以在mem_init需要先对pages结构进行物理内存分配。 之后所出现的物理页其实是指PageInfo，代码中对物理页的操作其实都是操作PageInfo这个结构 分配npages个PageInfo结构体用于管理物理页面，pages的起始地址即上述中的0xf0118000 pages = (struct PageInfo *)boot_alloc(npages * sizeof(struct PageInfo)); memset(pages, 0, sizeof(struct PageInfo) * npages); page_init() 分配完内存后自然就要对数据结构进行初始化，即将物理内存中的每一页都与pageInfo关联，其中分为可用页和不可用页。物理内存页到pages数组下标的映射关系为: ==物理地址/PGSIZE(4k)==。根据提示可知，一共有两大块空闲物理内存块。[1, npages_basemem)。第二块就是从内核代码往后，这个地址可以用boot_alloc(0)取到，即分配了pages内存之后的地址。但这个地址在代码中是虚拟地址，所以需要将其转换成物理地址，可以用PADDR()宏来转换。所以第二块范围就是[PADDR(boot_alloc(0)/PGSIZE)，npages)。找出这些空闲页后需要用page_free_list链表串起来。方便后续内存分配。 size_t i; for (i = 1; i \u003c npages_basemem; i++) { pages[i].pp_ref = 0; pages[i].pp_link = page_free_list; page_free_list = \u0026pages[i]; } // boot_alloc(0) returns the first free virtual address after the kernel for (i = PADDR(boot_alloc(0)) / PGSIZE; i \u003c npages; i++) { pages[i].pp_ref = 0; pages[i].pp_link = page_free_list; page_free_list = \u0026pages[i]; } 由此图也可以看出，pages即PageInfo数组的首地址是0xf0118000，其管理的虚拟地址是0xf0000000，其对应的物理地址是0x00000000，由此也说明pages数组的下标和物理地址的映射关系。 page_alloc() 空闲物理页的分配。在空闲物理页链表中取出一个物理页即可。返回的是PageInfo*，这个怎么与物理内存中的物理页对应呢？ - 注意: 两个指针相减，结果并不是两个指针数值上的差，而是把这个差除以指针指向类型的大小的结果，即个数 可以用page2pa(PageInfo*)的宏，因为pages数组是连续的物理内存，所以直接将PageInfo* pp 的地址减去pages就可以知道在数组中的下标是多少。在乘以4K就可以得到物理地址了: (pp-pages) « PGSHIFT。PGSHIFT = 1«12 = 4096 = 4k。 struct PageInfo * page_alloc(int alloc_flags) { // Fill this function in if (page_free_list == NULL) { return NULL; } struct PageInfo *page = page_free_list; page_free_list = page-\u003epp_link; page-\u003epp_link = NULL; if (alloc_flags \u0026 ALLOC_ZERO) { /* page还是虚拟地址，用page减去pages数组的起始地址，得到的是pages数组的下标 然后在乘上PGSIZE，得到的是其管理的物理地址. 最后通过KADDR也就是加上KERNBASE得到对应的管理物理地址的内核虚拟地址 */ memset(page2kva(page), 0, PGSIZE); cprintf(\"page: %x page2kva(page): %x page2pa(page): %x pages: %x page2kva(pages): %x PGSIZE: %d\\n\", page, page2kva(page), page2pa(page), pages,page2kva(pages), PGSIZE); ","date":"2023-10-31","objectID":"/6.828_lab2/:1:1","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"Part2 Virtual Address ","date":"2023-10-31","objectID":"/6.828_lab2/:2:0","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"exercise2 阅读Intel 80386 Reference Manual的第5第6章。 在x86结构下，使用的是分段分页机制，虚拟地址转换为物理地址需要中间还需要经历线性地址(分段的过程)。参考lab1的实模式和保护模式 下图是具体的地址结构转换过程。 在JOS中，虚拟地址=线性地址，为什么呢？因为在boot/boot.S中把所有的段地址都设置成了0 到0xffffffff，即段基址都等于0，相当于0+offset，所以就没有分段的效果了。这样我们就可以专注于实现分页机制了。 在lab1中已经安装了一个简易的页目录和页表，将虚拟地址[0, 4MB)映射到物理地址[0, 4MB)，[0xF0000000, 0xF0000000+4MB)映射到[0, 4MB）。具体实现在kern/entry.S中，临时的页目录线性地址为entry_pgdir，定义在kern/entrypgdir.c中。 ","date":"2023-10-31","objectID":"/6.828_lab2/:2:1","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"exercise4 补全kern/pmap.c下的这些函数，实现页表管理。 pgdir_walk() boot_map_region() page_lookup() page_remove() page_insert() 在补全这些函数之前，需要先明白一个图的含义。JOS采用的是二级页表机制，主要由五个元素组成，页目录表-页目录项(PDE, page diretory entry)，页表-页表项(PTE, page table entry)，物理页。PDE和PTE存储的都是地址。 其中一个页目录项对应一个页表，一个页表项对应一个物理页。页目录表的地址存储在CR3寄存器中。 pgdir_walk() 根据(页目录表，虚拟地址，创建标志)找到该虚拟地址所对应的物理页的虚拟地址。 通过PDX获得va的页目录项在页目录表中的偏移取得PDE，如果该PDE所指向的PT是空的话且create == 1，那就创建一个页目录表，即申请一页的物理内存。并设置为用户可读可写。然后再根据PTX获得va在页表项在页表中的偏移获取PTE，返回此PTE的地址。 **PTE_ADDR(*pde)**的作用是去掉后面的权限位。 需要注意的是通过一级页表项获取二级页表的起始地址时，页表项中存储的是物理地址，但是CPU需要接收的是虚拟地址，因此需要通过KADDR函数进行转换。而在6.s081中之所以不需要通过KADDR的转换是因为，在xv6中虚拟地址和物理地址是直接映射的关系，即虚拟地址也是物理地址，因此直接使用即可，但是要知道在取索引时CPU使用的是虚拟地址而不是物理地址。 pte_t * pgdir_walk(pde_t *pgdir, const void *va, int create) { // Fill this function in // PDX和PTX都是取低10位，都与3FF相与，一个是左移22位一个是左移12位 uint32_t pde_index = PDX(va); uint32_t pte_index = PTX(va); pde_t *pde = \u0026pgdir[pde_index]; // 根据索引获得pde页目录项的地址 if (*pde \u0026 PTE_P) // 如果pde存在 { //*pde是物理地址，但是内核需要读写的是虚拟地址，所以需要用KADDR转换一下 pte_t *pte = (pte_t *)KADDR(PTE_ADDR(*pde)); // 获得pte的地址 return \u0026pte[pte_index]; // 返回对应索引的pte的地址 } else if (create) // 如果pde不存在，且create为真 { struct PageInfo *page = page_alloc(ALLOC_ZERO); // 分配一个物理页 if (page == NULL) { return NULL; } page-\u003epp_ref++; // 引用计数加1 *pde = page2pa(page) | PTE_P | PTE_W | PTE_U; // page2pa用于获取page管理的物理页面的地址，设置pde的值 pte_t *pte = (pte_t *)KADDR(PTE_ADDR(*pde)); // 获得pte页表首地址 return \u0026pte[pte_index]; // 返回pte索引对应页表项的地址，但是此时*pte即页表项中还未填充内容，需要后续的函数填充 } return NULL; } boot_map_region() 之前的pgdir_walk是取到页表项，但页表项还未真正的映射到物理页上，此函数将从va开始的大小为size的地址按页从物理地址pa开始映射。相当于对页表项赋值。 static void boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm) { // 计算有多少页，并向上取整 size_t pages_num = size / PGSIZE; if (size % PGSIZE != 0) pages_num++; // 分别对每页调整 for (int i = 0; i \u003c pages_num; i++) { pte_t *pte = pgdir_walk(pgdir, (void *)va, 1); // 获取va对应的页表地址 if (pte == NULL) { panic(\"boot_map_region(): out of memory\\n\"); } // 修改va对应的页表PTE的值 *pte = pa | perm | PTE_P; pa += PGSIZE;c va += PGSIZE; } } page_lookup() 返回页表项所对应的物理页的虚拟地址，并把页表项存储在pte_store中。**pte_store二级指针相当于传入指针的引用。 struct PageInfo * page_lookup(pde_t *pgdir, void *va, pte_t **pte_store) { // Fill this function in pte_t* pte = pgdir_walk(pgdir, va, 0); if (pte == NULL || !(*pte \u0026 PTE_P)) { return NULL; } if (pte_store) { *pte_store = pte; } return (struct PageInfo*)pa2page(PTE_ADDR(*pte)); } page_remove() 清空页表项对应的物理页,并把物理页引用减减。 void page_remove(pde_t *pgdir, void *va) { // Fill this function in pte_t* pte_store; struct PageInfo* pp = page_lookup(pgdir, va, \u0026pte_store); if(pp == NULL || !(*pte_store \u0026 PTE_P)) return; page_decref(pp); *pte_store = 0; tlb_invalidate(pgdir, va); } page_insert() 给页表项赋值一个物理页。 int page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm) { // Fill this function in pte_t* pte = pgdir_walk(pgdir, va, 1); if (pte == NULL) { return -E_NO_MEM; } pp-\u003epp_ref++; if (*pte \u0026 PTE_P) { page_remove(pgdir, va); } *pte = page2pa(pp) | perm | PTE_P; // cprintf(\"page_insert: %x\\n\", *pte); return 0; } ","date":"2023-10-31","objectID":"/6.828_lab2/:2:2","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"Part 3: Kernel Address Space ","date":"2023-10-31","objectID":"/6.828_lab2/:3:0","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"exercise5 JOS的内核空间为[UTOP, KERNBASE)，一共为256MB。 填充完整mem_init()，将虚拟内核地址空间映射到物理地址上。 1. [UPAGES, UPAGES+PTSIZE)这段空间是pages数组的空间，将其映射到PADDR(pages)上。 2. [KSTACKTOP-KSTKSIZE, KSTACKTOP)是内核栈的空间，将其映射到PADDR(bootstack) 3. [KERNBASE, 2^32-1)，其中32位系统无法计算 2^32，但 2^32-1 == -KERNBASE。这段地址从物理地址0开始映射。 boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U); PTE_U); boot_map_region(kern_pgdir, KSTACKTOP-KSTKSIZE, KSTKSIZE, PADDR(bootstack), PTE_W); boot_map_region(kern_pgdir, KERNBASE, -KERNBASE, 0, PTE_W); 也就是我们需要将图中三块高亮区域进行映射，其中KSTACKTOP-KSTKSIZE是8*PGSIZE。 之后我们使用JOS的qemu提供查看虚拟地址被映射的区域可以看到： 0xefff8000~0x100000000即是KSTACKTOP-KSTKSIZE~4G的虚拟地址被映射。 0xef000000~0xef400000即是UPAGES开始的虚拟地址被映射， 0xef400000~0xef800000是虚拟页表被映射，所以中间两块0xef7bc000~0xef80000即是虚拟页表被映射，暂时实验还未提及，后续应该会提到，应该和mem_init中这行代码有关。 kern_pgdir[PDX(UVPT)] = PADDR(kern_pgdir) | PTE_U | PTE_P; 到目前为止页目录表中已经包含多少有效页目录项？他们都映射到哪里？ 3BC号页目录项，指向的是pages数组 3BD号页目录项，指向的是kern_pgdir本身，即虚拟地址UVPT~ULIM指向了kern_pgdir的物理地址，即页目录本身（一级页表指向本身，即二级页表仍然是用的一级页表） 由此图可以知道kern_dir的虚拟地址是0xf011b000,物理地址是0x11b000。再看之前的页表，pde[3bd]指向的是kern_pgdir的物理地址，虽然我们不知道pde[3bd]里面的内容，但是我们可以从二级页表中看出pte[3bd]指向的物理地址是0x0011b000，也就是kern_pgdir的物理地址。 也就是说在PED[3bd]下的pte[]均代表的是二级页表的物理地址，我们可以看出二级页表最终映射到0x3ff000并未超过4M。 3BF号页目录项，指向的是bootstack，正好是8*PGSIZE 剩下的对应物理地址[0M-256M] ","date":"2023-10-31","objectID":"/6.828_lab2/:3:1","tags":null,"title":"6.828_lab2","uri":"/6.828_lab2/"},{"categories":["6.828"],"content":"lab1 ","date":"2023-10-31","objectID":"/6.828_lab1/:0:0","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"part1 PC Bootstrap 实验分为三个部分： 熟悉汇编语言、QEMU x86模拟器、PC上电启动过程 检查我们的6.828内核的boot loader程序，它位于lab的boot目录下。 深入研究6.828内核本身的初始模板，位于kernel目录下。 使用qemu编译 $ make qemu-nox-gdb *** *** Now run 'make gdb'. *** qemu-system-i386 -nographic -drive file=obj/kern/kernel.img,index=0,media=disk,format=raw -serial mon:stdio -gdb tcp::26002 -D qemu.log -S 6828 decimal is XXX octal! entering test_backtrace 5 entering test_backtrace 4 entering test_backtrace 3 entering test_backtrace 2 entering test_backtrace 1 entering test_backtrace 0 leaving test_backtrace 0 leaving test_backtrace 1 leaving test_backtrace 2 leaving test_backtrace 3 leaving test_backtrace 4 leaving test_backtrace 5 Welcome to the JOS kernel monitor! Type 'help' for a list of commands. K\u003e 使用help和kerninfo两个命令 K\u003e help help - Display this list of commands kerninfo - Display information about the kernel K\u003e kerninfo Special kernel symbols: _start 0010000c (phys) entry f010000c (virt) 0010000c (phys) etext f0101acd (virt) 00101acd (phys) edata f0113060 (virt) 00113060 (phys) end f01136a0 (virt) 001136a0 (phys) Kernel executable memory footprint: 78KB K\u003e ","date":"2023-10-31","objectID":"/6.828_lab1/:1:0","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"The PC’s Physical Address Space 接下来会介绍PC的启动。一个PC的物理地址空间可以分成以下组成。 We will now dive into a bit more detail about how a PC starts up. A PC's physical address space is hard-wired to have the following general layout: +------------------+ \u003c- 0xFFFFFFFF (4GB) | 32-bit | | memory mapped | | devices | | | /\\/\\/\\/\\/\\/\\/\\/\\/\\/\\ /\\/\\/\\/\\/\\/\\/\\/\\/\\/\\ | | | Unused | | | +------------------+ \u003c- depends on amount of RAM | | | | | Extended Memory | | | | | +------------------+ \u003c- 0x00100000 (1MB) | BIOS ROM | 64KB +------------------+ \u003c- 0x000F0000 (960KB) | 16-bit devices, | | expansion ROMs | +------------------+ \u003c- 0x000C0000 (768KB) | VGA Display | 128KB +------------------+ \u003c- 0x000A0000 (640KB) | | | Low Memory | | | +------------------+ \u003c- 0x00000000 为了兼容性的考虑。PC在一开始是16位的。但是地址线却有20位。也就是能够寻址1MB的地址空间。其中640KB为低端内存。 这段内容非常重要，所以需要好好注意。Lab2内存分配的时候会用到这个。 除去低端的640KB。那么1MB还留下 1024KB - 640KB = 384KB。这384KB的范围就是 0x000A0000 ~ 0x000FFFFF。 其中BIOS占掉了顶端的64KB的内存。尽管后来内存从1MB前进到了16MB，后来又进展到了4GB。但是PC的内存布局还是没有改变。主要是为了兼容性考虑。因此，32位的CPU在这里还是会有个洞。0x000A0000 〜 0x00100000。 原本低端内存可以连续的1MB，变成了两段 0~640KB， 然后1MB〜更高的内存。 即 “conventional memory” (the first 640KB) “extended memory” 1MB以上 最新的x86架构可以支持4GB以上的物理内存了。所以RAM也可以扩展到0xFFFFFFFF以上的地址。在这种情况下BIOS需要设置第二个洞。也就是在32位地址的顶端。但是JOS目前来说，只是支持256MB的物理内存。所以这里设计时只考虑到了具有32位地址的地址空间的情况。 ","date":"2023-10-31","objectID":"/6.828_lab1/:1:1","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"The ROM BIOS 接下来的操作里面，你会用到QEMU的debug功能来深入了解IA-32计算机的启动流程。 需要做以下事情： 打开两个termainal 一个窗口运行make qemu-nox-gdb 另外一个窗口运行make gdb # make gdb GNU gdb (GDB) 6.8-debian Copyright (C) 2008 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u003chttp://gnu.org/licenses/gpl.html\u003e This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"i486-linux-gnu\". + target remote localhost:26000 The target architecture is assumed to be i8086 [f000:fff0] 0xffff0: ljmp $0xf000,$0xe05b 0x0000fff0 in ?? () + symbol-file obj/kern/kernel (gdb) 这里能够通过gdb一下子就连接上来，这是因为提供了一个.gdbinit文件，能够自动地attach到想要调试的程序上来。当然前提是已经把这个debug的程序运行起来的情况。 PC中BIOS大小为64k, 物理地址范围0x000f0000-0x000fffff PC 开机首先0xfffff0处执行 jmp [0xf000,0xe05b] 指令。在gdb中使用si(Step Instruction)进行跟踪。第一条要执行的指令就是ljmp。 [f000:fff0] 0xffff0: ljmp $0xf000,$0xe05b (gdb) si [f000:e05b] 0xfe05b: cmpw $0xffc8,%cs:(%esi) # 比较大小，改变PSW 0x0000e05b in ?? () (gdb) si [f000:e062] 0xfe062: jne 0xd241d416 # 不相等则跳转 0x0000e062 in ?? () (gdb) si [f000:e066] 0xfe066: xor %edx,%edx # 清零edx 0x0000e066 in ?? () (gdb) si [f000:e068] 0xfe068: mov %edx,%ss 0x0000e068 in ?? () (gdb) si [f000:e06a] 0xfe06a: mov $0x7000,%sp 0x0000e06a in ?? () 从这个要执行的指令可以看出来。 IBM PC开始执行的物理位置是0x000ffff0。这个是位于1MB里面的很高的地址。也就是ROM BIOS最顶上64KB的顶部。 [f000:fff0]可以看出来，此时CS = 0xf000 and IP = 0xfff0. 如果执行完这条指令之后 CS = 0xf000 and IP = 0xe05b. 为什么QEMU一开始执行的时候是这样的？这是因为8088的芯片就是这样的。 在IBM最原始的PC就是这么使用的。总之一句话，当PC上电之后。CS:IP两个寄存器就强制被设置为这个值。CS = 0xf000 and IP = 0xfff0。 注意16位的寻址模式 : physical address = 16 * segment + offset. 16 * 0xf000 + 0xfff0 # in hex multiplication by 16 is = 0xf0000 + 0xfff0 # easy--just append a 0. = 0xffff0 1MB尾巴上的地址就是0xffff0 + 16bytes。除了放个ljmp之外，你也不要指望16bytes能做啥了。 当 BIOS 运行时，它会建立一个中断描述符表并初始化各种设备，例如 VGA 显示器。这就是您在 QEMU 窗口中看到的 “ Starting SeaBIOS ”消息的来源。 初始化 PCI 总线和 BIOS 知道的所有重要设备后，它会搜索可引导设备，如软盘、硬盘或 CD-ROM。最终，当它找到可引导磁盘时，BIOS从磁盘 读取引导加载程序并将控制权转移给它。 ","date":"2023-10-31","objectID":"/6.828_lab1/:1:2","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"Part2 The BootLoader 软盘和磁盘一般都是被切分为512 byte区域，也就是扇区。一个扇区是一个块设备的最小传输单位。每次读写都是必须是扇区的整数倍。 如果这个软盘或者磁盘是可启动的。那么第一个扇区被叫做可启动扇区。这里也存放的就是可启动的代码。当BIOS找到这个扇区的时候，就把这512 byte读到0x7c00至0x7cff。然后用一个跳转指令 ljmp 0x07c0:0000 对于CD-ROM的支持，需要看 “El Torito” Bootable CD-ROM Format Specification. 对于6.828来说，由于完全是使用硬盘来启动的。所以在硬盘的开始必须是boot loader，并且这个boot loader必须是512 bytes大小。 这个boot loader主要是由两个文件构成 boot/boot.S boot/main.c 这里需要好好地读一下这个文件。然后知道这两个文件做了些啥。 boot loader把模式切到了32位保护模式。因为只有在这种模式下软件才可以访问1MB+以上的内存空间。保护模式在1.2.7 and 1.2.8 of PC Assembly Language 进行了介绍。 Intel architecture manuals也对这个有详细介绍。 在16位模式下只需要考虑段地址。 其次，需要注意的是boot loader读了kernel。从硬盘到内存。在操作的时候走的是PC的寄存器操作。如果想要了解更多，可以读一下\"IDE hard drive controller\" in the 6.828 reference page.的这一部分。 当你理解了boot loader的源码之后。接下来可以看一下obj/boot/boot.asm。b *0x7c00就可以把断点设置在0x7c00`。 b *0x7c00 # 设置断点在0x7c00 si 表示单步执行 si 2 表示接着执行两条指令 c 表示不再单步执行。直接开始运行了 查看内存中的指令，有时候可能需要查看内存操作的结果。这个时候需要用 x/i 调试命令 x/Ni 基中N是指令的数目; 会把指定内存里面的指令翻译成汇编。 ","date":"2023-10-31","objectID":"/6.828_lab1/:2:0","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"实模式和保护模式 实模式和保护模式都是CPU的工作模式，而CPU的工作模式是指CPU的寻址方式、寄存器大小等用来反应CPU在该环境下如何工作的概念。 1.实模式工作原理 实模式出现于早期8088CPU时期。当时由于CPU的性能有限，一共只有20位地址线（所以地址空间只有1MB），以及8个16位的通用寄存器，以及4个16位的段寄存器。所以为了能够通过这些16位的寄存器去构成20位的主存地址，必须采取一种特殊的方式。当某个指令想要访问某个内存地址时，它通常需要用下面的这种格式来表示： (段基址：段偏移量) 其中第一个字段是段基址，它的值是由段寄存器提供的(一般来说，段寄存器有6种，分别为cs，ds，ss，es，fs，gs，这几种段寄存器都有自己的特殊意义，这里不做介绍)。 第二字段是段内偏移量，代表你要访问的这个内存地址距离这个段基址的偏移。它的值就是由通用寄存器来提供的，所以也是16位。那么两个16位的值如何组合成一个20位的地址呢？CPU采用的方式是把段寄存器所提供的段基址先向左移4位。这样就变成了一个20位的值，然后再与段偏移量相加。 即： 物理地址 = 段基址«4 + 段内偏移 所以假设段寄存器中的值是0xff00，段偏移量为0x0110。则这个地址对应的真实物理地址是 0xff00«4 + 0x0110 = 0xff110。 由上面的介绍可见，实模式的\"实\"更多地体现在其地址是真实的物理地址。 2.保护模式工作原理 随着CPU的发展，CPU的地址线的个数也从原来的20根变为现在的32根，所以可以访问的内存空间也从1MB变为现在4GB，寄存器的位数也变为32位。所以实模式下的内存地址计算方式就已经不再适合了。所以就引入了现在的保护模式，实现更大空间的，更灵活也更安全的内存访问。 在保护模式下，CPU的32条地址线全部有效，可寻址高达4G字节的物理地址空间; 但是我们的内存寻址方式还是得兼容老办法(这也是没办法的，有时候是为了方便，有时候是一种无奈)，即(段基址：段偏移量)的表示方式。当然此时CPU中的通用寄存器都要换成32位寄存器(除了段寄存器，原因后面再说)来保证寄存器能访问所有的4GB空间。 我们的偏移值和实模式下是一样的，就是变成了32位而已，而段值仍旧是存放在原来16位的段寄存器中，但是这些段寄存器存放的却不再是段基址了，毕竟之前说过实模式下寻址方式不安全，我们在保护模式下需要加一些限制，而这些限制可不是一个寄存器能够容纳的，于是我们把这些关于内存段的限制信息放在一个叫做全局描述符表(GDT)的结构里。全局描述符表中含有一个个表项，每一个表项称为段描述符。而段寄存器在保护模式下存放的便是相当于一个数组索引的东西，通过这个索引，可以找到对应的表项。段描述符存放了段基址、段界限、内存段类型属性(比如是数据段还是代码段,注意一个段描述符只能用来定义一个内存段)等许多属性,具体信息见下图： 其中，段界限表示段边界的扩张最值，即最大扩展多少或最小扩展多少，用20位来表示，它的单位可以是字节，也可以是4KB，这是由G位决定的(G为1时表示单位为4KB)。 实际段界限边界值=(描述符中的段界限+1)*（段界限的单位大小(即字节或4KB))-1，如果偏移地址超过了段界限，CPU会抛出异常。 全局描述符表位于内存中，需要用专门的寄存器指向它后， CPU 才知道它在哪里。这个专门的寄存器便是GDTR(一个48位的寄存器),专门用来存储 GDT 的内存地址及大小。 ","date":"2023-10-31","objectID":"/6.828_lab1/:2:1","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"Boot.S\u0026main.c 设置一个断点在地址0x7c00处，这是boot sector被加载的位置。然后让程序继续运行直到这个断点。跟踪/boot/boot.S文件的每一条指令，同时使用boot.S文件和系统为你反汇编出来的文件obj/boot/boot.asm。你也可以使用GDB的x/i指令来获取去任意一个机器指令的反汇编指令，把源文件boot.S文件和boot.asm文件以及在GDB反汇编出来的指令进行比较。 追踪到bootmain函数中，而且还要具体追踪到readsect()子函数里面。找出和readsect()c语言程序的每一条语句所对应的汇编指令，回到bootmain()，然后找出把内核文件从磁盘读取到内存的那个for循环所对应的汇编语句。找出当循环结束后会执行哪条语句，在那里设置断点，继续运行到断点，然后运行完所有的剩下的语句。 答： 下面我们将分别分析一下这道练习中所涉及到的两个重要文件，它们一起组成了boot loader。分别是**/boot/boot.S和/boot/main.c**文件。其中前者是一个汇编文件，后者是一个C语言文件。当BIOS运行完成之后，CPU的控制权就会转移到boot.S文件上。所以我们首先看一下boot.S文件。 /boot/boot.S： 1 .globl start 2 start: 3 .code16 # Assemble for 16-bit mode 4 cli # Disable interrupts 这几条指令就是boot.S最开始的几句，其中cli是boot.S，也是boot loader的第一条指令。这条指令用于把所有的中断都关闭。因为在BIOS运行期间有可能打开了中断。此时CPU工作在实模式下。 5 cld # String operations increment 这条指令用于指定之后发生的串处理操作的指针移动方向。在这里现在对它大致了解就够了。 6 # Set up the important data segment registers (DS, ES, SS). 7 xorw %ax,%ax # Segment number zero 8 movw %ax,%ds # -\u003e Data Segment 9 movw %ax,%es # -\u003e Extra Segment 10 movw %ax,%ss # -\u003e Stack Segment 这几条命令主要是在把三个段寄存器，ds，es，ss全部清零，因为经历了BIOS，操作系统不能保证这三个寄存器中存放的是什么数。所以这也是为后面进入保护模式做准备。 11 # Enable A20: 12 # For backwards compatibility with the earliest PCs, physical 13 # address line 20 is tied low, so that addresses higher than 14 # 1MB wrap around to zero by default. This code undoes this. 15 seta20.1: 16 inb $0x64,%al # Wait for not busy 17 testb $0x2,%al 18 jnz seta20.1 19 movb $0xd1,%al # 0xd1 -\u003e port 0x64 20 outb %al,$0x64 21 seta20.2: 22 inb $0x64,%al # Wait for not busy 23 testb $0x2,%al 24 jnz seta20.2 25 movb $0xdf,%al # 0xdf -\u003e port 0x60 26 outb %al,$0x60 这部分指令就是在准备把CPU的工作模式从实模式转换为保护模式。我们可以看到其中的指令包括inb，outb这样的IO端口命令。所以这些指令都是在对外部设备进行操作。根据下面的链接： http://bochs.sourceforge.net/techspec/PORTS.LST 我们可以查看到，0x64端口属于键盘控制器804x，名称是控制器读取状态寄存器。下面是它各个位的含义。 所以16~18号指令是在不断的检测bit1。bit1的值代表输入缓冲区是否满了，也就是说CPU传送给控制器的数据，控制器是否已经取走了，如果CPU想向控制器传送新的数据的话，必须先保证这一位为0。所以这三条指令会一直等待这一位变为0，才能继续向后运行。 当0x64端口准备好读入数据后，现在就可以写入数据了，所以19~20这两条指令是把0xd1这条数据写入到0x64端口中。当向0x64端口写入数据时，则代表向键盘控制器804x发送指令。这个指令将会被送给0x60端口。 通过图中可见，D1指令代表下一次写入0x60端口的数据将被写入给804x控制器的输出端口。可以理解为下一个写入0x60端口的数据是一个控制指令。 然后21~24号指令又开始再次等待，等待刚刚写入的指令D1，是否已经被读取了。 如果指令被读取了，25~26号指令会向控制器输入新的指令，0xdf。通过查询我们看到0xDF指令的含义如下 这个指令的含义可以从图中看到，使能A20线，==代表可以进入保护模式==了。 27 # Switch from real to protected mode, using a bootstrap GDT 28 # and segment translation that makes virtual addresses 29 # identical to their physical addresses, so that the 30 # effective memory map does not change during the switch. 31 lgdt gdtdesc 32 movl %cr0, %eax 33 orl $CR0_PE_ON, %eax 34 movl %eax, %cr0 首先31号指令 lgdt gdtdesc，是把gdtdesc这个标识符的值送入全局映射描述符表寄存器GDTR中。这个GDT表是处理器工作于保护模式下一个非常重要的表。具体可以参照我们的Appendix 1关于实模式和保护模式的介绍。至于这条指令的功能就是把关于GDT表的一些重要信息存放到CPU的GDTR寄存器中，其中包括GDT表的内存起始地址，以及GDT表的长度。这个寄存器由48位组成，其中低16位表示该表长度，高32位表该表在内存中的起始地址。所以gdtdesc是一个标识符，标识着一个内存地址。从这个内存地址开始之后的6个字节中存放着GDT表的长度和起始地址。我们可以在这个文件的末尾看到gdtdesc，如下： 1 # Bootstrap GDT 2 .p2align 2 # force 4 byte alignment 3 gdt: 4 SEG_NULL # null seg 5 SEG(STA_X|STA_R, 0x0, 0xffffffff) # code seg 6 SEG(STA_W, 0x0, 0xffffffff) # data seg 7 8 gdtdesc: 9 .word 0x17 # sizeof(gdt) - 1 10 .long gdt # address gdt 其中第3行的gdt是一个标识符，标识从这里开始就是GDT表了。可见这个GDT表中包括三个表项(4,5,6行)，分别代表三个段，null seg，code seg，data seg。由于xv6其实并没有使用分段机制，也就是说数据和代码都是写在一起的，所以数据段和代码段的起始地址都是0x0，大小都是0xffffffff=4GB。 在第4~6行是调用SEG()子程序来构造GDT表项的。这个子函数定义在mmu.h中，形式如下：　#define SEG(type,base,lim) \\ .word (((lim) \u003e\u003e 12) \u0026 0xffff), ((base) \u0026 0xffff); \\ .byte (((base) \u003e\u003e 16) \u0026 0xff), (0x90 | (type)), \\ (0xC0 | (((lim) \u003e\u003e 28) \u0026 0xf)), (((base) \u003e\u003e 24) \u0026 0xff) 可见函数需要3个参数，一是type即这个段的访问权限，二是base，这个段的起始地址，三是lim，即这个段的大小界限。gdt表中的每一个表项的结构如图所示： 每个表项一共8字节，其中limit_low就是limit的低16位。base_low就是base的低16位，依次类推，所以我们就可以理解SEG函数为什么要那么写（其实还是有很多不理解的。。）。 然后在gdtdesc处就要存放这个GDT表的信息了，其中0x17是这个表的大小-1 = 0x17 = 23，至于为什么不直接存表的大小24，根据查询是官方规定的。紧接着就是这个表的起始地址gdt。 27 # Switch from real to protected mode, using a bootstrap GDT 28 # and segment translation that makes virtual addresses 29 # identical to their physical addresses, so that the 30 # effective memory map does not change during the","date":"2023-10-31","objectID":"/6.828_lab1/:2:2","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise3 Q：处理器什么时候开始执行 32 位代码？究竟是什么导致从 16 位模式切换到 32 位模式？ A：movl %eax, %cr0 如果指令被读取了，25~26号指令会向控制器输入新的指令，0xdf。通过查询我们看到0xDF指令的含义如下 这个指令的含义可以从图中看到，使能A20线，==代表可以进入保护模式==了。 movb $0xdf,%al # 0xdf -\u003e port 0x60 outb %al,$0x60 31指令当加载完GDT表的信息到GDTR寄存器之后。紧跟着3个操作，32~34指令。 这几步操作明显是在修改CR0寄存器的内容。CR0寄存器还有CR1~CR3寄存器都是80x86的控制寄存器。其中$CR0_PE的值定义于\"mmu.h\"文件中，为0x00000001。可见上面的操作是把CR0寄存器的bit0置1，CR0寄存器的bit0是保护模式启动位，把这一位值1代表==保护模式启动==。 27 # Switch from real to protected mode, using a bootstrap GDT 28 # and segment translation that makes virtual addresses 29 # identical to their physical addresses, so that the 30 # effective memory map does not change during the switch. 31 lgdt gdtdesc 32 movl %cr0, %eax 33 orl $CR0_PE_ON, %eax 34 movl %eax, %cr0 Q：引导加载程序执行的最后一条指令 是什么，它刚刚加载的内核的第一条指令是什么？ A：((void (*)(void)) (ELFHDR-\u003ee_entry))(); 自此bootloader就把控制权转交给了OS。查看boot.asm文件找到bootloader， ((void (*)(void)) (ELFHDR-\u003ee_entry))(); 7d81: ff 15 18 00 01 00 call *0x10018 随后通过在gdb中输入b *0x7d81在0x7d81处打上断点，随后c再si，查看加载内核的第一条指令。 0x10000c: movw $0x1234,0x472 Q：内核的第一条指令在哪里？ A：0x10000c: movw $0x1234,0x472 第一条指令在0x10000c处。 也可以通过objdump -f obj/kern/kernel查看 obj/kern/kernel: file format elf32-i386 architecture: i386, flags 0x00000112: EXEC_P, HAS_SYMS, D_PAGED start address 0x0010000c Q：引导加载程序如何决定它必须读取多少个扇区才能从磁盘中获取整个内核？它在哪里找到这些信息？ A：详细的将涉及到ELF格式，从ELF头部中获取到这部分信息 4 ph = (struct Proghdr *) ((uint8_t *) ELFHDR + ELFHDR-\u003ee_phoff); 我们知道头部中一定包含Program Header Table。这个表格存放着程序中所有段的信息。通过这个表我们才能找到要执行的代码段，数据段等等。所以我们要先获得这个表。 这条指令就可以完成这一点，首先elf是表头起址，而phoff字段代表Program Header Table距离表头的偏移量。所以ph可以被指定为Program Header Table表头。 5 eph = ph + ELFHDR-\u003ee_phnum; 由于phnum中存放的是Program Header Table表中表项的个数，即段的个数。所以这步操作是吧eph指向该表末尾。 6 for (; ph \u003c eph; ph++) // p_pa is the load address of this segment (as well // as the physical address) 7 readseg(ph-\u003ep_pa, ph-\u003ep_memsz, ph-\u003ep_offset); 这个for循环就是在加载所有的段到内存中。ph-\u003epaddr根据参考文献中的说法指的是这个段在内存中的物理地址。ph-\u003eoff字段指的是这一段的开头相对于这个elf文件的开头的偏移量。ph-\u003efilesz字段指的是这个段在elf文件中的大小。ph-\u003ememsz则指的是这个段被实际装入内存后的大小。通常来说memsz一定大于等于filesz，因为段在文件中时许多未定义的变量并没有分配空间给它们。 所以这个循环就是在把操作系统内核的各个段从外存读入内存中。 ","date":"2023-10-31","objectID":"/6.828_lab1/:2:3","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"Loading the kernel ","date":"2023-10-31","objectID":"/6.828_lab1/:2:4","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise4 ELF程序文是由一个固定长度的ELF头开始的。紧接着的是一个动态可变的程序头list。每个程序头，指明了每个程序段需要被加载的位置，长度，相对于整个程序的偏移量。 程序头list struct Proghdr { uint32_t p_type; uint32_t p_offset; uint32_t p_va; uint32_t p_pa; uint32_t p_filesz; uint32_t p_memsz; uint32_t p_flags; uint32_t p_align; }; 注意：相对的是整个程序头的偏移量，而不是相对于磁盘头的偏移量 由于在编译完成之后，才会把kernel写入磁盘的某个扇区。在编译的时候是无法知道会被写入到哪个扇区的。所以编译的时候只能说把相对的位置写入到ELF里面。 这些各种程序头比较常见的有以下几个： .text: The program's executable instructions. .rodata: Read-only data, such as ASCII string constants produced by the C compiler. (We will not bother setting up the hardware to prohibit writing, however.) .data: The data section holds the program's initialized data, such as global variables declared with initializers like int x = 5;. 当链接器在计算内存部局的时候，也会保留足够的空间给各种未初始化的全局变量（初始化为0）。一般而言这个空间被称之为.bss段。 由于全部都是被设置为0.所以也就没有必要记录这些内容在ELF里面。所以ELF文件里面只需要记住.bss在内存里面的起始位置，以及大小。然后加载方需要确保这些.bss在内存里面正确的设置。以及初始化为0。 可以通过如下命令来查看obj/kern/kernel里面的names, sizes, 以及各种链接地址。 $ objdump -h obj/kern/kernel obj/kern/kernel: file format elf32-i386 Sections: Idx Name Size VMA LMA File off Algn 0 .text 00001917 f0100000 00100000 00001000 2**4 CONTENTS, ALLOC, LOAD, READONLY, CODE 1 .rodata 00000714 f0101920 00101920 00002920 2**5 CONTENTS, ALLOC, LOAD, READONLY, DATA 2 .stab 00003889 f0102034 00102034 00003034 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 3 .stabstr 000018af f01058bd 001058bd 000068bd 2**0 CONTENTS, ALLOC, LOAD, READONLY, DATA 4 .data 0000a300 f0108000 00108000 00009000 2**12 CONTENTS, ALLOC, LOAD, DATA 5 .bss 00000644 f0112300 00112300 00013300 2**5 ALLOC 6 .comment 0000002b 00000000 00000000 00013300 2**0 CONTENTS, READONLY 真正要了解这个文件，需要查看链接设定文件： kern/kernel.ld 通过这个文件可以知道程序被加载的虚拟地址(VMA)，物理地址(LMA)分别是如何指定的。也可以通过File off查看相对文件的偏移量。这个File off偏移量是如何指定的？这个非常有意思。刚好在boot/main.c里面就是一开始就读了了8个扇区，也就是0x1000 bytes。 ELF文件格式简图 kernel在编译和链接的时候，其虚拟地址与物理地址是不一样的。在加载的时候，也就相应地需要设置好页表。 但是考虑一下boot loader。在加载的时候，肯定是没有什么页表等着给你用的。BIOS可不会给你设置页表。所以 $ objdump -h obj/boot/boot.out Sections: Idx Name Size VMA LMA File off Algn # 注意这里的VMA与LMA是完全一致的。这是由于被加载进BIOS的时候，没有页表与段表可用。 # size = 380 bytes. # 这里面就包含了boot loader所需要的所有信息。 0 .text 0000017c 00007c00 00007c00 00000074 2**2 CONTENTS, ALLOC, LOAD, CODE # size = 176这个段实际上是没有什么用的？ 1 .eh_frame 000000b0 00007d7c 00007d7c 000001f0 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA # 接下来的三个段，里面都是用于DEBUG的。所以真实用的时候，不会被用到。 2 .stab 000007b0 00000000 00000000 000002a0 2**2 CONTENTS, READONLY, DEBUGGING 3 .stabstr 00000846 00000000 00000000 00000a50 2**0 CONTENTS, READONLY, DEBUGGING 4 .comment 0000002b 00000000 00000000 00001296 2**0 CONTENTS, READONLY $ objdump -x obj/boot/boot.out obj/boot/boot.out: file format elf32-i386 obj/boot/boot.out architecture: i386, flags 0x00000012: EXEC_P, HAS_SYMS start address 0x00007c00 Program Header: LOAD off 0x00000074 vaddr 0x00007c00 paddr 0x00007c00 align 2**2 filesz 0x0000022c memsz 0x0000022c flags rwx STACK off 0x00000000 vaddr 0x00000000 paddr 0x00000000 align 2**4 filesz 0x00000000 memsz 0x00000000 flags rwx Sections: Idx Name Size VMA LMA File off Algn 0 .text 0000017c 00007c00 00007c00 00000074 2**2 CONTENTS, ALLOC, LOAD, CODE 1 .eh_frame 000000b0 00007d7c 00007d7c 000001f0 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 2 .stab 000007b0 00000000 00000000 000002a0 2**2 CONTENTS, READONLY, DEBUGGING 3 .stabstr 00000846 00000000 00000000 00000a50 2**0 CONTENTS, READONLY, DEBUGGING 4 .comment 0000002b 00000000 00000000 00001296 2**0 CONTENTS, READONLY SYMBOL TABLE: 00007c00 l d .text 00000000 .text 00007d7c l d .eh_frame 00000000 .eh_frame 00000000 l d .stab 00000000 .stab 00000000 l d .stabstr 00000000 .stabstr 00000000 l d .comment 00000000 .comment 00000000 l df *ABS* 00000000 obj/boot/boot.o 00000008 l *ABS* 00000000 PROT_MODE_CSEG 00000010 l *ABS* 00000000 PROT_MODE_DSEG 00000001 l *ABS* 00000000 CR0_PE_ON 00007c0a l .text 00000000 seta20.1 00007c14 l .text 00000000 seta20.2 00007c64 l .text 00000000 gdtdesc 00007c32 l .text 00000000 protcseg 00007c4a l .text 00000000 spin 00007c4c l .text 00000000 gdt 00000000 l df *ABS* 000000","date":"2023-10-31","objectID":"/6.828_lab1/:2:5","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise5 将bootloader的起始地址修改为-Ttext 0x7c01，在make qemu，随后在该地址打上断点。发生了陷入 Breakpoint 1 at 0x7c01 (gdb) c Continuing. Program received signal SIGTRAP, Trace/breakpoint trap. [ 0:7c30] =\u003e 0x7c30: ljmp $0x8,$0x7c36 EAX=00000011 EBX=00000000 ECX=00000000 EDX=00000080 ESI=00000000 EDI=00000000 EBP=00000000 ESP=00006f20 EIP=00007c30 EFL=00000006 [-----P-] CPL=0 II=0 A20=1 SMM=0 HLT=0 ES =0000 00000000 0000ffff 00009300 DPL=0 DS16 [-WA] CS =0000 00000000 0000ffff 00009b00 DPL=0 CS16 [-RA] SS =0000 00000000 0000ffff 00009300 DPL=0 DS16 [-WA] DS =0000 00000000 0000ffff 00009300 DPL=0 DS16 [-WA] FS =0000 00000000 0000ffff 00009300 DPL=0 DS16 [-WA] GS =0000 00000000 0000ffff 00009300 DPL=0 DS16 [-WA] LDT=0000 00000000 0000ffff 00008200 DPL=0 LDT TR =0000 00000000 0000ffff 00008b00 DPL=0 TSS32-busy GDT= 0000007c 00005000 IDT= 00000000 000003ff CR0=00000011 CR2=00000000 CR3=00000000 CR4=00000000 DR0=00000000 DR1=00000000 DR2=00000000 DR3=00000000 DR6=ffff0ff0 DR7=00000400 EFER=0000000000000000 Triple fault. Halting for inspection via QEMU monitor ","date":"2023-10-31","objectID":"/6.828_lab1/:2:6","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercies6 Q:在 BIOS 进入引导加载程序时检查 0x00100000 处的 8 个内存字，然后在引导加载程序进入内核时再次检查。他们为什么不同？第二个断点是什么？ A: (gdb) b *0x7c00 Breakpoint 1 at 0x7c00 (gdb) b *0x7d81 Breakpoint 2 at 0x7d81 (gdb) c Continuing. [ 0:7c00] =\u003e 0x7c00: cli Breakpoint 1, 0x00007c00 in ?? () (gdb) x/8x 0x100000 0x100000: 0x00000000 0x00000000 0x00000000 0x00000000 0x100010: 0x00000000 0x00000000 0x00000000 0x00000000 (gdb) c Continuing. The target architecture is assumed to be i386 =\u003e 0x7d81: call *0x10018 Breakpoint 2, 0x00007d81 in ?? () (gdb) x/8x 0x100000 0x100000: 0x1badb002 0x00000000 0xe4524ffe 0x7205c766 0x100010: 0x34000004 0x2000b812 0x220f0011 0xc0200fd8 不运行QEMU也可以知道原因，在BIOS进入引导程序的时候，还没有讲内核程序加载到内存中，因此0X10000之后的8个字肯定为0。而在BootLoader进入内核时，此时已经在内核程序加载在0X10000中。 在entry.S中 .text # The Multiboot header .align 4 .long MULTIBOOT_HEADER_MAGIC .long MULTIBOOT_HEADER_FLAGS .long CHECKSUM # '_start' specifies the ELF entry point. Since we haven't set up # virtual memory when the bootloader enters this code, we need the # bootloader to jump to the *physical* address of the entry point. .globl _start _start = RELOC(entry) .globl entry entry: movw $0x1234,0x472 # warm boot 前三个字正好对应0X10000中的前三个字 0x100000: 0x1badb002 0x00000000 0xe4524ffe 0x7205c766 ","date":"2023-10-31","objectID":"/6.828_lab1/:2:7","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"Part3 The Kernel ","date":"2023-10-31","objectID":"/6.828_lab1/:3:0","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise7 Ｑ：用Qemu和GDB跳到JOS的内核里面。并且暂停在movl %eax, %cr0这条指令这里。验证内存两个地址：0x00100000 and at 0xf0100000。接下来用s指令一条一条地执行。然后再验证一下这个两个内存地址的内容。确保你理解整个发生的过程。 首先需要明白：程序地址与寻址地址 1. 程序代码地址 2. 支持的寻址地址 如果支持的寻址地址不支持汇编里面的地址（比如页表没有建立起来）。比如： mov %eax, *$0xf0100000 这个时候必须要知道0xf0100000真正的物理地址是什么。程序代码里直接成相应的物理地址。 #define RELOC(x) ((x) - KERNBASE) 示例1： kernel的入口地址 # '_start' specifies the ELF entry point. Since we haven't set up # virtual memory when the bootloader enters this code, we need the # bootloader to jump to the *physical* address of the entry point. .globl _start _start = RELOC(entry) .globl entry entry: kernel在编译的时候是并不知道会被加载到哪里的。通过链接的时候kern/kernel.ld链接脚本可以指令被加载到的物理地址。但是程序的入口地址仍然需要告知ELF。 ELF执行的格式是_start是入口。如果不加任何处理，那么_start就是一个虚拟地址。这个值会反应在ELF header-\u003ee_entry值上面。(看boot/main.c)里面的跳转到内核的代码： // call the entry point from the ELF header // note: does not return! ((void (*)(void)) (ELFHDR-\u003ee_entry))(); 这里面e_entry就指向_start值。由于从boot loader跳转到的内核的时候，还在物理地址与虚拟地址完全重合的情况。并且也没有开启分页。所以这个时候必须在kern/entry.S里面 .globl _start _start = RELOC(entry) 把_start地址改造成物理地址。这会儿， root@debug:~/6.828/lab# objdump -f obj/kern/kernel obj/kern/kernel: file format elf32-i386 architecture: i386, flags 0x00000112: EXEC_P, HAS_SYMS, D_PAGED start address 0x0010000c 这个时候start address就是一个物理地址。 f010000c \u003centry\u003e: f010000c: 66 c7 05 72 04 00 00 movw $0x1234,0x472 这也就是为什么进入kernel的第一条指令在地址0x10000c（0xf01000c是虚拟地址） 原问题的正解 首先对entry.S代码加以注释。 # Load the physical address of entry_pgdir into cr3. entry_pgdir # is defined in entrypgdir.c. movl $(RELOC(entry_pgdir)), %eax f0100015: b8 00 20 11 00 mov $0x112000,%eax movl %eax, %cr3 将0x112000也就是页目录的物理地址放入cr3寄存器中 f010001a: 0f 22 d8 mov %eax,%cr3 # Turn on paging. movl %cr0, %eax f010001d: 0f 20 c0 mov %cr0,%eax orl $(CR0_PE|CR0_PG|CR0_WP), %eax f0100020: 0d 01 00 01 80 or $0x80010001,%eax movl %eax, %cr0 f0100025: 0f 22 c0 mov %eax,%cr0 所以原问题中在开启分页前去看0xf0100000地址时。肯定为0。因为在当前地址空间里面，这部分虚拟地址是没有内容的。页表也还没有。只能是假装去访问物理地址。 分页后去查看地址时。0xf0100000与0x00100000内容就完全一样了。这是因为把[0, 4MB)映射到了[0xf0000000, 0xf0000000 + 4MB)的地方了。 开启分页后的跳转 # Now paging is enabled, but we're still running at a low EIP # (why is this okay?). Jump up above KERNBASE before entering # C code. mov $relocated, %eax jmp *%eax relocated: 当开启分页之后，立马会进行相应的跳转。这里主要是因为后面会开始执行C语言的函数了。必须设置好相应的CS:IP, esp, ebp, ss等寄存器。如果还是在物理地址空间运行。但是C语言是以为自己在虚拟地址空间运行的。 CPU跑在物理地址空间上，而不是虚拟地址空间上。（尽管CS:IP会被翻译到真正的地址。） C语言认为是自己是跑在虚拟地址空间。 通过jmp，可以使得两者正常化。CPU在取指，寻址的时候，就会在有页映射的地址空间里面了。环境设置好，就可以开始跳转到C语言里面了。 可以发现，两者内容完全一致，虚拟地址0xf0100000已经被映射到0x00100000处了,为什么会出现这种变化？ 在修改cr0之前修改了cr3寄存器。将地址0x112000写入了页目录寄存器，页目录表应该就是存放在地址0x112000处。其他操作应该是由entry_pgdir的// Map VA’s [KERNBASE, KERNBASE+4MB) to PA’s [0, 4MB)，完成了映射。使得再读取0xf0100000地址时，自动映射到了0~4M的某个位置（暂时不清楚）。 CR3是页目录基址寄存器，保存页目录表的物理地址，页目录表总是放在以4K字节为单位的存储器边界上，因此，它的地址的低12位总为0，不起作用，即使写上内容，也不会被理会。 2. What is the first instruction after the new mapping is established that would fail to work properly if the mapping weren’t in place? Comment out the movl %eax, %cr0 in kern/entry.S, trace into it, and see if you were right. Q:*建立新映射后，*如果映射不存在，将无法正常工作的 第一条指令是什么？注释掉`kern/entry.S``movl %eax, %cr0`中的 内容，跟踪它，看看你是否正确。 主要意思是说，如果把movl %eax, %cr0删除掉会发生什么样的情况。 删除掉之后，只要后面有涉及到寻址的地方，就会立马出错。假设把这一行注释掉。 # movl %eax, %cr0 # Now paging is enabled, but we're still running at a low EIP # (why is this okay?). Jump up above KERNBASE before entering # C code. mov $relocated, %eax jmp *%eax relocated: # Clear the frame pointer register (EBP) # so that once we get into debugging C code, # stack backtraces will be terminated properly. movl $0x0,%ebp # nuke frame pointer # Set the stack pointer movl $(bootstacktop),%esp 那么在movl $(bootstacktop), %esp这里就立马出错了。 因为把$bootstacktop当成物理地址了。但是实际上，哪有那么大的物理地址空间。所以肯定会报错了。(万一真给了qemu那么大的物理地址空间，那边物理地址也没有内容，跳到C语言之后就会出错。) ","date":"2023-10-31","objectID":"/6.828_lab1/:3:1","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise8 仿照case ‘u’，其中putch是将字符打印在屏幕上 case 'o': // Replace this with your code. putch('0', putdat);//print 0 in front of octal num = getuint(\u0026ap, lflag); base = 8; goto number; Q：解释printf.c和 console.c之间的接口。具体 console.c导出的是什么功能？printf.c如何使用这个函数 ？ A： 这里分析过程如下： printf.c里面的调用链如下： cprintf -\u003e vcprintf -\u003e vprintfmt -\u003e putch -\u003e cputchar 然后cputchar的声明是在 ./inc/stdio.h:11:void cputchar(int c); 这个函数的定义是在console.c void cputchar(int c) { cons_putc(c); } // output a character to the console static void cons_putc(int c) { serial_putc(c); lpt_putc(c); cga_putc(c); } 接下主要就是看cga_putc。也就是显示到屏幕上的函数。首先看一下cga_init。这个函数的功能就是选定特定的屏幕。比如vga, cga等。 static void cga_init(void) { volatile uint16_t *cp; uint16_t was; unsigned pos; cp = (uint16_t*) (KERNBASE + CGA_BUF); was = *cp; *cp = (uint16_t) 0xA55A; if (*cp != 0xA55A) { cp = (uint16_t*) (KERNBASE + MONO_BUF); addr_6845 = MONO_BASE; } else { *cp = was; addr_6845 = CGA_BASE; } /* Extract cursor location */ outb(addr_6845, 14); pos = inb(addr_6845 + 1) \u003c\u003c 8; outb(addr_6845, 15); pos |= inb(addr_6845 + 1); crt_buf = (uint16_t*) cp; crt_pos = pos; } 一般而言，显示操作的时候，启动的时候，都是使用提CGA。也就是 ./kern/console.h:14:#define CGA_BUF 0xB8000 初始化的时候，需要设定光标的位置。设置完成之后。就可以利用cga_putc来CGA屏幕上显示字符了。这里可以看出来，除了各个字符的设定之外。还随时移动的光标。 static void cga_putc(int c) { // if no attribute given, then use black on white if (!(c \u0026 ~0xFF)) c |= 0x0700; switch (c \u0026 0xff) { case '\\b': if (crt_pos \u003e 0) { crt_pos--; crt_buf[crt_pos] = (c \u0026 ~0xff) | ' '; } break; case '\\n': crt_pos += CRT_COLS; /* fallthru */ case '\\r': crt_pos -= (crt_pos % CRT_COLS); break; case '\\t': cons_putc(' '); cons_putc(' '); cons_putc(' '); cons_putc(' '); cons_putc(' '); break; default: crt_buf[crt_pos++] = c; /* write the character */ break; } // What is the purpose of this? if (crt_pos \u003e= CRT_SIZE) { int i; memmove(crt_buf, crt_buf + CRT_COLS, (CRT_SIZE - CRT_COLS) * sizeof(uint16_t)); for (i = CRT_SIZE - CRT_COLS; i \u003c CRT_SIZE; i++) crt_buf[i] = 0x0700 | ' '; crt_pos -= CRT_COLS; } /* move that little blinky thing */ outb(addr_6845, 14); outb(addr_6845 + 1, crt_pos \u003e\u003e 8); outb(addr_6845, 15); outb(addr_6845 + 1, crt_pos); } Q：从console.c解释以下内容： 1 如果 (crt_pos \u003e= CRT_SIZE) { 2 int i; 3 memmove(crt_buf, crt_buf + CRT_COLS, (CRT_SIZE - CRT_COLS) * sizeof(uint16_t)); 4 for (i = CRT_SIZE - CRT_COLS; i \u003c CRT_SIZE; i++) 5 crt_buf[i] = 0x0700 | ' '; 6 crt_pos -= CRT_COLS; 7 } A： #define CRT_ROWS 25 行 #define CRT_COLS 80 列 #define CRT_SIZE (CRT_ROWS * CRT_COLS) 25*80 // 一页写满，滚动一行。 if (crt_pos \u003e= CRT_SIZE) { int i; // 把从第1~n行的内容复制到0~(n-1)行，第n行未变化 // 通过这一行代码完成了整个屏幕向上移动一行的操作。 // 即将[1,24]*80移到[0,23]*80 memmove(crt_buf, crt_buf + CRT_COLS, (CRT_SIZE - CRT_COLS) * sizeof(uint16_t)); // 把最后一行清空 for (i = CRT_SIZE - CRT_COLS; i \u003c CRT_SIZE; i++) crt_buf[i] = 0x0700 | ' '; // 清空了最后一行，同步crt_pos crt_pos -= CRT_COLS; } CRT_SIZE - CRT_COLS 24*80 void *memmove(void *str1, const void *str2, size_t n) 参数 str1 -- 指向用于存储复制内容的目标数组，类型强制转换为 void* 指针。 str2 -- 指向要复制的数据源，类型强制转换为 void* 指针。 n -- 要被复制的字节数。 Q：对于以下问题，您可能希望查阅第 2 讲的注释。这些注释涵盖了 GCC 在 x86 上的调用约定。 逐步跟踪以下代码的执行情况： 整数 x = 1, y = 3, z = 4; cprintf(\"x %d, y %x, z %d\\n\", x, y, z); 在调用 cprintf()时，fmt指向什么？ap指向什么？ 列出（按执行顺序）对 cons_putc、va_arg和 的每次调用vcprintf。对于cons_putc，也列出其参数。对于 va_arg，列出ap调用前后指向的内容。对于vcprintf列出其两个参数的值。 A： static void putch(int ch, int *cnt) { cputchar(ch); *cnt++; } int vcprintf(const char *fmt, va_list ap) { int cnt = 0; vprintfmt((void*)putch, \u0026cnt, fmt, ap); return cnt; } int cprintf(const char *fmt, ...) { va_list ap; int cnt; va_start(ap, fmt); cnt = vcprintf(fmt, ap); va_end(ap); return cnt; } 比如压入了5个char。也是需要用到2个long。在32位机器上。 // 指针定义为char *可以指向任意一个内存地址。 typedef char *va_list; // 类型大小，注意这里是与CPU位数对齐 ＝ sizeof(long)的作用。 #define __va_size(type) \\ (((sizeof(type) + sizeof(long) - 1) / sizeof(long)) * sizeof(long)) // 这里个宏并不是取得参数的起始地址。而是说参数将从什么地址开始放。 #define va_start(ap, last) \\ ((ap) = (va_list)\u0026(last) + __va_size(last)) // va_arg就是用来取参数的起始地址的。然后返回type类型。 // 从整个表达式的意义来说没有什么好用的。 // 其实等价于(*(type*)ap) // 但是实际上使ap指针移动一个参数大小。 #define va_arg(ap, type","date":"2023-10-31","objectID":"/6.828_lab1/:3:2","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"The Stack ","date":"2023-10-31","objectID":"/6.828_lab1/:3:3","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise9 Q:确定内核初始化堆栈的位置，以及堆栈在内存中的确切位置。内核如何为其堆栈保留空间？堆栈指针初始化指向这个保留区域的哪个“端”？ #结论 entry.S 77行初始化栈 栈的位置是0xf0108000-0xf0110000 设置栈的方法是在kernel的数据段预留32KB空间(entry.S 92行) 栈顶的初始化位置是0xf0110000 #分析 bootloader最后一条语句进入内核，进入内核后的几件事情顺序如下： 开启分页(entry.S 62行) 设置栈指针(entry.S 77行) 调用i386_init(entry.S 80行) 设置栈指针的代码如下： # Set the stack pointer movl $(bootstacktop),%esp 可以从kern/kernel文件中找出符号bootstacktop的位置： zzzz@ubuntu:~/workspace/github/xv6-note/Lab1/2014-jos-Lab1$ objdump -D obj/kern/kernel | grep -3 bootstacktop f0108000 \u003cbootstack\u003e: ... f0110000 \u003cbootstacktop\u003e: f0110000: 01 10 add %edx,(%eax) f0110002: 11 00 adc %eax,(%eax) ... 因为没有设置CS，因此CS还是指向之前在bootloader阶段设置的数据段描述符，该描述符指定的基地址为0x0，因此%esp的值就是栈顶的位置。因此栈顶的位置就是0xf0110000。 堆栈的大小由下面的指令设置(entry.S 92行): .data ################################################################### # boot stack ################################################################### .p2align PGSHIFT # force page alignment .globl bootstack bootstack: .space KSTKSIZE .globl bootstacktop bootstacktop: 可以看出，栈的设置方法是在数据段中预留出一些空间来用作栈空间。memlayout.h 97行定义的栈的大小: #define PGSIZE 4096 // bytes mapped by a page ... #define KSTKSIZE (8*PGSIZE) // size of a kernel stack 因此栈大小为32KB，栈的位置为0xf0108000-0xf0110000 #gdb验证 调用call i386_init函数的位置为0xf0100039，在该位置设置断点，查看寄存器内容： (gdb) b *0xf0100039 Breakpoint 1 at 0xf0100039: file kern/entry.S, line 80. (gdb) c Continuing. The target architecture is assumed to be i386 =\u003e 0xf0100039 \u003crelocated+10\u003e: call 0xf010009d \u003ci386_init\u003e Breakpoint 1, relocated () at kern/entry.S:80 80 call i386_init (gdb) info r eax 0xf010002f -267386833 ecx 0x0 0 edx 0x9d 157 ebx 0x10094 65684 esp 0xf0110000 0xf0110000 \u003centry_pgdir\u003e 可以看到，调用函数i386_init之前，栈的位置确实是在0xff010000(%esp)。查看栈的内容如下： (gdb) x /8xw 0xf010fff0 0xf010fff0: 0x00000000 0x00000000 0x00000000 0x00000000 0xf0110000 \u003centry_pgdir\u003e: 0x00111021 0x00000000 0x00000000 0x0000000 第一个压入栈的数据应该是call i386_init的返回地址，即这条指令的下一条指令的地址，stepi单步之后再查看： (gdb) x /8xw 0xf010fff0 0xf010fff0: 0x00000000 0x00000000 0x00000000 0xf010003e 0xf0110000 \u003centry_pgdir\u003e: 0x00111021 0x00000000 0x00000000 0x00000000 (gdb) x /2i 0xf0100039 0xf0100039 \u003crelocated+10\u003e: call 0xf010009d \u003ci386_init\u003e 0xf010003e \u003cspin\u003e: jmp 0xf010003e \u003cspin\u003e 再次查看栈中的内容验证了之前的猜测。 ","date":"2023-10-31","objectID":"/6.828_lab1/:3:4","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise10 在test_backtrace(5)出打上断点即0xf01000f0，随后查看寄存器$esp为0xf010ffe0 *// Test the stack backtrace function (lab 1 only)* ​ test_backtrace(5)*;* f01000f0: c7 04 24 05 00 00 00 movl $0x5,(%esp) f01000f7: e8 44 ff ff ff call f0100040 \u003ctest_backtrace\u003e f01000fc: 83 c4 10 add $0x10,%esp mon_backtrace(0, 0, 0); f0100097: 83 ec 04 sub $0x4,%esp f010009a: 6a 00 push $0x0 f010009c: 6a 00 push $0x0 f010009e: 6a 00 push $0x0 f01000a0: e8 0c 08 00 00 call f01008b1 \u003cmon_backtrace\u003e 然后再mon_backtrace函数上打上断点，查看esp寄存器的值。 反汇编代码如下： test_backtrace(int x) { cprintf(\"entering test_backtrace %d\\n\", x); if (x \u003e 0) test_backtrace(x-1); else mon_backtrace(0, 0, 0); cprintf(\"leaving test_backtrace %d\\n\", x); } f0100040: 55 push %ebp ;压入调用函数的%ebp f0100041: 89 e5 mov %esp,%ebp ;将当前%esp存到%ebp中，作为栈帧 f0100043: 53 push %ebx ;保存%ebx当前值，防止寄存器状态被破坏 f0100044: 83 ec 14 sub $0x14,%esp ;开辟20字节栈空间用于本函数内使用 f0100047: 8b 5d 08 mov 0x8(%ebp),%ebx ;取出调用函数传入的第一个参数 f010004a: 89 5c 24 04 mov %ebx,0x4(%esp) ;压入cprintf的最后一个参数，x的值 f010004e: c7 04 24 e0 19 10 f0 movl $0xf01019e0,(%esp) ;压入cprintf的倒数第二个参数，指向格式化字符串\"entering test_backtrace %d\\n\" f0100055: e8 27 09 00 00 call f0100981 \u003ccprintf\u003e ;调用cprintf函数，打印entering test_backtrace (x) f010005a: 85 db test %ebx,%ebx ;测试是否小于0 f010005c: 7e 0d jle f010006b \u003ctest_backtrace+0x2b\u003e ;如果小于0，则结束递归，跳转到0xf010006b处执行 f010005e: 8d 43 ff lea -0x1(%ebx),%eax ;如果不小于0，则将x的值减1，复制到栈上 f0100061: 89 04 24 mov %eax,(%esp) ;接上一行 f0100064: e8 d7 ff ff ff call f0100040 \u003ctest_backtrace\u003e ;递归调用test_backtrace f0100069: eb 1c jmp f0100087 \u003ctest_backtrace+0x47\u003e ;跳转到f0100087执行 f010006b: c7 44 24 08 00 00 00 movl $0x0,0x8(%esp) ;如果x小于等于0，则跳到这里执行，压入mon_backtrace的最后一个参数 f0100072: 00 f0100073: c7 44 24 04 00 00 00 movl $0x0,0x4(%esp) ;压入mon_backtrace的倒数第二个参数 f010007a: 00 f010007b: c7 04 24 00 00 00 00 movl $0x0,(%esp) ;压入mon_backtrace的倒数第三个参数 f0100082: e8 68 07 00 00 call f01007ef \u003cmon_backtrace\u003e ;调用mon_backtrace，这是这个练习需要实现的函数 f0100087: 89 5c 24 04 mov %ebx,0x4(%esp) ;压入cprintf的最后一个参数，x的值 f010008b: c7 04 24 fc 19 10 f0 movl $0xf01019fc,(%esp) ;压入cprintf的倒数第二个参数，指向格式化字符串\"leaving test_backtrace %d\\n\" f0100092: e8 ea 08 00 00 call f0100981 \u003ccprintf\u003e ;调用cprintf函数，打印leaving test_backtrace (x) f0100097: 83 c4 14 add $0x14,%esp ;回收开辟的栈空间 f010009a: 5b pop %ebx ;恢复寄存器%ebx的值 f010009b: 5d pop %ebp ;恢复寄存器%ebp的值 f010009c: c3 ret ;函数返回 一个栈帧(stack frame)的大小计算如下： 在执行call test_backtrace时有一个副作用就是压入这条指令下一条指令的地址，压入4字节返回地址 push %ebp，将上一个栈帧的地址压入，增加4字节 push %ebx，保存ebx寄存器的值，增加4字节 sub $0x14, %esp，开辟20字节的栈空间，后面的函数调用传参直接操作这个栈空间中的数，而不是用pu sh的方式压入栈中 加起来一共是32字节，也就是8个int。因此上面打印出来的栈内容，每两行表示一个栈帧，看v起来还算清晰。 #第一次调用分析 以第一调用栈为例分析，32个字节代码的含义如下图所示： 0xf010ffc0: 0x00000004 0x00000005 0x00000000 0xf010004e 0xf010ffd0: 0xf0111308 0x00010094 0xf010fff8 0xf01000fc +--------------------------------------------------------------+ | next x | this x | don't know | don't know | +--------------+----------------+---------------+--------------+ | don't know | last ebx | last ebp | return addr | +------ -------------------------------------------------------+ 中间的两字节不知道是干嘛用的(靠近this x的那一个在调用mon_backtrace时会用到)，按照理论分析，一个完整的调用栈最少需要的字节数等于4+4+4+4*3=24字节，即返回地址，上一个函数的ebp，保存的ebx，函数内没有分配局部变量，需要再加12个字节用来调用mon_backtrace时传参数。 有一个说法是，因为x86的栈大小必须是16的整数倍，所以才分配了32个字节的栈大小。 ","date":"2023-10-31","objectID":"/6.828_lab1/:3:5","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise11 --- a/Lab1/2014-jos-Lab1/kern/monitor.c +++ b/Lab1/2014-jos-Lab1/kern/monitor.c @@ -58,8 +58,17 @@ mon_kerninfo(int argc, char **argv, struct Trapframe *tf) int mon_backtrace(int argc, char **argv, struct Trapframe *tf) { - // Your code here. - return 0; + uint32_t ebp, *p; + + ebp = read_ebp(); + while (ebp != 0) + { + p = (uint32_t *) ebp; + cprintf(\"ebp %x eip %x args %08x %08x %08x %08x %08x\\n\", ebp, p[1], p[2], p[3], p[4], p[5], p[6]); + ebp = p[0]; + } + + return 0; } 先把ebp寄存器中存的地址存入ebp中并打印出来，然后把返回地址即ebp+4的地址打印出来，随后是args[1-5]。最后将ebp存的地址所指向的内容即上一个调用者的ebp地址赋值给ebp寄存器 ","date":"2023-10-31","objectID":"/6.828_lab1/:3:6","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"},{"categories":["6.828"],"content":"exercise12 仿照debuginfo_eip中的其他操作写出给eip_line赋值 stab_binsearch(stabs, \u0026lline, \u0026rline, N_SLINE, addr); if (lline \u003c= rline) info-\u003eeip_line = stabs[lline].n_desc; else cprintf(\"lline \u003e rline\\n\"); 随后修改monitor.h中的Command结构体 static struct Command commands[] = { {\"help\", \"Display this list of commands\", mon_help}, {\"kerninfo\", \"Display information about the kernel\", mon_kerninfo}, {\"backtrace\", \"Display information about the stack\", mon_backtrace}, }; 最后在mon_backtrace函数中打印出函数名，函数所在的行等信息 int mon_backtrace(int argc, char **argv, struct Trapframe *tf) { // Your code here. uint32_t ebp = read_ebp(); uint32_t eip = 0; struct Eipdebuginfo info; #define TO_INT(x) *((uint32_t *)(x)) while (ebp) { eip = TO_INT((ebp + 4)); // ebp f0109e58 eip f0100a62 args 00000001 f0109e80 f0109e98 f0100ed2 00000031 cprintf(\"ebp %08x eip %08x args %08x %08x %08x %08x %08x\\n\", ebp, /*ebp*/ eip, /*eip*/ TO_INT((ebp + 8)), /*arg1*/ TO_INT((ebp + 12)), /*arg2*/ TO_INT((ebp + 16)), /*arg3*/ TO_INT((ebp + 20)), /*arg4*/ TO_INT((ebp + 24))); /*arg5*/ if(!debuginfo_eip(eip, \u0026info)) { cprintf(\"%s:%d: %.*s+%d\\n\", info.eip_file, info.eip_line, info.eip_fn_namelen, info.eip_fn_name, eip - info.eip_fn_addr); } else { cprintf(\"debuginfo_epi error\\n\"); } ebp = TO_INT(ebp); } return 0; } ","date":"2023-10-31","objectID":"/6.828_lab1/:3:7","tags":null,"title":"6.828_lab1","uri":"/6.828_lab1/"}]